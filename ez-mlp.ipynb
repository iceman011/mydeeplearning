{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPISOzVaGurg9PexEDATrYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceman011/mydeeplearning/blob/master/ez-mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXy2soVzPQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers,output_classes, drop_p=0.5,lr =0.001, train_on_gpu=False):\n",
        "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Training on CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Training on GPU ...')\n",
        "            \n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.output_classes = output_classes\n",
        "\n",
        "        # Input to a hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "        \n",
        "        # Add a variable number of more hidden layers\n",
        "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "        \n",
        "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "        \n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def validation(model, validationloader, criterion):\n",
        "    accuracy = 0\n",
        "    validation_loss = 0\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if model.train_on_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "      for images, labels in validationloader:\n",
        "\n",
        "          if model.train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "          \n",
        "          images = images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "          output = model.forward(images)\n",
        "          validation_loss += criterion(output, labels).item()\n",
        "\n",
        "          ## Calculating the accuracy \n",
        "          # Model's output is log-softmax, take exponential to get the probabilities\n",
        "          ps = torch.exp(output)\n",
        "          # Class with highest probability is our predicted class, compare with true label\n",
        "          #equality = (labels.data == ps.max(1)[1])\n",
        "          # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "          #accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "\n",
        "\n",
        "        \n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))      \n",
        "        \n",
        "      validation_loss = validation_loss/len(validationloader)\n",
        "      accuracy = accuracy/len(validationloader)\n",
        "\n",
        "    return validation_loss, accuracy\n",
        "\n",
        "#############################\n",
        "# TEST MODEL\n",
        "############################\n",
        "def test(model,test_loader,criterion,checkpoint,outputfilepath):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(model.output_size))\n",
        "    class_total = list(0. for i in range(model.output_size))\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      model.eval()\n",
        "      # iterate over test data\n",
        "      for data, target in test_loader:\n",
        "          # move tensors to GPU if CUDA is available\n",
        "          if model.train_on_gpu:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          \n",
        "          # Flatten images into a 784 long vector\n",
        "          data.resize_(data.size()[0], model.input_size)\n",
        "\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the batch loss\n",
        "          loss = criterion(output, target)\n",
        "          # update test loss \n",
        "          test_loss += loss.item()*data.size(0)\n",
        "          # convert output probabilities to predicted class\n",
        "          _, pred = torch.max(output, 1)    \n",
        "          # compare predictions to true label\n",
        "          correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "          correct = np.squeeze(correct_tensor.numpy()) if not model.train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "          # calculate test accuracy for each object class\n",
        "          for i in range(batch_size):\n",
        "              label = target.data[i]\n",
        "              class_correct[label] += correct[i].item()\n",
        "              class_total[label] += 1\n",
        "\n",
        "      # average test loss\n",
        "      test_loss = test_loss/len(test_loader.dataset)\n",
        "      print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "      \n",
        "      for i in range(model.output_size):\n",
        "          if class_total[i] > 0:\n",
        "            current_key  = 'Test Accuracy of {}'.format(model.output_classes[i])\n",
        "            current_val = '{}% ({}/{})'.format(100 * class_correct[i] / class_total[i],\n",
        "                  np.sum(class_correct[i]), np.sum(class_total[i]))\n",
        "            ele={current_key:current_val}\n",
        "            checkpoint.update(ele)\n",
        "\n",
        "            \"\"\" print(current_key + current_val)\n",
        "          else:\n",
        "              print('Test Accuracy of %5s: N/A (no training examples)' % (model.output_classes[i]))\n",
        "          \"\"\"\n",
        "\n",
        "      current_key = 'Test Accuracy (Overall): '.format(100. * np.sum(class_correct) / np.sum(class_total))\n",
        "      current_val = '{}% ({}/{})'.format(100. * np.sum(class_correct) / np.sum(class_total),\n",
        "          np.sum(class_correct), np.sum(class_total) )\n",
        "      ele={current_key:current_val}\n",
        "      checkpoint.update(ele)\n",
        "      \n",
        "      #print(current_key + current_val)\n",
        "    \n",
        "    #model.train()\n",
        "    torch.save(checkpoint,outputfilepath)\n",
        "    return checkpoint\n",
        "\n",
        "#############################\n",
        "# TRAIN MODEL\n",
        "############################\n",
        "def train(model, trainloader, validationloader, criterion, optimizer, epochs=5, print_every=40 ):\n",
        "    # monitor training loss    \n",
        "    steps = 0    \n",
        "    start_time = time.time()\n",
        "    dateTimeObj = datetime.now()\n",
        "    start_time_timestamp = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    train_losses, valid_losses = [], []\n",
        "    \n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    print('Starting Training using Model Parameters ',model )\n",
        "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "    \n",
        "    for e in range(epochs):        \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            \n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if model.train_on_gpu:\n",
        "              images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Flatten images into a 784 long vector\n",
        "            images.resize_(images.size()[0], model.input_size)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        train_losses.append(train_loss)\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # Model in inference mode, dropout is off\n",
        "        model.eval()\n",
        "        \n",
        "        # Turn off gradients for validation, will speed up inference\n",
        "        with torch.no_grad():\n",
        "            valid_loss, accuracy = validation(model, validationloader, criterion)\n",
        "        \n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy : {:.6f}'.format(\n",
        "            e+1, \n",
        "            train_loss,\n",
        "            valid_loss,\n",
        "            accuracy\n",
        "            ))\n",
        "        \n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f} ,)  Accuracy: {:.6f}  TimeElapsed: {:.6f}.  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss,\n",
        "            accuracy,\n",
        "            (time.time() - start_time)))\n",
        "              \n",
        "            checkpoint = {'InputSize': model.input_size,\n",
        "                  'OutputSize': model.output_size,\n",
        "                  'HiddenLayers': [each.out_features for each in model.hidden_layers],\n",
        "                  'LearningRate':model.learning_rate,\n",
        "                  'TrainingLoss' :train_loss,\n",
        "                  'ValidationLoss ':valid_loss,\n",
        "                  'TrainingLosses' :train_losses,\n",
        "                  'ValidationLosses ':valid_losses,\n",
        "                  'ElapsedTime': (time.time() - start_time),\n",
        "                  'CheckPointTimestamp': time.time(),\n",
        "                  'CurrentEpoch': e,\n",
        "                  'GPUState': model.train_on_gpu,\n",
        "                  'OutputClasses':model.output_classes,\n",
        "                  'OutputFolder' : start_time_timestamp,\n",
        "                  'OutputFilePrefix' : 'checkpoint_',\n",
        "                  'StateDictionay': model.state_dict()}\n",
        "            \n",
        "            #print(checkpoint)\n",
        "            if not os.path.exists(start_time_timestamp):\n",
        "                os.makedirs(start_time_timestamp)\n",
        "                \n",
        "            torch.save(checkpoint, './'+start_time_timestamp+'/checkpoint_'+str(e)+'.pt')\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "    return train_losses , valid_losses , checkpoint\n",
        "\n",
        "\n",
        "def plotLossTrend(train_losses,validation_losses):\n",
        "  plt.plot(train_losses, label='Training loss')\n",
        "  plt.plot(validation_losses, label='Validation loss')\n",
        "  plt.legend(frameon=False)\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(OutputFolder,OutputFilePrefix):\n",
        "\n",
        "    file_epoch = 0\n",
        "    for filename in os.listdir(OutputFolder):\n",
        "      data = filename.split('_')\n",
        "      tmp=int(data[1][:-3])\n",
        "      #print('data[1]',data[1],' tmp[:-3] ',tmp[:-3])\n",
        "      if( file_epoch <= tmp ):\n",
        "        file_epoch = tmp\n",
        "        \n",
        "    filepath = OutputFolder+'/'+OutputFilePrefix + str(file_epoch)+'.pt'\n",
        "    \n",
        "    print('Loading Checkpoint ...')\n",
        "    indent=1\n",
        "    checkpoint = torch.load(filepath)\n",
        "    for key, value in checkpoint.items():\n",
        "        if(key == 'StateDictionay'):\n",
        "          continue\n",
        "        print('\\t' * indent + str(key))\n",
        "        print('\\t' * (indent+1) + str(value))\n",
        "\n",
        "    testmodel = Network(checkpoint['InputSize'],\n",
        "                             checkpoint['OutputSize'],\n",
        "                             checkpoint['HiddenLayers'],\n",
        "                            checkpoint['OutputClasses'],\n",
        "                            lr=checkpoint['LearningRate'],\n",
        "                            train_on_gpu=checkpoint['GPUState']\n",
        "                    )\n",
        "    testmodel.load_state_dict(checkpoint['StateDictionay'])\n",
        "    return testmodel , checkpoint , filepath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPtfMqA2zlN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        ######################    \n",
        "        # LOADING DATA #\n",
        "        ######################\n",
        "\n",
        "# import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7of0_y_zsQR",
        "colab_type": "code",
        "outputId": "36b6f3eb-4542-4b1e-9c32-5ce281550356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "        ######################    \n",
        "        # MAIN #\n",
        "        ######################\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['1', '2', '3', '4', '5',\n",
        "           '6', '7', '8', '9', '10']\n",
        "\n",
        "# Create the network, define the criterion and optimizer\n",
        "\n",
        "model = Network(784, 10, [512, 256, 128],classes, lr=0.001,train_on_gpu=True)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, epochs=2)\n",
        "\n",
        "plotLossTrend (train_lossess , valid_lossess)\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ezz  True\n",
            "CUDA is available!  Training on GPU ...\n",
            "Starting Training using Model Parameters  Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Epoch: 1 \tTraining Loss: 0.021492 \tValidation Loss: 0.178966 \tAccuracy : 0.946752\n",
            "Validation loss decreased (inf --> 0.178966 ,)  Accuracy: 0.946752  TimeElapsed: 9.694728.  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.011586 \tValidation Loss: 0.143478 \tAccuracy : 0.959086\n",
            "Validation loss decreased (0.178966 --> 0.143478 ,)  Accuracy: 0.959086  TimeElapsed: 19.208759.  Saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwIAAAHwCAYAAAAVediDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naqH/D9DyN19nc0u8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 385,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu907NvWVlBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "920637c0-a19b-47a5-92ea-559e99ece82f"
      },
      "source": [
        "model , checkpointtt , filepathh = load_checkpoint(checkpointt['OutputFolder'],checkpointt['OutputFilePrefix'])\n",
        "#load_checkpoint(checkpointt['OutputFolder'],checkpointt['OutputFilePrefix'])\n",
        "\n",
        "test(model,test_loader,criterion,checkpointtt,filepathh)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Checkpoint ...\n",
            "\tInputSize\n",
            "\t\t784\n",
            "\tOutputSize\n",
            "\t\t10\n",
            "\tHiddenLayers\n",
            "\t\t[512, 256, 128]\n",
            "\tLearningRate\n",
            "\t\t0.001\n",
            "\tTrainingLoss\n",
            "\t\t0.011585559079064599\n",
            "\tValidationLoss \n",
            "\t\t0.14347785773531843\n",
            "\tTrainingLosses\n",
            "\t\t[0.021491792484613446, 0.011585559079064599]\n",
            "\tValidationLosses \n",
            "\t\t[0.178966156312963, 0.14347785773531843]\n",
            "\tElapsedTime\n",
            "\t\t19.20912504196167\n",
            "\tCheckPointTimestamp\n",
            "\t\t1582837773.9842901\n",
            "\tCurrentEpoch\n",
            "\t\t1\n",
            "\tGPUState\n",
            "\t\tTrue\n",
            "\tOutputClasses\n",
            "\t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tOutputFolder\n",
            "\t\t27_Feb_2020_21_09_14\n",
            "\tOutputFilePrefix\n",
            "\t\tcheckpoint_\n",
            "\tTest Accuracy of 1\n",
            "\t\t98.77551020408163% (968.0/980.0)\n",
            "\tTest Accuracy of 2\n",
            "\t\t98.50220264317181% (1118.0/1135.0)\n",
            "\tTest Accuracy of 3\n",
            "\t\t98.06201550387597% (1012.0/1032.0)\n",
            "\tTest Accuracy of 4\n",
            "\t\t94.35643564356435% (953.0/1010.0)\n",
            "\tTest Accuracy of 5\n",
            "\t\t97.45417515274949% (957.0/982.0)\n",
            "\tTest Accuracy of 6\n",
            "\t\t93.49775784753363% (834.0/892.0)\n",
            "\tTest Accuracy of 7\n",
            "\t\t97.49478079331942% (934.0/958.0)\n",
            "\tTest Accuracy of 8\n",
            "\t\t95.62256809338521% (983.0/1028.0)\n",
            "\tTest Accuracy of 9\n",
            "\t\t95.89322381930185% (934.0/974.0)\n",
            "\tTest Accuracy of 10\n",
            "\t\t93.85530227948463% (947.0/1009.0)\n",
            "\tTest Accuracy (Overall): \n",
            "\t\t96.4% (9640.0/10000.0)\n",
            "CUDA is available!  Training on GPU ...\n",
            "Test Loss: 0.127570\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CheckPointTimestamp': 1582837773.9842901,\n",
              " 'CurrentEpoch': 1,\n",
              " 'ElapsedTime': 19.20912504196167,\n",
              " 'GPUState': True,\n",
              " 'HiddenLayers': [512, 256, 128],\n",
              " 'InputSize': 784,\n",
              " 'LearningRate': 0.001,\n",
              " 'OutputClasses': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n",
              " 'OutputFilePrefix': 'checkpoint_',\n",
              " 'OutputFolder': '27_Feb_2020_21_09_14',\n",
              " 'OutputSize': 10,\n",
              " 'StateDictionay': OrderedDict([('hidden_layers.0.weight',\n",
              "               tensor([[ 0.0031, -0.0113, -0.0357,  ...,  0.0198, -0.0015, -0.0233],\n",
              "                       [ 0.0275,  0.0155, -0.0295,  ..., -0.0177, -0.0221, -0.0324],\n",
              "                       [-0.0095,  0.0097, -0.0149,  ..., -0.0088,  0.0327, -0.0170],\n",
              "                       ...,\n",
              "                       [ 0.0267,  0.0152, -0.0268,  ...,  0.0092, -0.0262, -0.0118],\n",
              "                       [ 0.0304, -0.0015,  0.0160,  ..., -0.0144, -0.0164, -0.0070],\n",
              "                       [ 0.0014, -0.0027,  0.0070,  ...,  0.0227, -0.0085,  0.0185]],\n",
              "                      device='cuda:0')),\n",
              "              ('hidden_layers.0.bias',\n",
              "               tensor([-0.0448,  0.0066,  0.0385, -0.0691, -0.0114, -0.0837, -0.0633, -0.0378,\n",
              "                        0.0042, -0.0121, -0.0488,  0.0215, -0.0543, -0.0529,  0.0215, -0.0174,\n",
              "                       -0.0557,  0.0206,  0.0186,  0.0572,  0.0184, -0.0158, -0.0313, -0.0444,\n",
              "                       -0.0312, -0.0167, -0.1219,  0.0015, -0.0279, -0.0419,  0.0389, -0.0234,\n",
              "                       -0.0494,  0.0427, -0.0448,  0.0084, -0.0375,  0.0818, -0.0506, -0.0064,\n",
              "                       -0.0120,  0.0804, -0.0568, -0.0399,  0.0355, -0.0599, -0.0227,  0.0436,\n",
              "                       -0.0515,  0.0451,  0.0731,  0.0108, -0.0704, -0.0099, -0.0099,  0.0066,\n",
              "                       -0.0054, -0.0104, -0.0800, -0.0194,  0.0270, -0.0266, -0.0758,  0.0244,\n",
              "                       -0.0730, -0.0385, -0.0189,  0.0160, -0.0401, -0.0906, -0.0501, -0.0193,\n",
              "                       -0.0292, -0.1025,  0.0629, -0.0506,  0.0010, -0.0158,  0.0116, -0.0168,\n",
              "                       -0.0746, -0.0429,  0.0109, -0.0049,  0.0306,  0.0649,  0.0085, -0.0867,\n",
              "                        0.0037,  0.0007, -0.1331, -0.0489,  0.1305, -0.0010, -0.0347,  0.0155,\n",
              "                       -0.0141, -0.0107, -0.0348, -0.0523, -0.0178, -0.0937, -0.0146,  0.0509,\n",
              "                        0.0175, -0.0108, -0.0037, -0.0236, -0.0636, -0.0165,  0.0509,  0.0085,\n",
              "                        0.0705, -0.0834, -0.0422, -0.0130,  0.0308, -0.0911,  0.0513, -0.0358,\n",
              "                        0.0352,  0.0675,  0.0482, -0.0052,  0.0133, -0.0492, -0.0125, -0.0190,\n",
              "                       -0.0374,  0.0421, -0.0053, -0.0284, -0.0015,  0.0132,  0.0305, -0.0665,\n",
              "                       -0.0317, -0.0122,  0.0661, -0.0518, -0.0190, -0.0173, -0.0737, -0.0373,\n",
              "                       -0.0804, -0.0768,  0.0155, -0.1327, -0.0302, -0.0515,  0.0109,  0.0188,\n",
              "                       -0.0376, -0.0002,  0.0943, -0.0308, -0.0397, -0.0078, -0.0492, -0.0448,\n",
              "                        0.0088, -0.0112, -0.0645, -0.0701,  0.0477,  0.0083, -0.0014, -0.0134,\n",
              "                        0.0122,  0.0072,  0.0345,  0.0019, -0.0179, -0.0331, -0.0244,  0.0122,\n",
              "                       -0.0354, -0.0049, -0.0319,  0.0211,  0.0482, -0.0162,  0.0306,  0.0393,\n",
              "                       -0.0873,  0.0055, -0.1010,  0.0132,  0.0306,  0.0306, -0.0031,  0.0198,\n",
              "                       -0.0048,  0.0719, -0.0464, -0.0719,  0.0156,  0.0747,  0.0217,  0.0573,\n",
              "                        0.0043, -0.0187, -0.0752,  0.0219, -0.0370,  0.0009,  0.0119,  0.0332,\n",
              "                       -0.0354,  0.0004, -0.0095, -0.0036,  0.0049, -0.0169,  0.0319, -0.0423,\n",
              "                       -0.0323, -0.0723,  0.0471, -0.0070, -0.0025, -0.0038, -0.0060, -0.0058,\n",
              "                        0.0147,  0.0256, -0.0305,  0.0369, -0.0228, -0.0084,  0.0115, -0.0082,\n",
              "                       -0.0266,  0.0188, -0.0496, -0.0437,  0.0517, -0.0590, -0.0319, -0.0240,\n",
              "                        0.0624, -0.0557,  0.0126, -0.0808, -0.0779,  0.0211, -0.0496, -0.0162,\n",
              "                       -0.0852,  0.0155,  0.0112,  0.0534, -0.0534, -0.0361, -0.0473, -0.0344,\n",
              "                        0.0246,  0.0304, -0.0057, -0.0671,  0.0720,  0.0053,  0.0458,  0.0478,\n",
              "                       -0.0524, -0.0471, -0.0172, -0.1258,  0.0266, -0.0743, -0.1027, -0.0200,\n",
              "                       -0.0621, -0.0063, -0.0127,  0.0138, -0.0548, -0.0741, -0.0376, -0.0346,\n",
              "                        0.0220, -0.0351, -0.0161, -0.0749, -0.0712,  0.0410, -0.0398, -0.0265,\n",
              "                        0.0226,  0.0171,  0.0158, -0.0467, -0.0305, -0.1084,  0.0513,  0.0656,\n",
              "                        0.0011, -0.0311,  0.0022,  0.0223,  0.0362, -0.0678, -0.0917,  0.0122,\n",
              "                       -0.0286,  0.0203, -0.0284, -0.0558, -0.0643, -0.0323,  0.0136, -0.0918,\n",
              "                       -0.0201, -0.0628, -0.0448, -0.0003, -0.0650,  0.0573, -0.0092, -0.0727,\n",
              "                       -0.0213, -0.0251, -0.0222, -0.0542, -0.0316, -0.0174,  0.0063, -0.0483,\n",
              "                       -0.0755, -0.0728, -0.0083,  0.0627, -0.0952,  0.0248, -0.1002, -0.0098,\n",
              "                       -0.0080, -0.1395, -0.0212, -0.0236, -0.0058, -0.0361, -0.0288, -0.0469,\n",
              "                        0.0352, -0.0278, -0.1032,  0.0136, -0.1138,  0.0326, -0.0445, -0.0040,\n",
              "                       -0.0535, -0.0448,  0.0479, -0.0278, -0.0239, -0.0351, -0.0867, -0.0295,\n",
              "                        0.0543, -0.0481,  0.0397, -0.0727, -0.0228, -0.0178, -0.0275,  0.0790,\n",
              "                        0.0719, -0.0319, -0.0394, -0.0292, -0.0628, -0.0456,  0.0607, -0.0288,\n",
              "                        0.0132,  0.0058, -0.0252,  0.0134, -0.0718, -0.0280, -0.0119, -0.0174,\n",
              "                       -0.0058,  0.0760, -0.0229,  0.0502,  0.0071, -0.0125, -0.0110,  0.0412,\n",
              "                        0.0190,  0.0040, -0.0212, -0.0569, -0.0749,  0.0455,  0.0364,  0.0134,\n",
              "                       -0.0279,  0.0352, -0.0247, -0.0433,  0.0396,  0.0593, -0.0897, -0.0395,\n",
              "                        0.0285, -0.0085, -0.0335, -0.0129,  0.0406,  0.0614, -0.0075, -0.1248,\n",
              "                       -0.0092, -0.1030,  0.0301, -0.0357,  0.0382, -0.0108, -0.0253,  0.0307,\n",
              "                        0.0072, -0.0026, -0.0418,  0.0473, -0.0393,  0.0065,  0.0031, -0.0580,\n",
              "                       -0.0161,  0.0129,  0.0165, -0.0218, -0.1220,  0.0142,  0.0628, -0.0937,\n",
              "                        0.0731, -0.0592, -0.0646, -0.0212, -0.0137, -0.0612, -0.0315,  0.1211,\n",
              "                        0.0224,  0.0318,  0.0070, -0.0363, -0.0130, -0.0181,  0.0162, -0.0442,\n",
              "                       -0.0550, -0.0273, -0.1392,  0.1160, -0.0331, -0.0377, -0.0208, -0.0291,\n",
              "                        0.0435,  0.0540, -0.0237, -0.0522, -0.0393, -0.0324,  0.0367, -0.0287,\n",
              "                       -0.0556,  0.0170, -0.0130, -0.0736, -0.0089, -0.0236, -0.0184, -0.0288,\n",
              "                        0.0381,  0.0021, -0.0031,  0.0397,  0.0385, -0.0869,  0.0477,  0.0070,\n",
              "                       -0.0343,  0.0092, -0.0724,  0.1187, -0.0289, -0.0144,  0.0076, -0.0716,\n",
              "                       -0.0135,  0.0123, -0.0027, -0.0341, -0.0168,  0.0146, -0.0620, -0.0045,\n",
              "                        0.0158,  0.0158, -0.0203, -0.0402, -0.0225, -0.0163, -0.0589, -0.0145],\n",
              "                      device='cuda:0')),\n",
              "              ('hidden_layers.1.weight',\n",
              "               tensor([[-0.0839,  0.0317, -0.0065,  ...,  0.0499, -0.0238, -0.0398],\n",
              "                       [-0.0651, -0.0372, -0.1066,  ..., -0.0596, -0.1502, -0.0384],\n",
              "                       [-0.0973,  0.0258, -0.1091,  ...,  0.0299,  0.1291,  0.0349],\n",
              "                       ...,\n",
              "                       [-0.0615, -0.0202, -0.0082,  ...,  0.0021,  0.0178,  0.0158],\n",
              "                       [ 0.0213,  0.0361,  0.0944,  ...,  0.0228, -0.0929, -0.0237],\n",
              "                       [ 0.0600,  0.0230,  0.1191,  ..., -0.0335,  0.0730, -0.0223]],\n",
              "                      device='cuda:0')),\n",
              "              ('hidden_layers.1.bias',\n",
              "               tensor([ 4.9947e-02,  5.2296e-02,  2.2583e-02, -8.7004e-02, -2.1009e-02,\n",
              "                       -5.8020e-02, -8.2871e-02, -4.4088e-02, -2.1729e-02,  9.7968e-02,\n",
              "                        1.2123e-02,  2.6667e-02,  5.8633e-02, -3.6646e-02,  1.1318e-01,\n",
              "                        6.4320e-02, -8.0640e-02, -6.1807e-02, -6.5736e-02, -6.3871e-02,\n",
              "                        1.1489e-01, -6.1202e-02, -7.0667e-02,  4.6799e-02, -5.0557e-02,\n",
              "                       -4.0636e-02, -4.6738e-02, -8.1773e-02, -9.5990e-02,  8.8885e-02,\n",
              "                        4.7819e-03, -3.8378e-02, -1.0043e-03,  2.5065e-02, -3.3397e-02,\n",
              "                       -2.2781e-02,  2.6410e-03, -8.8778e-03,  1.4000e-02, -2.0079e-02,\n",
              "                       -6.5305e-02, -6.7017e-02,  8.1679e-06,  3.6194e-03, -2.4579e-02,\n",
              "                        1.2389e-01, -5.4234e-02,  1.1414e-02,  6.5537e-02, -8.8890e-02,\n",
              "                        2.8498e-02,  1.6851e-01, -4.6107e-02, -3.5252e-02, -2.7285e-03,\n",
              "                        3.2755e-02,  2.7479e-02,  4.5635e-02, -1.2181e-01, -5.2292e-02,\n",
              "                       -1.0270e-01, -1.7244e-02, -9.3944e-02, -2.6804e-04, -6.7015e-02,\n",
              "                       -7.4671e-03,  2.3255e-02,  3.5566e-02,  1.3109e-01, -1.5641e-02,\n",
              "                        3.6124e-02, -2.2852e-02, -9.7244e-02,  3.7526e-02,  2.2019e-03,\n",
              "                        2.2138e-02, -5.4357e-03,  7.7563e-02, -3.0933e-03, -1.0081e-01,\n",
              "                       -6.8946e-02, -1.9256e-02, -2.9809e-02, -5.2728e-02, -8.9303e-02,\n",
              "                       -5.4008e-03,  6.9092e-02, -6.5395e-02, -8.7387e-03,  1.2153e-01,\n",
              "                       -3.7996e-02,  9.4386e-02,  1.5019e-01, -5.5527e-03, -8.7431e-02,\n",
              "                       -7.8701e-02, -3.0945e-02,  8.5314e-02,  6.4925e-03, -8.0522e-02,\n",
              "                        4.7411e-02, -9.7362e-03,  7.0078e-02,  7.8870e-02, -9.7520e-02,\n",
              "                       -1.4202e-02,  5.0686e-03,  1.9409e-01, -7.5869e-02, -6.9676e-02,\n",
              "                       -5.9495e-02, -3.0996e-02,  2.3873e-02,  1.5673e-03,  2.9831e-03,\n",
              "                       -7.6673e-02,  1.6183e-02, -4.3258e-02,  1.4244e-01,  4.5865e-02,\n",
              "                        1.8903e-02,  2.8914e-02,  6.8218e-02, -7.6292e-02,  7.6225e-02,\n",
              "                        1.2185e-02,  2.0767e-02, -1.1755e-01, -6.2459e-02, -1.0632e-01,\n",
              "                       -4.4726e-02,  3.7581e-02, -5.7076e-02, -2.1792e-02, -2.8905e-02,\n",
              "                       -5.4134e-02, -1.1968e-02, -2.7598e-02, -6.2061e-02, -4.9675e-02,\n",
              "                       -3.6559e-02,  9.3053e-03,  7.0446e-03, -6.9719e-02,  3.5588e-02,\n",
              "                       -1.3564e-01, -3.1687e-03,  5.5836e-04, -6.6768e-02,  2.7355e-02,\n",
              "                       -7.2595e-02,  1.3431e-01, -5.5662e-02, -1.0031e-01,  1.3839e-01,\n",
              "                       -6.3379e-02,  1.0954e-01, -7.1491e-02,  3.2645e-03, -6.1833e-02,\n",
              "                       -1.2297e-01,  8.7747e-02,  1.8214e-01, -5.8868e-02, -2.7306e-02,\n",
              "                       -8.5955e-02, -2.8493e-02,  1.0919e-01, -1.2384e-01,  1.1891e-02,\n",
              "                       -8.8810e-04, -1.4377e-02, -8.6233e-02,  5.3444e-02, -5.1637e-02,\n",
              "                       -6.4097e-02, -1.3393e-01, -3.6934e-02, -4.4289e-03,  1.1629e-01,\n",
              "                       -7.1762e-02, -1.1183e-01,  8.9011e-02, -8.6450e-02, -1.5715e-02,\n",
              "                       -2.0188e-02, -3.5377e-02, -9.2887e-02,  1.8274e-03,  3.5807e-02,\n",
              "                       -6.0513e-02, -1.0490e-01, -1.8862e-02,  1.0051e-01, -5.1726e-03,\n",
              "                       -9.5860e-02, -4.4621e-03,  9.5803e-02,  7.1631e-03, -1.1226e-01,\n",
              "                       -1.5184e-01, -3.5827e-03, -6.4659e-02, -4.5277e-02, -8.2070e-03,\n",
              "                        5.7944e-02, -4.8837e-02,  3.9750e-02, -5.5714e-02, -5.9549e-02,\n",
              "                       -2.4219e-02, -1.1902e-01, -3.1229e-02,  1.5815e-01, -3.9533e-02,\n",
              "                       -9.1758e-02,  1.7129e-01, -6.8013e-02,  1.0130e-01, -2.0529e-02,\n",
              "                       -1.1033e-01,  1.0969e-02, -6.3240e-02, -5.6450e-02, -8.5395e-02,\n",
              "                        1.2701e-01,  9.0036e-03, -3.2435e-02, -7.6945e-02, -8.2592e-02,\n",
              "                       -2.5584e-02, -6.5865e-02, -2.7033e-02,  1.0549e-01, -3.0927e-03,\n",
              "                       -2.1332e-02,  2.6748e-02,  5.9155e-03,  4.9219e-02,  4.8728e-02,\n",
              "                       -1.4891e-01,  3.8628e-02, -7.8162e-02, -6.0326e-02, -1.0200e-01,\n",
              "                        5.5639e-02, -7.0571e-02,  4.3074e-02, -9.1965e-02, -1.0990e-01,\n",
              "                       -1.3945e-02,  2.9613e-02, -9.8031e-02, -4.2686e-03, -3.5047e-02,\n",
              "                       -6.4494e-02], device='cuda:0')),\n",
              "              ('hidden_layers.2.weight',\n",
              "               tensor([[ 0.0952, -0.1098, -0.0158,  ...,  0.0455, -0.1168, -0.1151],\n",
              "                       [ 0.0303, -0.0563, -0.0649,  ..., -0.1245,  0.0079,  0.0422],\n",
              "                       [-0.0648, -0.0683, -0.1710,  ..., -0.0961,  0.0869, -0.1000],\n",
              "                       ...,\n",
              "                       [ 0.1148, -0.0643, -0.0671,  ...,  0.0745, -0.1340, -0.1424],\n",
              "                       [ 0.0124,  0.0092,  0.0572,  ...,  0.0908, -0.1307, -0.0396],\n",
              "                       [-0.0670,  0.0591, -0.0606,  ..., -0.0870,  0.0525, -0.1552]],\n",
              "                      device='cuda:0')),\n",
              "              ('hidden_layers.2.bias',\n",
              "               tensor([ 0.0441, -0.0285,  0.0258,  0.0577,  0.0888,  0.0237, -0.0087,  0.1444,\n",
              "                       -0.1229,  0.1679,  0.0812,  0.1040,  0.0262,  0.1062,  0.0565,  0.0484,\n",
              "                       -0.0515, -0.0180,  0.1281,  0.1263,  0.0784,  0.1059,  0.0103,  0.0633,\n",
              "                        0.0447,  0.1475, -0.0317, -0.0489,  0.1795, -0.0138, -0.0076,  0.1313,\n",
              "                        0.0500,  0.1062,  0.1823,  0.2055,  0.1317,  0.1598, -0.0956,  0.0754,\n",
              "                        0.1833,  0.0943,  0.0791,  0.0221, -0.1709,  0.1707,  0.1878,  0.1631,\n",
              "                        0.1731,  0.0689,  0.0448,  0.0077,  0.1455,  0.0042,  0.0538, -0.0455,\n",
              "                       -0.0011, -0.0058,  0.0278,  0.1019, -0.0054,  0.0724, -0.0336,  0.0183,\n",
              "                        0.0916, -0.0522,  0.0356,  0.0041,  0.1211,  0.1056,  0.2086,  0.0745,\n",
              "                        0.0566,  0.0236,  0.0288,  0.0072,  0.0243,  0.1818, -0.0554,  0.0768,\n",
              "                       -0.0189,  0.0747,  0.0736,  0.0099, -0.0576, -0.0885, -0.0117, -0.0743,\n",
              "                       -0.0183,  0.0287, -0.0284, -0.0605, -0.0192,  0.0567,  0.1375,  0.0322,\n",
              "                        0.0454,  0.1218, -0.0459,  0.0110,  0.0594,  0.1364, -0.0424, -0.0212,\n",
              "                        0.0967,  0.2310,  0.0623, -0.0468, -0.0075,  0.0192,  0.0705,  0.0110,\n",
              "                        0.0145,  0.1156,  0.1538,  0.0368, -0.0165,  0.0834,  0.0265, -0.0132,\n",
              "                        0.0448,  0.1834,  0.0723, -0.0007, -0.0800,  0.2118, -0.0794,  0.0183],\n",
              "                      device='cuda:0')),\n",
              "              ('output.weight',\n",
              "               tensor([[ 0.0610,  0.0124,  0.0830,  ..., -0.1250, -0.1381,  0.0349],\n",
              "                       [-0.0081, -0.1278, -0.0438,  ...,  0.0451,  0.0464, -0.1804],\n",
              "                       [ 0.0724,  0.0671, -0.0022,  ..., -0.0025,  0.0783, -0.1075],\n",
              "                       ...,\n",
              "                       [-0.0864, -0.0413, -0.1075,  ..., -0.0719,  0.0842,  0.0620],\n",
              "                       [ 0.0893, -0.0777, -0.0442,  ...,  0.0547, -0.0423, -0.0649],\n",
              "                       [-0.1914, -0.1549, -0.1980,  ..., -0.1907,  0.0900,  0.0981]],\n",
              "                      device='cuda:0')),\n",
              "              ('output.bias',\n",
              "               tensor([ 0.0469, -0.1144, -0.0590, -0.0589,  0.0619, -0.0910, -0.0550, -0.1186,\n",
              "                        0.3296,  0.0731], device='cuda:0'))]),\n",
              " 'Test Accuracy (Overall): ': '96.4% (9640.0/10000.0)',\n",
              " 'Test Accuracy of 1': '98.77551020408163% (968.0/980.0)',\n",
              " 'Test Accuracy of 10': '93.85530227948463% (947.0/1009.0)',\n",
              " 'Test Accuracy of 2': '98.50220264317181% (1118.0/1135.0)',\n",
              " 'Test Accuracy of 3': '98.06201550387597% (1012.0/1032.0)',\n",
              " 'Test Accuracy of 4': '94.35643564356435% (953.0/1010.0)',\n",
              " 'Test Accuracy of 5': '97.45417515274949% (957.0/982.0)',\n",
              " 'Test Accuracy of 6': '93.49775784753363% (834.0/892.0)',\n",
              " 'Test Accuracy of 7': '97.49478079331942% (934.0/958.0)',\n",
              " 'Test Accuracy of 8': '95.62256809338521% (983.0/1028.0)',\n",
              " 'Test Accuracy of 9': '95.89322381930185% (934.0/974.0)',\n",
              " 'TrainingLoss': 0.011585559079064599,\n",
              " 'TrainingLosses': [0.021491792484613446, 0.011585559079064599],\n",
              " 'ValidationLoss ': 0.14347785773531843,\n",
              " 'ValidationLosses ': [0.178966156312963, 0.14347785773531843]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    }
  ]
}