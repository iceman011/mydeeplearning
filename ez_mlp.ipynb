{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPid78dzk3vX+Jud3i5UoaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceman011/mydeeplearning/blob/master/ez_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXy2soVzPQr"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "import datetime\n",
        "import os\n",
        "import pdb\n",
        "import sys\n",
        "import torchvision.models as pretrained_models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "from google.colab import drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "from torch import optim\n",
        "import itertools\n",
        "#!/usr/bin/env python3\n",
        "import mmap\n",
        "import re\n",
        "from itertools import dropwhile, product\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers,output_classes,transform,dataset, drop_p=0.5,lr =0.001, train_on_gpu=False,network_type ='normal'):\n",
        "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Using CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Using GPU ...')\n",
        "        \n",
        "        if(network_type == 'normal'):\n",
        "          print('init Network with Type ',network_type)    \n",
        "          self.input_size = input_size\n",
        "          \n",
        "          # Input to a hidden layer\n",
        "          self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "          \n",
        "          # Add a variable number of more hidden layers\n",
        "          layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "          self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "          \n",
        "          self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "          \n",
        "          self.dropout = nn.Dropout(p=drop_p)\n",
        "\n",
        "        else:\n",
        "          self.init_pre_trained_model(hidden_layers,network_type,output_size)\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.drop_ratio = drop_p\n",
        "        self.output_classes = output_classes\n",
        "        self.transform=transform\n",
        "        self.dataset=dataset\n",
        "        self.network_type = network_type\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "        if(self.network_type == 'normal'):\n",
        "          for each in self.hidden_layers:\n",
        "              x = F.relu(each(x))\n",
        "              x = self.dropout(x)\n",
        "        else:\n",
        "          x = self.classifier(x)\n",
        "          \n",
        "        x = self.output(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def init_pre_trained_model(self,hidden_layers,network_type,output_size):\n",
        "        print('init Network with Type ',network_type)\n",
        "        if ( network_type == 'alexnet'):\n",
        "            model = pretrained_models.alexnet(pretrained=True)\n",
        "            self.input_size = model.classifier[1].in_features\n",
        "        elif ( network_type == 'vgg11'):\n",
        "            model = pretrained_models.vgg11(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg11_bn'):\n",
        "            model = pretrained_models.vgg11_bn(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg13'):\n",
        "            model = pretrained_models.vgg13(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg13_bn'):\n",
        "            model = pretrained_models.vgg13_bn(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg16'):\n",
        "            model = pretrained_models.vgg16(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg16_bn'):\n",
        "            model = pretrained_models.vgg16_bn(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg19'):\n",
        "            model = pretrained_models.vgg19(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'vgg19_bn'):\n",
        "            model = pretrained_models.vgg19_bn(pretrained=True)\n",
        "            self.input_size = model.classifier[0].in_features\n",
        "        elif ( network_type == 'resnet18'):\n",
        "            model = pretrained_models.resnet18(pretrained=True)\n",
        "            self.input_size = model.fc.in_features\n",
        "        elif ( network_type == 'resnet34'):\n",
        "            model = pretrained_models.resnet34(pretrained=True)\n",
        "            self.input_size = model.fc.in_features\n",
        "        elif ( network_type == 'resnet50'):\n",
        "            model = pretrained_models.resnet50(pretrained=True)\n",
        "            self.input_size = model.fc.in_features\n",
        "        elif ( network_type == 'resnet101'):\n",
        "            model = pretrained_models.resnet101(pretrained=True)\n",
        "            self.input_size = model.fc.in_features\n",
        "        elif ( network_type == 'resnet152'):\n",
        "            model = pretrained_models.resnet152(pretrained=True)\n",
        "            self.input_size = model.fc.in_features\n",
        "        elif ( network_type == 'squeezenet1_0'):\n",
        "            model = pretrained_models.squeezenet1_0(pretrained=True)\n",
        "            self.input_size = 512\n",
        "        elif ( network_type == 'squeezenet1_1'):\n",
        "            model = pretrained_models.squeezenet1_1(pretrained=True)\n",
        "            self.input_size = 512\n",
        "        elif ( network_type == 'densenet121'):\n",
        "            model = pretrained_models.densenet121(pretrained=True)\n",
        "            self.input_size = model.classifier.in_features\n",
        "        elif ( network_type == 'densenet169'):\n",
        "            model = pretrained_models.densenet169(pretrained=True)\n",
        "            self.input_size = model.classifier.in_features\n",
        "        elif ( network_type == 'densenet161'):\n",
        "            model = pretrained_models.densenet161(pretrained=True)\n",
        "            self.input_size = model.classifier.in_features\n",
        "        elif ( network_type == 'densenet201'):\n",
        "            model = pretrained_models.densenet201(pretrained=True)\n",
        "            self.input_size = model.classifier.in_features\n",
        "        elif ( network_type == 'inception_v3'):\n",
        "            model = pretrained_models.inception_v3(pretrained=True)\n",
        "            self.input_size = model.fc.in_features\n",
        "\n",
        "            \n",
        "        #print(model)\n",
        "        # here we get all the modules(layers) before the fc layer at the end\n",
        "        # note that currently at pytorch 1.0 the named_children() is not supported\n",
        "        # and using that instead of children() will fail with an error\n",
        "        self.features = nn.ModuleList(model.children())[:-1]\n",
        "\n",
        "        # Now we have our layers up to the fc layer, but we are not finished yet \n",
        "        # we need to feed these to nn.Sequential() as well, this is needed because,\n",
        "        # nn.ModuleList doesnt implement forward() \n",
        "        # so you cant do sth like self.features(images). Therefore we use \n",
        "        # nn.Sequential and since sequential doesnt accept lists, we \n",
        "        # unpack all the items and send them like this\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        # now lets add our new layers \n",
        "        #in_features = model.fc.in_features\n",
        "\n",
        "        hidden_layers = [item for item in hidden_layers]\n",
        "        hidden_layers.insert(0,self.input_size)\n",
        "\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.network_type = network_type\n",
        "        \n",
        "        self.classifier, self.output = self.make_dynamic_layers(hidden_layers,output_size)\n",
        "        \n",
        "        # Freeze parameters so we don't backprop through them\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def make_dynamic_layers(self,layers,n_classes):\n",
        "        layers_block = nn.Sequential(*[\n",
        "                                    nn.Sequential(\n",
        "                                        nn.Linear(in_f, out_f),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout()\n",
        "                                        ) \n",
        "                          for in_f, out_f in zip(layers, layers[1:])])\n",
        "        \n",
        "        output = nn.Linear(layers[-1], n_classes)\n",
        "        \n",
        "        return layers_block,output\n",
        "\n",
        "\"\"\"\n",
        "class PreTrainenModels(nn.Module):\n",
        "      \n",
        "      def __init__(self, input_size, output_size, hidden_layers,output_classes,transform,dataset, drop_p=0.5,lr =0.001, train_on_gpu=False,network_type ='normal'):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if ( network_type == 'resnet18'):\n",
        "          #assert input_size == 1024 ,  'input features of pre-trained networks must bet 1024'\n",
        "          model = pretrained_models.resnet18(pretrained=True)\n",
        "          input_size = model.fc.in_features\n",
        "          \n",
        "        #print(model)\n",
        "        # here we get all the modules(layers) before the fc layer at the end\n",
        "        # note that currently at pytorch 1.0 the named_children() is not supported\n",
        "        # and using that instead of children() will fail with an error\n",
        "        self.features = nn.ModuleList(model.children())[:-1]\n",
        "\n",
        "        # Now we have our layers up to the fc layer, but we are not finished yet \n",
        "        # we need to feed these to nn.Sequential() as well, this is needed because,\n",
        "        # nn.ModuleList doesnt implement forward() \n",
        "        # so you cant do sth like self.features(images). Therefore we use \n",
        "        # nn.Sequential and since sequential doesnt accept lists, we \n",
        "        # unpack all the items and send them like this\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        # now lets add our new layers \n",
        "        in_features = model.fc.in_features\n",
        "\n",
        "        # from now, you can add any kind of layers in any quantity!  \n",
        "        # Here I'm creating two new layers \n",
        "        #self.fc0 = nn.Linear(in_features, 256)\n",
        "        #self.fc0_bn = nn.BatchNorm1d(256, eps = 1e-2)\n",
        "        #self.fc1 = nn.Linear(256, output_size)\n",
        "        #self.fc1_bn = nn.BatchNorm1d(output_size, eps = 1e-2)\n",
        "        \n",
        "        # initialize all fc layers to xavier\n",
        "        #for m in self.modules():\n",
        "        #    if isinstance(m, nn.Linear):\n",
        "        #        torch.nn.init.xavier_normal_(m.weight, gain = 1)\n",
        "        \n",
        "        hidden_layers = [item for item in hidden_layers]\n",
        "        hidden_layers.insert(0,in_features)\n",
        "\n",
        "        self.hidden_layers = hidden_layers\n",
        "        \n",
        "        self.classifier, self.output = classifier_block(hidden_layers,output_size) #MyPreTrainedClassifier(hidden_layers,output_size)\n",
        "        \n",
        "        #self.model = model\n",
        "        #self.model.last_linear= self.classifier\n",
        "        #self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        #self.dropout = nn.Dropout(p=drop_p)\n",
        "\n",
        "        self.classifier = nn.Sequential(OrderedDict([\n",
        "                                  ('fc1', nn.Linear(in_features, 500)),\n",
        "                                  ('relu', nn.ReLU()),\n",
        "                                  ('fc2', nn.Linear(500, output_size)),\n",
        "                                  ('output', nn.LogSoftmax(dim=1))\n",
        "                                  ]))\n",
        "           \n",
        "        #model.classifier = classifier\n",
        "\n",
        "        # Freeze parameters so we don't backprop through them\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Using CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Using GPU ...')\n",
        "            \n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.drop_ratio = drop_p\n",
        "        self.output_classes = output_classes\n",
        "        self.transform=transform\n",
        "        self.dataset=dataset\n",
        "        self.network_type= network_type\n",
        "\n",
        "\n",
        "      def get_dense_net(self):        \n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = None\n",
        "\n",
        "        if ( self.network_type == 'densenet121'):\n",
        "          assert self.input_size == 1024 ,  'input features of pre-trained networks must bet 1024'\n",
        "          model = pretrained_models.densenet121(pretrained=True)\n",
        "\n",
        "        # Freeze parameters so we don't backprop through them\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        model.classifier = nn.Sequential(nn.Linear(self.input_size, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(self.drop_ratio),\n",
        "                                        nn.Linear(256, self.output_size),\n",
        "                                        nn.LogSoftmax(dim=1))\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # Only train the classifier parameters, feature parameters are frozen\n",
        "        optimizer = optim.Adam(model.classifier.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        model.to(device)\n",
        "        \n",
        "        return model , criterion , optimizer\n",
        "\n",
        "\n",
        "      def forward(self, x):\n",
        "        #return self.model(x)\n",
        "       # now in forward pass, you have the full control, \n",
        "       # we can use the feature part from our pretrained model  like this\n",
        "        #x = self.features(x)\n",
        "\n",
        "        # since we are using fc layers from now on, we need to flatten the output.\n",
        "        # we used the avgpooling but we still need to flatten from the shape (batch, 1,1, features)\n",
        "        # to (batch, features) so we reshape like this. input_imgs.size(0) gives the batchsize, and \n",
        "        # we use -1 for inferring the rest\n",
        "        #output = output.view(input_imgs.size(0), -1)\n",
        "       # and also our new layers. \n",
        "        #output = self.fc0_bn(F.relu(self.fc1(output)))\n",
        "        #output = self.fc1_bn(F.relu(self.fc1(output)))\n",
        "        \n",
        "        x = self.classifier(x)\n",
        "\n",
        "        #x = F.relu(self.classifier.fc1(x))\n",
        "        #x = self.dropout(x)\n",
        "        #x = F.relu(self.classifier.fc2(x))\n",
        "        #x = self.dropout(x)\n",
        "        #x = self.classifier.output(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "        #return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyPreTrainedClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, classifier_sizes, n_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(*[classifier_block(in_f, out_f) \n",
        "                       for in_f, out_f in zip(classifier_sizes, classifier_sizes[1:])])\n",
        "        self.output = nn.Linear(classifier_sizes[-1], n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "        \n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def forward_last(self, x):\n",
        "      x = self.classifier(x)\n",
        "      x = self.output(x)\n",
        "\n",
        "      for each in self.hidden_layers:\n",
        "        x = F.relu(each(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "      \n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "######################    \n",
        "# LOADING DATA #\n",
        "######################\n",
        "def LoadData(datasetName,batch_size=20,valid_size = 0.2,network_type='normal'):\n",
        "      \n",
        "    # number of subprocesses to use for data loading\n",
        "    num_workers = 0\n",
        "\n",
        "    # convert data to torch.FloatTensor\n",
        "    #transform = transforms.ToTensor()\n",
        "\n",
        "    # convert data to a normalized torch.FloatTensor\n",
        "    transform = None\n",
        "    train_data= None\n",
        "    valid_data= None\n",
        "    test_data=None\n",
        "    classes=None\n",
        "    normalize = None\n",
        "\n",
        "    if network_type != 'normal' :\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    elif network_type == 'normal' and datasetName == 'MINST' :\n",
        "      normalize = transforms.Normalize((0.5, ), (0.5, ))\n",
        "    elif network_type == 'normal' and datasetName == 'CIFAR' :\n",
        "      normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    if(datasetName == 'MINST'):\n",
        "       # convert data to a normalized torch.FloatTensor\n",
        "      train_transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          #transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),          \n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "\n",
        "      valid_transform = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "\n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.MNIST(root='data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "      valid_data = datasets.MNIST(root='data', train=False,\n",
        "                                        download=True, transform=valid_transform)\n",
        "      test_data = datasets.MNIST(root='data', train=False,\n",
        "                                        download=True, transform=valid_transform)\n",
        "      # specify the image classes\n",
        "      classes = ['1', '2', '3', '4', '5',\n",
        "           '6', '7', '8', '9', '10']\n",
        "    elif(datasetName == 'CIFAR'):\n",
        "      \n",
        "      # convert data to a normalized torch.FloatTensor\n",
        "      valid_transform = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "      \n",
        "      train_transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "      valid_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                        download=True, transform=valid_transform)\n",
        "      test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                        download=True, transform=valid_transform)\n",
        "      # specify the image classes\n",
        "      classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "        sampler=valid_sampler, num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "        num_workers=num_workers)\n",
        "    return train_loader , valid_loader ,test_loader ,classes , transform\n",
        "\n",
        "\n",
        "\n",
        "#############################\n",
        "# VALIDATE MODEL\n",
        "############################\n",
        "def validation(model, validationloader, criterion):\n",
        "    accuracy = 0\n",
        "    validation_loss = 0\n",
        "    validate_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : ','Starting Validation....')\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if model.train_on_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "      for images, labels in validationloader:\n",
        "\n",
        "          if model.train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "          \n",
        "          images = images.resize_(images.size()[0], model.input_size)\n",
        "          output = model.forward(images)\n",
        "          validation_loss += criterion(output, labels).item()\n",
        "\n",
        "          ## Calculating the accuracy \n",
        "          # Model's output is log-softmax, take exponential to get the probabilities\n",
        "          ps = torch.exp(output)\n",
        "          # Class with highest probability is our predicted class, compare with true label\n",
        "          #equality = (labels.data == ps.max(1)[1])\n",
        "          # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "          #accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "\n",
        "\n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))            \n",
        "          print('accuracy ',accuracy )\n",
        "        \n",
        "      validation_loss = validation_loss/len(validationloader)\n",
        "      accuracy = 100. * accuracy/len(validationloader)\n",
        "    \n",
        "    print('Finished Validation In ',datetime.timedelta(seconds = time.time() - validate_start_time) )\n",
        "    return validation_loss, accuracy\n",
        "\n",
        "#############################\n",
        "# TEST MODEL\n",
        "############################\n",
        "def test(model,test_loader,criterion,checkpoint,outputfilepath,batch_size,override_checkpoint):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(model.output_size))\n",
        "    class_total = list(0. for i in range(model.output_size))\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Testing....')\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      model.eval()\n",
        "      # iterate over test data\n",
        "      for data, target in test_loader:\n",
        "          # move tensors to GPU if CUDA is available\n",
        "          if model.train_on_gpu:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          \n",
        "          # Flatten images into a 784 long vector\n",
        "          data.resize_(data.size()[0], model.input_size)\n",
        "\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the batch loss\n",
        "          loss = criterion(output, target)\n",
        "          # update test loss \n",
        "          test_loss += loss.item()*data.size(0)\n",
        "          # convert output probabilities to predicted class\n",
        "          _, pred = torch.max(output, 1)    \n",
        "          # compare predictions to true label\n",
        "          correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "          correct = np.squeeze(correct_tensor.numpy()) if not model.train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "          # calculate test accuracy for each object class\n",
        "          for i in range(batch_size):\n",
        "              label = target.data[i]\n",
        "              class_correct[label] += correct[i].item()\n",
        "              class_total[label] += 1\n",
        "\n",
        "      # average test loss\n",
        "      print('Finished Testing during ',datetime.timedelta(seconds=time.time() - test_start_time))\n",
        "      test_loss = test_loss/len(test_loader.dataset)\n",
        "      print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "      \n",
        "      test_checkpoint = dict()\n",
        "\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update({'TestLoss': test_loss})\n",
        "      else :\n",
        "        test_checkpoint.update({'TestLoss': test_loss})\n",
        "\n",
        "      for i in range(model.output_size):\n",
        "          if class_total[i] > 0:\n",
        "            current_key  = 'Test Accuracy of '+model.output_classes[i]\n",
        "            current_val = '{:.3f}% ({}/{})'.format(100 *( class_correct[i] / class_total[i]),\n",
        "                  np.sum(class_correct[i]), np.sum(class_total[i]))\n",
        "            ele={current_key:current_val}\n",
        "            if(override_checkpoint):\n",
        "              checkpoint.update(ele)\n",
        "              print(current_key,current_val)\n",
        "            else:\n",
        "              test_checkpoint.update(ele)\n",
        "\n",
        "\n",
        "          \"\"\"else:\n",
        "              print('Test Accuracy of %5s: N/A (no training examples)' % (model.output_classes[i]))\n",
        "          \"\"\"\n",
        "\n",
        "      current_key = 'Test Accuracy (Overall): '#.format(100. * np.sum(class_correct) / np.sum(class_total))\n",
        "      current_val = '{:.3f}% ({}/{})'.format(100 * (np.sum(class_correct) / np.sum(class_total)),\n",
        "          np.sum(class_correct), np.sum(class_total) )\n",
        "      ele={current_key:current_val}\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update(ele)      \n",
        "        print(current_key,current_val)\n",
        "      else:\n",
        "        test_checkpoint.update(ele)\n",
        "        test_checkpoint.update({'Detailed ':checkpoint})\n",
        "\n",
        "    \n",
        "    #model.train()\n",
        "    if(override_checkpoint):\n",
        "      print('Overriding Checkpoint')\n",
        "      torch.save(checkpoint,outputfilepath)\n",
        "\n",
        "    return checkpoint , test_checkpoint\n",
        "\n",
        "#############################\n",
        "# TRAIN MODEL\n",
        "############################\n",
        "def train(model, trainloader, validationloader, criterion, optimizer, uploadToGDrive,checkpointPath,PreviousCheckPointId,\n",
        "          PreviousValidationLoss,start_time,exp_id,epochs=5, print_every=40):\n",
        "    # monitor training loss    \n",
        "    steps = 0    \n",
        "    #start_time = time.time()\n",
        "    #dateTimeObj = datetime.now()\n",
        "    #start_time_timestamp = './results/'+dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    #start_time_timestamp = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    train_losses, valid_losses = [], []\n",
        "    checkpoint = dict()\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training using Model Parameters \\n '+str(model)+' \\n from PreviousModel '+ PreviousCheckPointId + ' with validationLoss '+ str(PreviousValidationLoss) )\n",
        "    valid_loss_min = PreviousValidationLoss #np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "    for e in range(epochs):        \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training of Epoch'+str(e)  )\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            \n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if model.train_on_gpu:\n",
        "              images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Flatten images into a 784 long vector\n",
        "            images.resize_(images.size()[0], model.input_size)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        train_losses.append(train_loss)\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # Model in inference mode, dropout is off\n",
        "        model.eval()\n",
        "        print(' Finished Training of Epoch '+str(e),' In ',datetime.timedelta(seconds = time.time() - epoch_start_time) )\n",
        "\n",
        "        # Turn off gradients for validation, will speed up inference\n",
        "        with torch.no_grad():\n",
        "            valid_loss, accuracy = validation(model, validationloader, criterion)\n",
        "        \n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print('{} : Epoch: {} \\tTraining Loss: {:.9f} \\tValidation Loss: {:.9f} \\tAccuracy : {:.6f}'.format(\n",
        "            dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"),\n",
        "            e, \n",
        "            train_loss,\n",
        "            valid_loss,\n",
        "            accuracy\n",
        "            ))\n",
        "        \n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.9f} --> {:.9f} ,)  Accuracy: {:.6f}  TimeElapsed: {}.  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss,\n",
        "            accuracy,\n",
        "            (datetime.timedelta(seconds = time.time() - start_time))))\n",
        "            \n",
        "            checkpoint = {'InputSize': model.input_size,\n",
        "                  'OutputSize': model.output_size,\n",
        "                  'HiddenLayers': [each.out_features for each in model.hidden_layers] if model.network_type == 'normal' else model.hidden_layers[1:], \n",
        "                  'LearningRate':model.learning_rate,\n",
        "                  'DropRatio':model.drop_ratio,\n",
        "                  'TrainingLoss' :train_loss,\n",
        "                  'ValidationLoss':valid_loss,\n",
        "                  'ValidationAccuracy':accuracy,\n",
        "                  'ElapsedTime': datetime.timedelta(seconds = time.time() - start_time),\n",
        "                  'Dataset':model.dataset,\n",
        "                  'LastEpoch': e,\n",
        "                  'PreviousCheckPoint': PreviousCheckPointId,\n",
        "                  'GPUState': model.train_on_gpu,                  \n",
        "                  'OutputFolder' : (exp_id+'/'+ model.network_type) if model.network_type != 'normal' else exp_id,\n",
        "                  'CheckPointTimestamp': dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"), #time.time(),\n",
        "                  'OutputFilePrefix' : 'checkpoint_',\n",
        "                  'OutputClasses': model.output_classes,\n",
        "                  'NetworkType' : model.network_type,\n",
        "                  'Transforms': model.transform,\n",
        "                  'TrainingLosses' :train_losses,\n",
        "                  'ValidationLosses':valid_losses,\n",
        "                  'StateDictionay': model.state_dict()}\n",
        "            \n",
        "            save_model(checkpoint,uploadToGDrive,checkpointPath)\n",
        "            valid_loss_min = valid_loss\n",
        "        \n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return train_losses , valid_losses , checkpoint\n",
        "\n",
        "\n",
        "#############################\n",
        "# SAVE MODEL TO GOOGLE DRIVE\n",
        "############################\n",
        "def save_model(checkpoint,uploadToGDrive,checkpointPath):\n",
        "  \n",
        "  file_path = ''\n",
        "  if(uploadToGDrive):\n",
        "    drive.mount('/content/gdrive')\n",
        "    file_path = checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)      \n",
        "    #path = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])+'.pt'\n",
        "    print('Saving Model to ',file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "  else:\n",
        "    file_path=checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "    print('Saving Model to ',file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "\n",
        "\n",
        "###########################\n",
        "# SKIP CERTAIN ENTERIES FROM ITERTOOLS.PRODUCT\n",
        "#####################33\n",
        "def resume(iterable, sentinel):\n",
        "    yield from dropwhile(lambda x: x != sentinel, iterable)\n",
        "\n",
        "#############################\n",
        "# LOAD LAST EXPERIMENT PARAMS\n",
        "##################################\n",
        "def load_last_exp_param(logPath,checkpointPath):\n",
        "\n",
        "  #with open(checkpointPath+'/3-3-2020.txt') as f:\n",
        "  #    total = f.read()\n",
        "  #    print( total.count('Starting New') )\n",
        "      \n",
        "  f = open(logPath, 'r')\n",
        "  content = f.read()\n",
        "\n",
        "  network_type = re.findall('Network Type: \\{(.+?)\\}', content)\n",
        "  network_type = network_type[len(network_type)-1]\n",
        "\n",
        "  hidden_layers = re.findall('hidden_layers: \\{(.+?)\\}', content)\n",
        "  hidden_layers = int(hidden_layers[len(hidden_layers)-1])\n",
        "\n",
        "  hidden_layer_width = re.findall('hidden_layer_width: \\{(.+?)\\}', content)\n",
        "  hidden_layer_width = hidden_layer_width[len(hidden_layer_width)-1]\n",
        "\n",
        "  Learning_rate = re.findall('Learning_Rate: \\{(.+?)\\}', content)\n",
        "  Learning_rate = float(Learning_rate[len(Learning_rate)-1])\n",
        "\n",
        "  drop_ratio = re.findall('drop_ratio: \\{(.+?)\\}', content)\n",
        "  drop_ratio = float(drop_ratio[len(drop_ratio)-1])\n",
        "  prev_valid_loss = np.Inf\n",
        "  return_checkpoint = None\n",
        "  return_model = None\n",
        "\n",
        "  #for filename in os.listdir(checkpointPath):\n",
        "  for root, dirs, files in os.walk(checkpointPath):\n",
        "    for filename in files:\n",
        "      model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "      if( float(checkpoint['ValidationLoss']) < prev_valid_loss ):\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "      \n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      hidden_layer_width_local = hidden_layer_width.strip()\n",
        "      hidden_layer_width_local = hidden_layer_width_local[1:len(hidden_layer_width_local)-1]\n",
        "      hidden_layer_width_local = list(map(int, hidden_layer_width_local.split(',')))\n",
        "\n",
        "      if( network_type == checkpoint['NetworkType'] and hidden_layer_width_local == checkpoint['HiddenLayers'] and float(checkpoint['ValidationLoss']) <= prev_valid_loss ):\n",
        "        print(' Found Matching Checkpoint with Last Experiement Parame ValidationLoss:{} hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} network_type:{} '.format(checkpoint['ValidationLoss'],hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,checkpoint['NetworkType']))\n",
        "        return_checkpoint = checkpoint\n",
        "        return_model = model\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "        #return checkpoint,model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "  if( not bool(return_checkpoint)): \n",
        "    print(' Not Found Matching Checkpoint with Last Experiement Parameters hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} network_type:{} '.format(hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,network_type))    \n",
        "\n",
        "  return return_checkpoint,return_model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss,network_type\n",
        "\n",
        "\n",
        "################################\n",
        "# TUNE NETWORK LAYERS AND NO. OF NODES\n",
        "##################################\n",
        "def tune_train_network(dataset,epochs,resumeExp=False,resume_logPath='',resume_checkpointPath='',network_type='normal',nodes_per_layer=[2048, 1024, 512, 256, 128, 64, 32],full_learning_rates= [0.0001, 0.001],full_drop_ratios= [0.8, 0.5, 0.3, 0.1]):\n",
        "\n",
        "  start_time = time.time()\n",
        "  dateTimeObj = dt.now()\n",
        "  exp_id = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  \n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "  \n",
        "  checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  \n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  global_train_loss = None\n",
        "  global_valid_loss = None\n",
        "  \n",
        "  global_train_loss , global_valid_loss, min_train_loss , min_valid_loss = plotLossTrendAll(resume_checkpointPath,True,global_train_loss,global_valid_loss)\n",
        "\n",
        "  print('Starting Tunning With MinTrainLoss:', min_train_loss, ' MinValidLoss:',min_valid_loss)\n",
        "\n",
        "  #Load Data\n",
        "  drive.mount('/content/gdrive')\n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size,network_type)\n",
        "\n",
        "  #nodes_per_layer =  [2048, 1024, 512, 256, 128, 64, 32]\n",
        "  #full_learning_rates= [0.0001, 0.001] #[0.1, 0.01, 0.0001, 0.001]\n",
        "  #full_drop_ratios= [0.8, 0.5, 0.3, 0.1]\n",
        "  full_pre_trained_models = ['vgg11','vgg11_bn','vgg13','vgg13_bn','vgg16','vgg16_bn','vgg19','vgg19_bn',\n",
        "                             'resnet18','resnet34','resnet50','resnet101','resnet152',\n",
        "                             'squeezenet1_0','squeezenet1_1',\n",
        "                             'densenet121','densenet169','densenet161','densenet201',\n",
        "                             'inception_v3',\n",
        "                             'alexnet'\n",
        "                            ] if network_type != 'normal' else ['normal']\n",
        "  learning_rates = None\n",
        "  drop_ratios = None\n",
        "  pre_trained_models = None\n",
        "  PreviousValidationLoss= min_valid_loss #np.Inf\n",
        "  max_hidden_layers = 4\n",
        "  iter_hidden_layer_nodes = None\n",
        "  iter_hidden_layer_nodes_list = None\n",
        "  model = None\n",
        "  resume_checkpoint = None\n",
        "\n",
        "  if(resumeExp):\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Resuming Experiment With New ID ',exp_id )\n",
        "    resume_checkpoint,model,resum_hidden_layers,resume_hidden_layer_width,resume_Learning_rate,resume_drop_ratio,resume_prev_valid_loss,resume_network_type =  load_last_exp_param(resume_logPath,resume_checkpointPath)\n",
        "    max_hidden_layers = resum_hidden_layers  \n",
        "\n",
        "    learning_rates = full_learning_rates[full_learning_rates.index(resume_Learning_rate):]\n",
        "    drop_ratios = full_drop_ratios[full_drop_ratios.index(resume_drop_ratio):]\n",
        "    pre_trained_models = full_pre_trained_models[full_pre_trained_models.index(resume_network_type):]\n",
        "\n",
        "    PreviousValidationLoss = resume_prev_valid_loss\n",
        "    checkpointPath = resume_checkpointPath\n",
        "\n",
        "      #iter_hidden_layer_nodes = resume(itertools.product(nodes_per_layer, repeat=hidden_layers+1), resume_checkpoint['HiddenLayers'])      \n",
        "    #if(bool(resume_checkpoint)):\n",
        "    #  model , resume_checkpoint , filepath = load_checkpoint(checkpointPath,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')])\n",
        "      \n",
        "\n",
        "  global_counter = 0\n",
        "  #VARY HIDDEN LAEYERS\n",
        "  for hidden_layers in range(max_hidden_layers,0,-1):\n",
        "    #VARY HIDDEN LAYER NODES/WIDTH\n",
        "    if(resumeExp):\n",
        "      \n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat= hidden_layers)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      resume_hidden_layer_width_list = tuple(list(map(int, resume_hidden_layer_width.split(','))))\n",
        "      iter_hidden_layer_nodes_list = iter_hidden_layer_nodes_list[iter_hidden_layer_nodes_list.index(resume_hidden_layer_width_list):]\n",
        "      \n",
        "      print('Resume ValidationLoss ',PreviousValidationLoss,'Resume Learning Rate Array ',learning_rates ,' Resume Drop Ratio List ',drop_ratios , ' Resume Hidden Nodes List with Size :',len(iter_hidden_layer_nodes_list) , ' Starting From :',iter_hidden_layer_nodes_list[0],' resume Pretrained Model with Size: ',len(pre_trained_models), ' Starting From: ',pre_trained_models[0] )\n",
        "    else:      \n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat=hidden_layers)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "    for hidden_layer in iter_hidden_layer_nodes_list:\n",
        "      #VARY LEARNING RATE\n",
        "      if( not resumeExp):\n",
        "        learning_rates = full_learning_rates\n",
        "      for lr in learning_rates :\n",
        "        #VARY DROP RATIO\n",
        "        if( not resumeExp):\n",
        "          drop_ratios = full_drop_ratios\n",
        "        for drop_ratio in drop_ratios :\n",
        "          if( not resumeExp):\n",
        "            pre_trained_models = full_pre_trained_models\n",
        "          for pre_trained_model in pre_trained_models :\n",
        "            print('Start Global Counter: '+str(global_counter)+' Network Type:'+pre_trained_model+' EXP-ID:'+exp_id+'  DurationSinceStart :',datetime.timedelta(seconds = time.time() - start_time),' with hidden_layers:',hidden_layers,' hidden_layer_width:',hidden_layer ,' Learning_Rate:',lr, ' drop_ratio:',drop_ratio )\n",
        "            #model = Network(784, 10, [first_layer, second_layer, third_layer ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "\n",
        "            if( not bool(resume_checkpoint) or not resumeExp ):\n",
        "              #intput_size = (28*28) if network_type == 'normal' and dataset == 'MINST' else (32*32*3) if network_type == 'normal' and dataset == 'CIFAR' else 1024 # 1024 is defult for pretrained networks\n",
        "              input_size = 0\n",
        "              if network_type == 'normal' and dataset == 'MINST':\n",
        "                input_size = (28*28) \n",
        "              elif network_type == 'normal' and dataset == 'CIFAR' :\n",
        "                input_size = (32*32*3)  \n",
        "              #elif network_type != 'normal' :\n",
        "              #input_size = 1024 # 1024 is defult for pretrained networks\n",
        "\n",
        "              #if network_type == 'normal':\n",
        "              model = Network(input_size, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=pre_trained_model)\n",
        "              #else:\n",
        "              #  model = PreTrainenModels(input_size, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "\n",
        "            criterion = nn.NLLLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "              #if(dataset == 'MINST'):\n",
        "              #  model = Network(784, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "              #else:\n",
        "              #  model = Network((32*32*3), 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "\n",
        "            #Upload checkpoint to local drive\n",
        "            #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "            \n",
        "            train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',PreviousValidationLoss,start_time,exp_id,epochs=epochs)\n",
        "            if( bool(checkpointt) and checkpointt['ValidationLoss'] < PreviousValidationLoss ):\n",
        "              PreviousValidationLoss = checkpointt['ValidationLoss']\n",
        "            \n",
        "            print('Finished Global Counter: '+str(global_counter)+' %Network Type: {'+pre_trained_model+'}% EXP-ID : '+exp_id+'  DurationSinceStart :',datetime.timedelta(seconds = time.time() - start_time),' with %hidden_layers: {',hidden_layers,'}% %hidden_layer_width: {',hidden_layer ,'}% %Learning_Rate: {',lr, '}% %drop_ratio: {',drop_ratio,'}%')\n",
        "            global_counter+= 1\n",
        "\n",
        "            #Plot current experiement training vs validation losses\n",
        "            plotLossTrend(train_lossess,valid_lossess, mode='Single')\n",
        "\n",
        "            #Plot global training vs validation losses\n",
        "            key = dataset + '_' + exp_id + '_' + (str(checkpointt['LastEpoch']) if bool(checkpointt) else str(epochs))            \n",
        "            \n",
        "            # get minimal loss here , as may be during certain experiment its loss is not less than global loss\n",
        "            min_loss = np.Inf\n",
        "            for curr_loss in train_lossess:\n",
        "              if(curr_loss < min_loss):\n",
        "                min_loss = curr_loss\n",
        "            ele = {key: min_loss}\n",
        "            global_train_loss.append(ele)\n",
        "            \n",
        "\n",
        "            ele = {}\n",
        "            min_loss = np.Inf\n",
        "            for curr_loss in valid_lossess:\n",
        "              if(curr_loss < min_loss):\n",
        "                min_loss = curr_loss\n",
        "            ele = {key: min_loss}\n",
        "            global_valid_loss.append(ele)\n",
        "\n",
        "            global_train_loss , global_valid_loss, min_train_loss , min_valid_loss = plotLossTrendAll(resume_checkpointPath,False,global_train_loss,global_valid_loss)\n",
        "          \n",
        "          #pdb.set_trace()\n",
        "          resumeExp=False\n",
        "\n",
        "#######################\n",
        "# PLOT LOSS TREND OF TRAINING & VALIDATON OF ALL EXP\n",
        "##############################3\n",
        "def plotLossTrendAll(OutputFolder,init,train_loss,valid_loss):\n",
        "\n",
        "    #OutputFolder = './results/'\n",
        "    file_epoch = 0\n",
        "    result_list=[]\n",
        "    print('Starting Plotting Loss Trend Across All Experiements ....')\n",
        "    training_loss=[]\n",
        "    validation_loss=[]\n",
        "    ele = dict()\n",
        "    min_train = np.Inf\n",
        "    min_valid = np.Inf\n",
        "\n",
        "    \"\"\"\n",
        "        for folder in os.listdir(OutputFolder):\n",
        "          for filename in os.listdir(OutputFolder+folder):\n",
        "            data = filename.split('_')\n",
        "            #pdb.set_trace()\n",
        "            print(data)\n",
        "            file_epoch=int(data[1][:-3])\n",
        "\n",
        "            model , checkpoint , filepath = load_checkpoint(OutputFolder+folder,OutputFilePrefix,False,file_epoch)\n",
        "    \"\"\"\n",
        "    if(init):\n",
        "      for root, dirs, files in os.walk(OutputFolder):\n",
        "        for filename in files:\n",
        "          model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=False)\n",
        "\n",
        "          key = checkpoint['Dataset'] + '_' + checkpoint['OutputFolder'] + '_' + str(checkpoint['LastEpoch'])\n",
        "          ele = {key: checkpoint['TrainingLoss']}\n",
        "          training_loss.append(ele)\n",
        "          \n",
        "          ele = {}\n",
        "          ele ={key: checkpoint['ValidationLoss']}\n",
        "          validation_loss.append(ele)\n",
        "\n",
        "          if(checkpoint['ValidationLoss'] < min_valid ):\n",
        "            min_valid = checkpoint['ValidationLoss']\n",
        "          if(checkpoint['TrainingLoss'] < min_train ):\n",
        "            min_train = checkpoint['TrainingLoss']\n",
        "            \n",
        "    else:\n",
        "      training_loss = train_loss\n",
        "      validation_loss = valid_loss\n",
        "        \n",
        "    #print('Result List',result_list)\n",
        "    plotLossTrend (training_loss,validation_loss,mode='Multiple')\n",
        "\n",
        "    return training_loss,validation_loss, min_train, min_valid \n",
        "      \n",
        "\n",
        "#############################\n",
        "# PLOT TRAINING LOSS VS VALIDATION LOSS \n",
        "############################\n",
        "def plotLossTrend(train_losses,validation_losses,test_losses=[],mode='Single'):\n",
        "  #TRAINING LOSS DATA\n",
        "  values = []\n",
        "  labels = []\n",
        "  \n",
        "  if(mode !='Single'):\n",
        "    for item in train_losses:    \n",
        "      for key, value in item.items():\n",
        "        values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "        labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "  else:\n",
        "    for epoch in range(len(train_losses)):    \n",
        "      values.append(\"{0:.5f}\".format(train_losses[epoch])) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(epoch) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "    #values = train_losses\n",
        "    #labels = train_losses\n",
        "\n",
        "  plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Training Loss',linestyle='dashed')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #VALIDATON LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  if(mode !='Single'):\n",
        "    for item in validation_losses:   \n",
        "      #pdb.set_trace() \n",
        "      for key, value in item.items():      \n",
        "        values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "        labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "  else:\n",
        "    for epoch in range(len(validation_losses)):    \n",
        "      values.append(\"{0:.5f}\".format(validation_losses[epoch])) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(epoch) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Validation Loss',marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #TEST LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in test_losses:    \n",
        "    for key, value in item.items():\n",
        "      #pdb.set_trace()\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "  if(bool(test_losses)):\n",
        "    plt.title(\"Training vs Validation vs Testing\")\n",
        "  else:\n",
        "    plt.title(\"Training vs Validation\")\n",
        "  if( mode == 'Single' ):\n",
        "    plt.xlabel(\"Epochs\")\n",
        "  else:\n",
        "    plt.xlabel(\"Expriements\")\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Test Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "\n",
        "  plt.ylabel(\"Loss\")  \n",
        "  #plt.ylim((0,1.))\n",
        "\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  #for i, v in enumerate(values):\n",
        "  #    ax.text(i, v+25, \"%d\" %v, ha=\"center\")\n",
        "  #plt.ylim(-10, 595)\n",
        "\n",
        "#############################\n",
        "# LOAD CHECKPOINT\n",
        "############################\n",
        "def load_checkpoint(OutputFolder,OutputFilePrefix,max_epoch,exact_epoch,load_model=True):\n",
        "    \n",
        "    file_epoch = 0\n",
        "    if(max_epoch):\n",
        "      for filename in os.listdir(OutputFolder):\n",
        "        data = filename.split('_')\n",
        "        tmp=int(data[1][:-3])\n",
        "        #print('data[1]',data[1],' tmp[:-3] ',tmp[:-3])\n",
        "        if( file_epoch <= tmp ):\n",
        "          file_epoch = tmp\n",
        "    else:\n",
        "      file_epoch=exact_epoch\n",
        "\n",
        "    filepath = OutputFolder+'/'+OutputFilePrefix + str(file_epoch)+'.pt'\n",
        "    print('Loading Checkpoint from '+filepath+'...')\n",
        "    indent=1\n",
        "    \n",
        "    cpu_or_gpu = torch.cuda.is_available()\n",
        "    checkpoint = None\n",
        "\n",
        "    if not cpu_or_gpu :\n",
        "      checkpoint = torch.load(filepath,map_location='cpu')\n",
        "    else:\n",
        "      checkpoint = torch.load(filepath)\n",
        "\n",
        "    for key, value in checkpoint.items():\n",
        "        if(key == 'StateDictionay'):\n",
        "          continue\n",
        "        print('\\t' * indent + str(key),'\\t' * (indent+1) + str(value))\n",
        "        #print('\\t' * (indent+1) + str(value))\n",
        "    \n",
        "    testmodel = None\n",
        "\n",
        "    if(load_model ):\n",
        "      testmodel = Network(    checkpoint['InputSize'],\n",
        "                              checkpoint['OutputSize'],\n",
        "                              checkpoint['HiddenLayers'],\n",
        "                              checkpoint['OutputClasses'],\n",
        "                              checkpoint['Transforms'],\n",
        "                              checkpoint['Dataset'],\n",
        "                              checkpoint['DropRatio'],\n",
        "                              checkpoint['LearningRate'],\n",
        "                              train_on_gpu=checkpoint['GPUState'],\n",
        "                              network_type=checkpoint['NetworkType']\n",
        "                      )\n",
        "      testmodel.load_state_dict(checkpoint['StateDictionay'])\n",
        "    \"\"\"elif(load_model and checkpoint['NetworkType'] !='normal' ):\n",
        "      testmodel = PreTrainenModels(    checkpoint['InputSize'],\n",
        "                              checkpoint['OutputSize'],\n",
        "                              checkpoint['HiddenLayers'],\n",
        "                              checkpoint['OutputClasses'],\n",
        "                              checkpoint['Transforms'],\n",
        "                              checkpoint['Dataset'],\n",
        "                              checkpoint['DropRatio'],\n",
        "                              checkpoint['LearningRate'],\n",
        "                              train_on_gpu=checkpoint['GPUState'],\n",
        "                              network_type=checkpoint['NetworkType']\n",
        "                      )\n",
        "    \n",
        "    print(testmodel.state_dict().keys())\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return testmodel , checkpoint , filepath\n",
        "\n",
        "\n",
        "def test_all_epochs(OutputFolder,test_loader,criterion,batch_size,OutputFilePrefix='checkpoint_'):\n",
        "\n",
        "    #OutputFolder = './results/'\n",
        "    file_epoch = 0\n",
        "    result_list=[]\n",
        "    print('Starting Test All Saved Epochs ....')\n",
        "    trainig_loss=[]\n",
        "    validation_loss=[]\n",
        "    test_loss=[]\n",
        "    ele = dict()\n",
        "\n",
        "    \"\"\"\n",
        "        for folder in os.listdir(OutputFolder):\n",
        "          for filename in os.listdir(OutputFolder+folder):\n",
        "            data = filename.split('_')\n",
        "            #pdb.set_trace()\n",
        "            print(data)\n",
        "            file_epoch=int(data[1][:-3])\n",
        "\n",
        "            model , checkpoint , filepath = load_checkpoint(OutputFolder+folder,OutputFilePrefix,False,file_epoch)\n",
        "    \"\"\"\n",
        "    #for filename in os.listdir(checkpointPath):\n",
        "    for root, dirs, files in os.walk(OutputFolder):\n",
        "      for filename in files:\n",
        "        model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "\n",
        "        checkpoint , checkpoint_test = test(model,test_loader,criterion,checkpoint,filepath,batch_size,False)\n",
        "\n",
        "        print('Test Accuracy ',checkpoint_test['Test Accuracy (Overall): '])\n",
        "        result_list.append(checkpoint_test)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['TrainingLoss'])\n",
        "        key = checkpoint['Dataset'] + '_' + checkpoint['OutputFolder'] + '_' + str(checkpoint['LastEpoch'])\n",
        "        ele = {key: checkpoint['TrainingLoss']}\n",
        "        trainig_loss.append(ele)\n",
        "        \n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['ValidationLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint['ValidationLoss']}\n",
        "        validation_loss.append(ele)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint_test['TestLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint_test['TestLoss']}\n",
        "        test_loss.append(ele)\n",
        "    \n",
        "    #print('Result List',result_list)\n",
        "    plotLossTrend (trainig_loss,validation_loss,test_loss)\n",
        "    return result_list\n",
        "      \n",
        "\n",
        "#############################\n",
        "# VISUALIZE ALL IMAGES IN BATCH\n",
        "############################\n",
        "def visualize_images_in_batch(loader,batch_size) :\n",
        "    # obtain one batch of training images\n",
        "\t\tdataiter = iter(loader)\n",
        "\t\timages, labels = dataiter.next()\n",
        "\t\timages = images.numpy()\n",
        "\n",
        "\t\t# plot the images in the batch, along with the corresponding labels\n",
        "\t\tfig = plt.figure(figsize=(25, 4))\n",
        "\t\tfor idx in np.arange(batch_size):\n",
        "\t\t\tax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\t\t\tax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "\t\t\t# print out the correct label for each image\n",
        "\t\t\t# .item() gets the value contained in a Tensor\n",
        "\t\t\tax.set_title(str(labels[idx].item()))\n",
        "\t\treturn images\n",
        "\n",
        "#############################\n",
        "# VISUALIZE ALL RGB IMAGES IN BATCH\n",
        "############################\n",
        "def visualize_rgb_images_in_batch(train_loader):\n",
        "  # obtain one batch of training images\n",
        "  dataiter = iter(train_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images = images.numpy() # convert images to numpy for display\n",
        "  # plot the images in the batch, along with the corresponding labels\n",
        "  fig = plt.figure(figsize=(25, 4))\n",
        "  # display 20 images\n",
        "  for idx in np.arange(20):\n",
        "      ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "      #imshow(images[idx])\n",
        "      img = images[idx]\n",
        "      img = img / 2 + 0.5  # unnormalize\n",
        "      plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "      ax.set_title(classes[labels[idx]])\n",
        "\n",
        "\n",
        "\n",
        "#############################\n",
        "# VISUALIZE PIXEL OF AN IMAGE RETURN FROM PREV FUNCTION\n",
        "############################\n",
        "def visualize_image_pixels_value(image):\n",
        "\n",
        "  img = np.squeeze(image)\n",
        "  fig = plt.figure(figsize = (12,12)) \n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.imshow(img, cmap='gray')\n",
        "  width, height = img.shape\n",
        "  thresh = img.max()/2.5\n",
        "  for x in range(width):\n",
        "      for y in range(height):\n",
        "          val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "          ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center',\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_rgb_image_pixels_value(image):\n",
        "  rgb_img = np.squeeze(image)\n",
        "  channels = ['red channel', 'green channel', 'blue channel']\n",
        "\n",
        "  fig = plt.figure(figsize = (36, 36)) \n",
        "  for idx in np.arange(rgb_img.shape[0]):\n",
        "      ax = fig.add_subplot(1, 3, idx + 1)\n",
        "      img = rgb_img[idx]\n",
        "      ax.imshow(img, cmap='gray')\n",
        "      ax.set_title(channels[idx])\n",
        "      width, height = img.shape\n",
        "      thresh = img.max()/2.5\n",
        "      for x in range(width):\n",
        "          for y in range(height):\n",
        "              val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "              ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center', size=8,\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_test_results(model,test_loader,batch_size,RGB=False):\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  # move model inputs to cuda, if GPU available\n",
        "  if model.train_on_gpu:\n",
        "      images = images.cuda()\n",
        "\n",
        "  # Flatten images into a 784 long vector\n",
        "  images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "  # get sample outputs\n",
        "  output = model(images)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, preds_tensor = torch.max(output, 1)\n",
        "  preds = np.squeeze(preds_tensor.numpy()) if not model.train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "  # plot the images in the batch, along with predicted and true labels\n",
        "  fig = plt.figure(figsize=(25, 4))\n",
        "  for idx in np.arange(batch_size):\n",
        "      ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "      image_cpu = images.cpu()[idx]\n",
        "      image_cpu = image_cpu / 2 + 0.5  # unnormalize\n",
        "      if(RGB):\n",
        "        plt.imshow(np.transpose(image_cpu, (1, 2, 0)))  # convert from Tensor image\n",
        "      else:\n",
        "        ax.imshow(np.squeeze(image_cpu), cmap='gray')\n",
        "\n",
        "      ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                  color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voDNqAjioodn"
      },
      "source": [
        "############################# TEST TUNE NETWORK ####################################\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\"\"\"\n",
        "def test_tune_train_network(dataset):    \n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "  #tune_train_network(resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "  #tune_train_network('MINST',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt','/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #sys.path.append('./mydeeplearning/')\n",
        "  \n",
        "  #MINST\n",
        "\n",
        "  #cmd_str = \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/') 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt'\" \n",
        "  func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/all/')\\\"\" \n",
        "  log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume.txt'\"\n",
        "\n",
        "\n",
        "  #CIFAR\n",
        "  #func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\\\"\" \n",
        "  #log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt'\"\n",
        "\n",
        "  print(func_str)\n",
        "  print(log_path)\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network('CIFAR',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/06_Mar_2020_07_57_24/')\" 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt'\n",
        "  !python -c $func_str 2>&1 | tee -a $log_path\n",
        "\n",
        "\n",
        "  #!python -c $cmd_str\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\" -u /mydeeplearning/ez_mlp.py 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt' #2>&1\n",
        "  \n",
        "  \n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs','/content/gdrive/My Drive/Colab Notebooks/models/MINST/04_Mar_2020_20_13_34')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXvRQ0zv5hd"
      },
      "source": [
        "########################### TEST ALL EPOCHS ##############################\n",
        "def test_test_all_epochs():\n",
        "    \n",
        "\n",
        "  #model , checkpointtt , filepathh = load_checkpoint(checkpointttt['OutputFolder'],checkpointttt['OutputFilePrefix'],False,10)\n",
        "\n",
        "  #result_list = test_all_epochs(test_loader,criterion,batch_size)\n",
        "\n",
        "  dataset = 'MINST'\n",
        "\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  dataset = 'MINST'\n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size)\n",
        "\n",
        "\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  gfolder = '/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/'\n",
        "\n",
        "  localfolder = './results/'+dataset+'/'\n",
        "\n",
        "  result_list = test_all_epochs(gfolder,test_loader,nn.NLLLoss(),batch_size,OutputFilePrefix='checkpoint_')\n",
        "\n",
        "  #checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks/models/minst/29_Feb_2020_20_06_50_74.pt')\n",
        "\n",
        "  #print(checkpoint)\n",
        "  #load_checkpoint('/gdrive/My Drive/Colab Notebooks/models/'+dataset,'29_Feb_2020_20_06_50',False,74)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu907NvWVlBm"
      },
      "source": [
        "######################## TEST VISUALIZATION###################\n",
        "def test_visualize_images_in_batch():\n",
        "  images = visualize_images_in_batch(train_loader,batch_size)\n",
        "  visualize_image_pixels_value(images[2])\n",
        "\n",
        "  model , checkpointtt , filepathh = load_checkpoint(checkpointt['OutputFolder'],checkpointt['OutputFilePrefix'],True,0)\n",
        "\n",
        "  checkpointttt = test(model,test_loader,criterion,checkpointtt,filepathh,batch_size,True)\n",
        "\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  #visualize_rgb_image_pixels_value(images[2])\n",
        "  #visualize_test_results(model,test_loader,batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbNJdUIMSTBL"
      },
      "source": [
        "# Clone github repository setup\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
        "from os.path import join  \n",
        "#import helper.py\n",
        "import imp \n",
        "\n",
        "\n",
        "################################\n",
        "# INIT GITHUB\n",
        "#######################################\n",
        "def init_github():\n",
        "  \"\"\"\n",
        "  !git init\n",
        "  !git config — global user.email “ice.man011@gmail.com”\n",
        "  !git config — global user.name “iceman011”\n",
        "  !git add -A\n",
        "  !git commit -m “first commit”\n",
        "  !git remote add origin https://<username>:<password>github@github.com/iceman011/mydeeplearning.git\n",
        "  !git push -u origin master\n",
        "  \"\"\"\n",
        "  #!git clone -l -s git://github.com/iceman011/mydeeplearning.git #mydeeplearning-repo\n",
        "  #!git clone -l -s https://github.com/iceman011@github.com/mydeeplearning.git\n",
        "\n",
        "  #%cd mydeeplearning-repo\n",
        "  #!ls\n",
        "\n",
        "  # path to your project on Google Drive\n",
        "  #MY_GOOGLE_DRIVE_PATH = 'My Drive/MyDrive/Udacity/deep-learning-v2-pytorch' \n",
        "  # replace with your Github username \n",
        "  GIT_USERNAME = \"iceman011\" \n",
        "  # definitely replace with your\n",
        "  GIT_TOKEN = \"1aeb0c6f424e3c604988a3a636f74c6e5180cd89\"  \n",
        "  # Replace with your github repository in this case we want \n",
        "  # to clone deep-learning-v2-pytorch repository\n",
        "  GIT_REPOSITORY = \"mydeeplearning\" \n",
        "\n",
        "  # REMOVE IT BEFORE INIT\n",
        "  if( os.path.isdir('./'+GIT_REPOSITORY) ):\n",
        "    shutil.rmtree('./'+GIT_REPOSITORY)\n",
        "\n",
        "  #PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "  # It's good to print out the value if you are not sure \n",
        "  #print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "  # In case we haven't created the folder already; we will create a folder in the project path \n",
        "  #!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "  #GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "\n",
        "  GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "  print(\"GIT_PATH: \", GIT_PATH)\n",
        "\n",
        "  !git clone \"{GIT_PATH}\" # clone the github repository\n",
        "\n",
        "############################3\n",
        "# CONVERT FROM COLAB INTO PYTHON\n",
        "#############################3\n",
        "def from_colab_to_python():\n",
        "\n",
        "  !pip install ipython\n",
        "  !pip install nbconvert\n",
        "  !ipython nbconvert --to python ./mydeeplearning/ez_mlp.ipynb\n",
        "\n",
        "  #helper = imp.new_module('ez-mlp')\n",
        "  #exec(open('./'+GIT_REPOSITORY+\"/ez-mlp.py\").read(), helper.__dict__)\n",
        "\n",
        "\n",
        "######################################\n",
        "# PUSH CHANGES TO GITHUB\n",
        "##################################\n",
        "def push_github():\n",
        "  !git add -u\n",
        "  !git commit -m \"new commit\"\n",
        "  !git push mydeeplearning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7of0_y_zsQR"
      },
      "source": [
        "import shutil\n",
        "\n",
        "######################    \n",
        "# MAIN #\n",
        "######################\n",
        "def main():\n",
        "\n",
        "  # Create the network, define the criterion and optimizer\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  dataset = 'MINST'\n",
        "\n",
        "  init_github()\n",
        "  from_colab_to_python()\n",
        "  test_tune_train_network(dataset)\n",
        "\n",
        "  #CONTINUE FROM PREVIOUS MODEL\n",
        "  #model , checkpoint , filepath = load_checkpoint('/content/drive/My Drive/Colab Notebooks/models/MINST/01_Mar_2020_18_42_35','checkpoint_',True,0)\n",
        "  #PreviousCheckPointId = checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])\n",
        "  #PreviousValidationLoss= checkpoint['ValidationLoss'] #np.Inf\n",
        "\n",
        "  #model = Network(784, 10, [512, 256, 128 , 64 ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "\n",
        "  #criterion = nn.NLLLoss()\n",
        "  #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  #checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "\n",
        "\n",
        "  #Upload checkpoint to local drive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "  #Upload to gDrive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',np.Inf,epochs=10)\n",
        "\n",
        "  #plotLossTrend (train_lossess , valid_lossess)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mp5Be63OjI7"
      },
      "source": [
        "#if __name__ == '__main__':\n",
        "#main()\n",
        "\n",
        "#tune_train_network('CIFAR',1,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/test.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/test/',network_type='resnet18')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ5bRRFqG7QE"
      },
      "source": [
        "# tune_train_network('CIFAR',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/test.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/',network_type='resenet')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3pXag-J41mi"
      },
      "source": [
        "\n",
        "\"\"\",'vgg11'-> model.classifier[0].in_features\n",
        "'vgg11_bn'-> model.classifier[0].in_features\n",
        "'vgg13' -> model.classifier[0].in_features\n",
        "'vgg13_bn'-> model.classifier[0].in_features\n",
        "\n",
        "'vgg16','vgg16_bn','vgg19','vgg19_bn',\n",
        "                             'resnet18' ->model.fc.in_features\n",
        "                             \n",
        "                             ,'resnet34','resnet50','resnet101','resnet152',\n",
        "                             'squeezenet1_0','squeezenet1_1', -> 512\n",
        "\n",
        "                             'densenet121','densenet169','densenet161','densenet201', -> model.classifier.in_features\n",
        "                             'inception_v3'\n",
        "                            ]\n",
        "\n",
        "\"\"\"\n",
        "#model  = pretrained_models.vgg11(pretrained=True)\n",
        "#print(model)\n",
        "#print(model.classifier.in_features)\n",
        "\n",
        "\n",
        "def test_pre_trained_mode():\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData('CIFAR',20,0.2,'resnet18')\n",
        "\n",
        "  model = pretrained_models.vgg11(pretrained=True)\n",
        "\n",
        "  print(model)\n",
        "  # Freeze parameters so we don't backprop through them\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "      \n",
        "  model.classifier[6] = nn.Linear(4096,10)\n",
        "                     #nn.Sequential(nn.Linear(512, 1024),\n",
        "                     #             nn.ReLU(),\n",
        "                     #             nn.Dropout(0.2),\n",
        "                     #             nn.Linear(1024, 10),\n",
        "                     #             nn.LogSoftmax(dim=1))\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  # Only train the classifier parameters, feature parameters are frozen\n",
        "  optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
        "\n",
        "  model.to(device);\n",
        "\n",
        "  epochs = 10\n",
        "  steps = 0\n",
        "  running_loss = 0\n",
        "  print_every = 2\n",
        "  for epoch in range(epochs):\n",
        "      for inputs, labels in train_loader:\n",
        "          steps += 1\n",
        "          # Move input and label tensors to the default device\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          logps = model.forward(inputs)\n",
        "          loss = criterion(logps, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          if steps % print_every == 0:\n",
        "              test_loss = 0\n",
        "              accuracy = 0\n",
        "              model.eval()\n",
        "              with torch.no_grad():\n",
        "                  for inputs, labels in test_loader:\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      logps = model.forward(inputs)\n",
        "                      batch_loss = criterion(logps, labels)\n",
        "                      \n",
        "                      test_loss += batch_loss.item()\n",
        "                      \n",
        "                      # Calculate accuracy\n",
        "                      ps = torch.exp(logps)\n",
        "                      top_p, top_class = ps.topk(1, dim=1)\n",
        "                      equals = top_class == labels.view(*top_class.shape)\n",
        "                      accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "                      \n",
        "              print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                    f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                    f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
        "                    f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
        "              running_loss = 0\n",
        "              model.train()\n",
        "\n",
        "\n",
        "#test_pre_trained_mode()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}