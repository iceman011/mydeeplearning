{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0TMmPrnzr5UtU/4FOdnF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceman011/mydeeplearning/blob/master/ez_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXy2soVzPQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "import datetime\n",
        "import os\n",
        "import pdb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "from google.colab import drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from torch import optim\n",
        "import itertools\n",
        "#!/usr/bin/env python3\n",
        "import mmap\n",
        "import re\n",
        "from itertools import dropwhile, product\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers,output_classes,transform,dataset, drop_p=0.5,lr =0.001, train_on_gpu=False):\n",
        "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Using CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Using GPU ...')\n",
        "            \n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.drop_ratio = drop_p\n",
        "        self.output_classes = output_classes\n",
        "        self.transform=transform\n",
        "        self.dataset=dataset\n",
        "\n",
        "        # Input to a hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "        \n",
        "        # Add a variable number of more hidden layers\n",
        "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "        \n",
        "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "        \n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "######################    \n",
        "# LOADING DATA #\n",
        "######################\n",
        "def LoadData(datasetName,batch_size=20,valid_size = 0.2):\n",
        "      \n",
        "    # number of subprocesses to use for data loading\n",
        "    num_workers = 0\n",
        "\n",
        "\n",
        "    # convert data to torch.FloatTensor\n",
        "    #transform = transforms.ToTensor()\n",
        "\n",
        "    # convert data to a normalized torch.FloatTensor\n",
        "    transform = None\n",
        "    train_data= None\n",
        "    test_data=None\n",
        "    classes=None\n",
        "    if(datasetName == 'MINST'):\n",
        "       # convert data to a normalized torch.FloatTensor\n",
        "      transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          #transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),          \n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, ), (0.5, ))\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.MNIST(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "      test_data = datasets.MNIST(root='data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "      # specify the image classes\n",
        "      classes = ['1', '2', '3', '4', '5',\n",
        "           '6', '7', '8', '9', '10']\n",
        "    elif(datasetName == 'CIFAR'):\n",
        "      \n",
        "      # convert data to a normalized torch.FloatTensor\n",
        "      transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "      test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "      # specify the image classes\n",
        "      classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "        sampler=valid_sampler, num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "        num_workers=num_workers)\n",
        "    return train_loader , valid_loader ,test_loader ,classes , transform\n",
        "\n",
        "\n",
        "#############################\n",
        "# VALIDATE MODEL\n",
        "############################\n",
        "def validation(model, validationloader, criterion):\n",
        "    accuracy = 0\n",
        "    validation_loss = 0\n",
        "    validate_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : ','Starting Validation....')\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if model.train_on_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "      for images, labels in validationloader:\n",
        "\n",
        "          if model.train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "          \n",
        "          images = images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "          output = model.forward(images)\n",
        "          validation_loss += criterion(output, labels).item()\n",
        "\n",
        "          ## Calculating the accuracy \n",
        "          # Model's output is log-softmax, take exponential to get the probabilities\n",
        "          ps = torch.exp(output)\n",
        "          # Class with highest probability is our predicted class, compare with true label\n",
        "          #equality = (labels.data == ps.max(1)[1])\n",
        "          # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "          #accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "\n",
        "\n",
        "        \n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))      \n",
        "        \n",
        "      validation_loss = validation_loss/len(validationloader)\n",
        "      accuracy = 100. * accuracy/len(validationloader)\n",
        "    \n",
        "    print('Finished Validation In ',datetime.timedelta(seconds = time.time() - validate_start_time) )\n",
        "    return validation_loss, accuracy\n",
        "\n",
        "#############################\n",
        "# TEST MODEL\n",
        "############################\n",
        "def test(model,test_loader,criterion,checkpoint,outputfilepath,batch_size,override_checkpoint):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(model.output_size))\n",
        "    class_total = list(0. for i in range(model.output_size))\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Testing....')\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      model.eval()\n",
        "      # iterate over test data\n",
        "      for data, target in test_loader:\n",
        "          # move tensors to GPU if CUDA is available\n",
        "          if model.train_on_gpu:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          \n",
        "          # Flatten images into a 784 long vector\n",
        "          data.resize_(data.size()[0], model.input_size)\n",
        "\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the batch loss\n",
        "          loss = criterion(output, target)\n",
        "          # update test loss \n",
        "          test_loss += loss.item()*data.size(0)\n",
        "          # convert output probabilities to predicted class\n",
        "          _, pred = torch.max(output, 1)    \n",
        "          # compare predictions to true label\n",
        "          correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "          correct = np.squeeze(correct_tensor.numpy()) if not model.train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "          # calculate test accuracy for each object class\n",
        "          for i in range(batch_size):\n",
        "              label = target.data[i]\n",
        "              class_correct[label] += correct[i].item()\n",
        "              class_total[label] += 1\n",
        "\n",
        "      # average test loss\n",
        "      print('Finished Testing during ',datetime.timedelta(seconds=time.time() - test_start_time))\n",
        "      test_loss = test_loss/len(test_loader.dataset)\n",
        "      print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "      \n",
        "      test_checkpoint = dict()\n",
        "\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update({'TestLoss': test_loss})\n",
        "      else :\n",
        "        test_checkpoint.update({'TestLoss': test_loss})\n",
        "\n",
        "      for i in range(model.output_size):\n",
        "          if class_total[i] > 0:\n",
        "            current_key  = 'Test Accuracy of '+model.output_classes[i]\n",
        "            current_val = '{:.3f}% ({}/{})'.format(100 *( class_correct[i] / class_total[i]),\n",
        "                  np.sum(class_correct[i]), np.sum(class_total[i]))\n",
        "            ele={current_key:current_val}\n",
        "            if(override_checkpoint):\n",
        "              checkpoint.update(ele)\n",
        "              print(current_key,current_val)\n",
        "            else:\n",
        "              test_checkpoint.update(ele)\n",
        "\n",
        "\n",
        "          \"\"\"else:\n",
        "              print('Test Accuracy of %5s: N/A (no training examples)' % (model.output_classes[i]))\n",
        "          \"\"\"\n",
        "\n",
        "      current_key = 'Test Accuracy (Overall): '#.format(100. * np.sum(class_correct) / np.sum(class_total))\n",
        "      current_val = '{:.3f}% ({}/{})'.format(100 * (np.sum(class_correct) / np.sum(class_total)),\n",
        "          np.sum(class_correct), np.sum(class_total) )\n",
        "      ele={current_key:current_val}\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update(ele)      \n",
        "        print(current_key,current_val)\n",
        "      else:\n",
        "        test_checkpoint.update(ele)\n",
        "        test_checkpoint.update({'Detailed ':checkpoint})\n",
        "\n",
        "    \n",
        "    #model.train()\n",
        "    if(override_checkpoint):\n",
        "      print('Overriding Checkpoint')\n",
        "      torch.save(checkpoint,outputfilepath)\n",
        "\n",
        "    return checkpoint , test_checkpoint\n",
        "\n",
        "#############################\n",
        "# TRAIN MODEL\n",
        "############################\n",
        "def train(model, trainloader, validationloader, criterion, optimizer, uploadToGDrive,checkpointPath,PreviousCheckPointId,\n",
        "          PreviousValidationLoss,start_time,exp_id,epochs=5, print_every=40):\n",
        "    # monitor training loss    \n",
        "    steps = 0    \n",
        "    #start_time = time.time()\n",
        "    #dateTimeObj = datetime.now()\n",
        "    #start_time_timestamp = './results/'+dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    #start_time_timestamp = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    train_losses, valid_losses = [], []\n",
        "    checkpoint = dict()\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training using Model Parameters \\n '+str(model)+' \\n from PreviousModel '+ PreviousCheckPointId + ' with validationLoss '+ str(PreviousValidationLoss) )\n",
        "    valid_loss_min = PreviousValidationLoss #np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "    for e in range(epochs):        \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training of Epoch'+str(e)  )\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            \n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if model.train_on_gpu:\n",
        "              images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Flatten images into a 784 long vector\n",
        "            images.resize_(images.size()[0], model.input_size)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        train_losses.append(train_loss)\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # Model in inference mode, dropout is off\n",
        "        model.eval()\n",
        "        print(' Finished Training of Epoch '+str(e),' In ',datetime.timedelta(seconds = time.time() - epoch_start_time) )\n",
        "\n",
        "        # Turn off gradients for validation, will speed up inference\n",
        "        with torch.no_grad():\n",
        "            valid_loss, accuracy = validation(model, validationloader, criterion)\n",
        "        \n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print('{} : Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy : {:.6f}'.format(\n",
        "            dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"),\n",
        "            e+1, \n",
        "            train_loss,\n",
        "            valid_loss,\n",
        "            accuracy\n",
        "            ))\n",
        "        \n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f} ,)  Accuracy: {:.6f}  TimeElapsed: {:.6f}.  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss,\n",
        "            accuracy,\n",
        "            (time.time() - start_time)))\n",
        "            \n",
        "            \n",
        "\n",
        "            checkpoint = {'InputSize': model.input_size,\n",
        "                  'OutputSize': model.output_size,\n",
        "                  'HiddenLayers': [each.out_features for each in model.hidden_layers],\n",
        "                  'LearningRate':model.learning_rate,\n",
        "                  'DropRatio':model.drop_ratio,\n",
        "                  'TrainingLoss' :train_loss,\n",
        "                  'ValidationLoss':valid_loss,\n",
        "                  'ValidationAccuracy':accuracy,\n",
        "                  'ElapsedTime': datetime.timedelta(seconds = time.time() - start_time),\n",
        "                  'Dataset':model.dataset,\n",
        "                  'LastEpoch': e,\n",
        "                  'PreviousCheckPoint': PreviousCheckPointId,\n",
        "                  'GPUState': model.train_on_gpu,                  \n",
        "                  'OutputFolder' : exp_id,\n",
        "                  'CheckPointTimestamp': dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"), #time.time(),\n",
        "                  'OutputFilePrefix' : 'checkpoint_',\n",
        "                  'OutputClasses': model.output_classes,\n",
        "                  'Transforms': model.transform,\n",
        "                  'TrainingLosses' :train_losses,\n",
        "                  'ValidationLosses':valid_losses,\n",
        "                  'StateDictionay': model.state_dict()}\n",
        "            \n",
        "            #print(checkpoint)\n",
        "            save_model(checkpoint,uploadToGDrive,checkpointPath)\n",
        "            valid_loss_min = valid_loss\n",
        "\n",
        "    return train_losses , valid_losses , checkpoint\n",
        "\n",
        "\n",
        "#############################\n",
        "# SAVE MODEL TO GOOGLE DRIVE\n",
        "############################\n",
        "def save_model(checkpoint,uploadToGDrive,checkpointPath):\n",
        "  \n",
        "  file_path = ''\n",
        "  if(uploadToGDrive):\n",
        "    drive.mount('/content/gdrive')\n",
        "    file_path = checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)      \n",
        "    #path = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])+'.pt'\n",
        "    print('Saving Model to ',file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "  else:\n",
        "    file_path=checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "    \n",
        "    print('Saving Model to ',file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "\n",
        "\n",
        "###########################\n",
        "# SKIP CERTAIN ENTERIES FROM ITERTOOLS.PRODUCT\n",
        "#####################33\n",
        "def resume(iterable, sentinel):\n",
        "    yield from dropwhile(lambda x: x != sentinel, iterable)\n",
        "\n",
        "#############################\n",
        "# LOAD LAST EXPERIMENT PARAMS\n",
        "##################################\n",
        "def load_last_exp_param(logPath,checkpointPath):\n",
        "\n",
        "  #with open(checkpointPath+'/3-3-2020.txt') as f:\n",
        "  #    total = f.read()\n",
        "  #    print( total.count('Starting New') )\n",
        "      \n",
        "  f = open(logPath, 'r')\n",
        "  content = f.read()\n",
        "\n",
        "  hidden_layers = re.findall('hidden_layers: \\{(.+?)\\}', content)\n",
        "  hidden_layers = int(hidden_layers[len(hidden_layers)-1])\n",
        "\n",
        "  hidden_layer_width = re.findall('hidden_layer_width: \\{(.+?)\\}', content)\n",
        "  hidden_layer_width = hidden_layer_width[len(hidden_layer_width)-1]\n",
        "\n",
        "  Learning_rate = re.findall('Learning_Rate: \\{(.+?)\\}', content)\n",
        "  Learning_rate = float(Learning_rate[len(Learning_rate)-1])\n",
        "\n",
        "  drop_ratio = re.findall('drop_ratio: \\{(.+?)\\}', content)\n",
        "  drop_ratio = float(drop_ratio[len(drop_ratio)-1])\n",
        "  prev_valid_loss = np.Inf\n",
        "  return_checkpoint = None\n",
        "  return_model = None\n",
        "\n",
        "  #for filename in os.listdir(checkpointPath):\n",
        "  for root, dirs, files in os.walk(checkpointPath):\n",
        "    for filename in files:\n",
        "      model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "      if( float(checkpoint['ValidationLoss']) < prev_valid_loss ):\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "      \n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      hidden_layer_width_local = hidden_layer_width.strip()\n",
        "      hidden_layer_width_local = hidden_layer_width_local[1:len(hidden_layer_width_local)-1]\n",
        "      hidden_layer_width_local = list(map(int, hidden_layer_width_local.split(',')))\n",
        "\n",
        "      if(hidden_layer_width_local == checkpoint['HiddenLayers'] and float(checkpoint['ValidationLoss']) <= prev_valid_loss ):\n",
        "        print(' Found Matching Checkpoint with Last Experiement Parame ValidationLoss:{} hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} '.format(checkpoint['ValidationLoss'],hidden_layers,hidden_layer_width,Learning_rate,drop_ratio))\n",
        "        return_checkpoint = checkpoint\n",
        "        return_model = model\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "        #return checkpoint,model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "  if( not bool(return_checkpoint)): \n",
        "    print(' Not Found Matching Checkpoint with Last Experiement Parameters hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} '.format(hidden_layers,hidden_layer_width,Learning_rate,drop_ratio))    \n",
        "\n",
        "  return return_checkpoint,return_model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "\n",
        "################################\n",
        "# TUNE NETWORK LAYERS AND NO. OF NODES\n",
        "##################################\n",
        "def tune_train_network(dataset,epochs,resumeExp=False,resume_logPath='',resume_checkpointPath=''):\n",
        "\n",
        "  start_time = time.time()\n",
        "  dateTimeObj = dt.now()\n",
        "  exp_id = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "  #dataset = 'MINST'\n",
        "  checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  \n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size)\n",
        "\n",
        "  nodes_per_layer =  [2048, 1024, 512, 256, 128, 64, 32]\n",
        "  full_learning_rates= [0.1, 0.01, 0.0001, 0.001]\n",
        "  full_drop_ratios= [0.8, 0.5, 0.3, 0.1]\n",
        "  learning_rates = None\n",
        "  drop_ratios = None\n",
        "  PreviousValidationLoss= np.Inf\n",
        "  max_hidden_layers = 3\n",
        "  iter_hidden_layer_nodes = None\n",
        "  iter_hidden_layer_nodes_list = None\n",
        "  model = None\n",
        "  resume_checkpoint = None\n",
        "\n",
        "  if(resumeExp):\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Resuming Experiment With New ID ',exp_id )\n",
        "    resume_checkpoint,model,resum_hidden_layers,resume_hidden_layer_width,resume_Learning_rate,resume_drop_ratio,resume_prev_valid_loss =  load_last_exp_param(resume_logPath,resume_checkpointPath)\n",
        "    max_hidden_layers = resum_hidden_layers  \n",
        "\n",
        "    learning_rates = full_learning_rates[full_learning_rates.index(resume_Learning_rate):]\n",
        "    drop_ratios = full_drop_ratios[full_drop_ratios.index(resume_drop_ratio):]\n",
        "    PreviousValidationLoss = resume_prev_valid_loss\n",
        "    checkpointPath = resume_checkpointPath\n",
        "\n",
        "      #iter_hidden_layer_nodes = resume(itertools.product(nodes_per_layer, repeat=hidden_layers+1), resume_checkpoint['HiddenLayers'])      \n",
        "    #if(bool(resume_checkpoint)):\n",
        "    #  model , resume_checkpoint , filepath = load_checkpoint(checkpointPath,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')])\n",
        "      \n",
        "\n",
        "\n",
        "  #VARY HIDDEN LAEYERS\n",
        "  for hidden_layers in range(max_hidden_layers,0,-1):\n",
        "    #VARY HIDDEN LAYER NODES/WIDTH\n",
        "    if(resumeExp ):\n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat= hidden_layers)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      resume_hidden_layer_width_list = tuple(list(map(int, resume_hidden_layer_width.split(','))))\n",
        "      iter_hidden_layer_nodes_list = iter_hidden_layer_nodes_list[iter_hidden_layer_nodes_list.index(resume_hidden_layer_width_list):]\n",
        "      \n",
        "      print('Resume ValidationLoss ',PreviousValidationLoss,'Resume Learning Rate Array ',learning_rates ,' Resume Drop Ratio List ',drop_ratios , ' Resume Hidden Nodes List ',iter_hidden_layer_nodes_list )\n",
        "    else:      \n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat=hidden_layers+1)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "    for hidden_layer in iter_hidden_layer_nodes_list:\n",
        "      #VARY LEARNING RATE\n",
        "      if( not resumeExp):\n",
        "        learning_rates = full_learning_rates\n",
        "      for lr in learning_rates :\n",
        "        #VARY DROP RATIO\n",
        "        if( not resumeExp):\n",
        "          drop_ratios = full_drop_ratios\n",
        "        for drop_ratio in drop_ratios :\n",
        "          \n",
        "          print(exp_id+':  DurationSinceStart :',datetime.timedelta(seconds = time.time() - start_time),' Starting New Experiment with %hidden_layers: {',hidden_layers,'}% %hidden_layer_width: {',hidden_layer ,'}% %Learning_Rate: {',lr, '}% %drop_ratio: {',drop_ratio,'}%')\n",
        "          #model = Network(784, 10, [first_layer, second_layer, third_layer ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "          if( not bool(resume_checkpoint) or not resumeExp ):\n",
        "            if(dataset == 'MINST'):\n",
        "              model = Network(784, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True)\n",
        "            else:\n",
        "              model = Network((32*32*3), 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True)\n",
        "\n",
        "\n",
        "          criterion = nn.NLLLoss()\n",
        "          optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "          #Upload checkpoint to local drive\n",
        "          #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "          #Upload to gDrive\n",
        "          train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',PreviousValidationLoss,start_time,exp_id,epochs=epochs)\n",
        "          if( bool(checkpointt) and checkpointt['ValidationLoss'] < PreviousValidationLoss ):\n",
        "            PreviousValidationLoss = checkpointt['ValidationLoss']\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        resumeExp=False\n",
        "\n",
        "\n",
        "#############################\n",
        "# PLOT TRAINING LOSS VS VALIDATION LOSS \n",
        "############################\n",
        "def plotLossTrend(train_losses,validation_losses,test_losses=[]):\n",
        "  #TRAINING LOSS DATA\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in train_losses:    \n",
        "    for key, value in item.items():\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Training Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #VALIDATON LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in validation_losses:   \n",
        "    #pdb.set_trace() \n",
        "    for key, value in item.items():      \n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Validation Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #TEST LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in test_losses:    \n",
        "    for key, value in item.items():\n",
        "      #pdb.set_trace()\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Test Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()\n",
        "  #for i, v in enumerate(values):\n",
        "  #    ax.text(i, v+25, \"%d\" %v, ha=\"center\")\n",
        "  #plt.ylim(-10, 595)\n",
        "\n",
        "#############################\n",
        "# LOAD CHECKPOINT\n",
        "############################\n",
        "def load_checkpoint(OutputFolder,OutputFilePrefix,max_epoch,exact_epoch,load_model=True):\n",
        "    \n",
        "    file_epoch = 0\n",
        "    if(max_epoch):\n",
        "      for filename in os.listdir(OutputFolder):\n",
        "        data = filename.split('_')\n",
        "        tmp=int(data[1][:-3])\n",
        "        #print('data[1]',data[1],' tmp[:-3] ',tmp[:-3])\n",
        "        if( file_epoch <= tmp ):\n",
        "          file_epoch = tmp\n",
        "    else:\n",
        "      file_epoch=exact_epoch\n",
        "\n",
        "    filepath = OutputFolder+'/'+OutputFilePrefix + str(file_epoch)+'.pt'\n",
        "    print('Loading Checkpoint from '+filepath+'...')\n",
        "    indent=1\n",
        "    checkpoint = torch.load(filepath)\n",
        "    for key, value in checkpoint.items():\n",
        "        if(key == 'StateDictionay'):\n",
        "          continue\n",
        "        print('\\t' * indent + str(key),'\\t' * (indent+1) + str(value))\n",
        "        #print('\\t' * (indent+1) + str(value))\n",
        "    \n",
        "    testmodel = None\n",
        "\n",
        "    if(load_model):\n",
        "      testmodel = Network(checkpoint['InputSize'],\n",
        "                              checkpoint['OutputSize'],\n",
        "                              checkpoint['HiddenLayers'],\n",
        "                              checkpoint['OutputClasses'],\n",
        "                              checkpoint['Transforms'],\n",
        "                              checkpoint['Dataset'],\n",
        "                              checkpoint['DropRatio'],\n",
        "                              checkpoint['LearningRate'],\n",
        "                              train_on_gpu=checkpoint['GPUState']\n",
        "                      )\n",
        "      testmodel.load_state_dict(checkpoint['StateDictionay'])\n",
        "      \n",
        "    return testmodel , checkpoint , filepath\n",
        "\n",
        "\n",
        "def test_all_epochs(OutputFolder,test_loader,criterion,batch_size,OutputFilePrefix='checkpoint_'):\n",
        "\n",
        "    #OutputFolder = './results/'\n",
        "    file_epoch = 0\n",
        "    result_list=[]\n",
        "    print('Starting Test All Saved Epochs ....')\n",
        "    trainig_loss=[]\n",
        "    validation_loss=[]\n",
        "    test_loss=[]\n",
        "    ele = dict()\n",
        "\n",
        "    \"\"\"\n",
        "        for folder in os.listdir(OutputFolder):\n",
        "          for filename in os.listdir(OutputFolder+folder):\n",
        "            data = filename.split('_')\n",
        "            #pdb.set_trace()\n",
        "            print(data)\n",
        "            file_epoch=int(data[1][:-3])\n",
        "\n",
        "            model , checkpoint , filepath = load_checkpoint(OutputFolder+folder,OutputFilePrefix,False,file_epoch)\n",
        "    \"\"\"\n",
        "    #for filename in os.listdir(checkpointPath):\n",
        "    for root, dirs, files in os.walk(OutputFolder):\n",
        "      for filename in files:\n",
        "        model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "\n",
        "        checkpoint , checkpoint_test = test(model,test_loader,criterion,checkpoint,filepath,batch_size,False)\n",
        "\n",
        "        print('Test Accuracy ',checkpoint_test['Test Accuracy (Overall): '])\n",
        "        result_list.append(checkpoint_test)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['TrainingLoss'])\n",
        "        key = checkpoint['Dataset'] + '_' + checkpoint['OutputFolder'] + '_' + str(checkpoint['LastEpoch'])\n",
        "        ele = {key: checkpoint['TrainingLoss']}\n",
        "        trainig_loss.append(ele)\n",
        "        \n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['ValidationLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint['ValidationLoss']}\n",
        "        validation_loss.append(ele)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint_test['TestLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint_test['TestLoss']}\n",
        "        test_loss.append(ele)\n",
        "    \n",
        "    #print('Result List',result_list)\n",
        "    plotLossTrend (trainig_loss,validation_loss,test_loss)\n",
        "    return result_list\n",
        "      \n",
        "\n",
        "#############################\n",
        "# VISUALIZE ALL IMAGES IN BATCH\n",
        "############################\n",
        "def visualize_images_in_batch(loader,batch_size) :\n",
        "    # obtain one batch of training images\n",
        "\t\tdataiter = iter(loader)\n",
        "\t\timages, labels = dataiter.next()\n",
        "\t\timages = images.numpy()\n",
        "\n",
        "\t\t# plot the images in the batch, along with the corresponding labels\n",
        "\t\tfig = plt.figure(figsize=(25, 4))\n",
        "\t\tfor idx in np.arange(batch_size):\n",
        "\t\t\tax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\t\t\tax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "\t\t\t# print out the correct label for each image\n",
        "\t\t\t# .item() gets the value contained in a Tensor\n",
        "\t\t\tax.set_title(str(labels[idx].item()))\n",
        "\t\treturn images\n",
        "\n",
        "\n",
        "#############################\n",
        "# VISUALIZE PIXEL OF AN IMAGE RETURN FROM PREV FUNCTION\n",
        "############################\n",
        "def visualize_image_pixels_value(image):\n",
        "\n",
        "  img = np.squeeze(image)\n",
        "  fig = plt.figure(figsize = (12,12)) \n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.imshow(img, cmap='gray')\n",
        "  width, height = img.shape\n",
        "  thresh = img.max()/2.5\n",
        "  for x in range(width):\n",
        "      for y in range(height):\n",
        "          val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "          ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center',\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_rgb_image_pixels_value(image):\n",
        "  rgb_img = np.squeeze(image)\n",
        "  channels = ['red channel', 'green channel', 'blue channel']\n",
        "\n",
        "  fig = plt.figure(figsize = (36, 36)) \n",
        "  for idx in np.arange(rgb_img.shape[0]):\n",
        "      ax = fig.add_subplot(1, 3, idx + 1)\n",
        "      img = rgb_img[idx]\n",
        "      ax.imshow(img, cmap='gray')\n",
        "      ax.set_title(channels[idx])\n",
        "      width, height = img.shape\n",
        "      thresh = img.max()/2.5\n",
        "      for x in range(width):\n",
        "          for y in range(height):\n",
        "              val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "              ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center', size=8,\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_test_results(model,test_loader,batch_size,RGB=False):\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  # move model inputs to cuda, if GPU available\n",
        "  if model.train_on_gpu:\n",
        "      images = images.cuda()\n",
        "\n",
        "  # Flatten images into a 784 long vector\n",
        "  images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "  # get sample outputs\n",
        "  output = model(images)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, preds_tensor = torch.max(output, 1)\n",
        "  preds = np.squeeze(preds_tensor.numpy()) if not model.train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "  # plot the images in the batch, along with predicted and true labels\n",
        "  fig = plt.figure(figsize=(25, 4))\n",
        "  for idx in np.arange(batch_size):\n",
        "      ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "      image_cpu = images.cpu()[idx]\n",
        "      image_cpu = image_cpu / 2 + 0.5  # unnormalize\n",
        "      if(RGB):\n",
        "        plt.imshow(np.transpose(image_cpu, (1, 2, 0)))  # convert from Tensor image\n",
        "      else:\n",
        "        ax.imshow(np.squeeze(image_cpu), cmap='gray')\n",
        "\n",
        "      ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                  color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voDNqAjioodn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# TEST TUNE NETWORK ####################################\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\"\"\"\n",
        "def test_tune_train_network(dataset):    \n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "  #tune_train_network(resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "  #tune_train_network('MINST',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt','/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #sys.path.append('./mydeeplearning/')\n",
        "  \n",
        "  #MINST\n",
        "  #cmd_str = \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/') 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt'\" \n",
        "  func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\\\"\" \n",
        "  log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-8-2020.txt'\"\n",
        "\n",
        "\n",
        "  #CIFAR\n",
        "  #func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\\\"\" \n",
        "  #log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt'\"\n",
        "\n",
        "  print(func_str)\n",
        "  print(log_path)\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network('CIFAR',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/06_Mar_2020_07_57_24/')\" 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt'\n",
        "  !python -c $func_str 2>&1 | tee -a $log_path\n",
        "\n",
        "\n",
        "  #!python -c $cmd_str\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\" -u /mydeeplearning/ez_mlp.py 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt' #2>&1\n",
        "  \n",
        "  \n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs','/content/gdrive/My Drive/Colab Notebooks/models/MINST/04_Mar_2020_20_13_34')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXvRQ0zv5hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### TEST ALL EPOCHS ##############################\n",
        "def test_test_all_epochs():\n",
        "    \n",
        "\n",
        "  #model , checkpointtt , filepathh = load_checkpoint(checkpointttt['OutputFolder'],checkpointttt['OutputFilePrefix'],False,10)\n",
        "\n",
        "  #result_list = test_all_epochs(test_loader,criterion,batch_size)\n",
        "\n",
        "  dataset = 'MINST'\n",
        "\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  dataset = 'MINST'\n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size)\n",
        "\n",
        "\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  gfolder = '/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/'\n",
        "\n",
        "  localfolder = './results/'+dataset+'/'\n",
        "\n",
        "  result_list = test_all_epochs(gfolder,test_loader,nn.NLLLoss(),batch_size,OutputFilePrefix='checkpoint_')\n",
        "\n",
        "  #checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks/models/minst/29_Feb_2020_20_06_50_74.pt')\n",
        "\n",
        "  #print(checkpoint)\n",
        "  #load_checkpoint('/gdrive/My Drive/Colab Notebooks/models/'+dataset,'29_Feb_2020_20_06_50',False,74)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu907NvWVlBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## TEST VISUALIZATION###################\n",
        "def test_visualize_images_in_batch():\n",
        "  images = visualize_images_in_batch(train_loader,batch_size)\n",
        "  visualize_image_pixels_value(images[2])\n",
        "\n",
        "  model , checkpointtt , filepathh = load_checkpoint(checkpointt['OutputFolder'],checkpointt['OutputFilePrefix'],True,0)\n",
        "\n",
        "  checkpointttt = test(model,test_loader,criterion,checkpointtt,filepathh,batch_size,True)\n",
        "\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  #visualize_rgb_image_pixels_value(images[2])\n",
        "  #visualize_test_results(model,test_loader,batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbNJdUIMSTBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository setup\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
        "from os.path import join  \n",
        "#import helper.py\n",
        "import imp \n",
        "\n",
        "\n",
        "################################\n",
        "# INIT GITHUB\n",
        "#######################################\n",
        "def init_github():\n",
        "  \"\"\"\n",
        "  !git init\n",
        "  !git config — global user.email “ice.man011@gmail.com”\n",
        "  !git config — global user.name “iceman011”\n",
        "  !git add -A\n",
        "  !git commit -m “first commit”\n",
        "  !git remote add origin https://<username>:<password>github@github.com/iceman011/mydeeplearning.git\n",
        "  !git push -u origin master\n",
        "  \"\"\"\n",
        "  #!git clone -l -s git://github.com/iceman011/mydeeplearning.git #mydeeplearning-repo\n",
        "  #!git clone -l -s https://github.com/iceman011@github.com/mydeeplearning.git\n",
        "\n",
        "  #%cd mydeeplearning-repo\n",
        "  #!ls\n",
        "\n",
        "  # path to your project on Google Drive\n",
        "  #MY_GOOGLE_DRIVE_PATH = 'My Drive/MyDrive/Udacity/deep-learning-v2-pytorch' \n",
        "  # replace with your Github username \n",
        "  GIT_USERNAME = \"iceman011\" \n",
        "  # definitely replace with your\n",
        "  GIT_TOKEN = \"1aeb0c6f424e3c604988a3a636f74c6e5180cd89\"  \n",
        "  # Replace with your github repository in this case we want \n",
        "  # to clone deep-learning-v2-pytorch repository\n",
        "  GIT_REPOSITORY = \"mydeeplearning\" \n",
        "\n",
        "  # REMOVE IT BEFORE INIT\n",
        "  if( os.path.isdir('./'+GIT_REPOSITORY) ):\n",
        "    shutil.rmtree('./'+GIT_REPOSITORY)\n",
        "\n",
        "  #PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "  # It's good to print out the value if you are not sure \n",
        "  #print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "  # In case we haven't created the folder already; we will create a folder in the project path \n",
        "  #!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "  #GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "\n",
        "  GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "  print(\"GIT_PATH: \", GIT_PATH)\n",
        "\n",
        "  !git clone \"{GIT_PATH}\" # clone the github repository\n",
        "\n",
        "############################3\n",
        "# CONVERT FROM COLAB INTO PYTHON\n",
        "#############################3\n",
        "def from_colab_to_python():\n",
        "\n",
        "  !pip install ipython\n",
        "  !pip install nbconvert\n",
        "  !ipython nbconvert --to python ./mydeeplearning/ez_mlp.ipynb\n",
        "\n",
        "  #helper = imp.new_module('ez-mlp')\n",
        "  #exec(open('./'+GIT_REPOSITORY+\"/ez-mlp.py\").read(), helper.__dict__)\n",
        "\n",
        "\n",
        "######################################\n",
        "# PUSH CHANGES TO GITHUB\n",
        "##################################\n",
        "def push_github():\n",
        "  !git add -u\n",
        "  !git commit -m \"new commit\"\n",
        "  !git push mydeeplearning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7of0_y_zsQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "######################    \n",
        "# MAIN #\n",
        "######################\n",
        "def mainn():\n",
        "\n",
        "  # Create the network, define the criterion and optimizer\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  dataset = 'MINST'\n",
        "\n",
        "  init_github()\n",
        "  from_colab_to_python()\n",
        "  test_tune_train_network(dataset)\n",
        "\n",
        "  #CONTINUE FROM PREVIOUS MODEL\n",
        "  #model , checkpoint , filepath = load_checkpoint('/content/drive/My Drive/Colab Notebooks/models/MINST/01_Mar_2020_18_42_35','checkpoint_',True,0)\n",
        "  #PreviousCheckPointId = checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])\n",
        "  #PreviousValidationLoss= checkpoint['ValidationLoss'] #np.Inf\n",
        "\n",
        "  #model = Network(784, 10, [512, 256, 128 , 64 ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "\n",
        "  #criterion = nn.NLLLoss()\n",
        "  #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  #checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "\n",
        "\n",
        "  #Upload checkpoint to local drive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "  #Upload to gDrive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',np.Inf,epochs=10)\n",
        "\n",
        "  #plotLossTrend (train_lossess , valid_lossess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mp5Be63OjI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mainn()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}