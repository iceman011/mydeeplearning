{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQwsG6jW5PDaISKfBkIHfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceman011/mydeeplearning/blob/master/ez_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXy2soVzPQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "import datetime\n",
        "import os\n",
        "import pdb\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "from google.colab import drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from torch import optim\n",
        "import itertools\n",
        "#!/usr/bin/env python3\n",
        "import mmap\n",
        "import re\n",
        "from itertools import dropwhile, product\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers,output_classes,transform,dataset, drop_p=0.5,lr =0.001, train_on_gpu=False,network_type ='normal'):\n",
        "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Using CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Using GPU ...')\n",
        "            \n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.drop_ratio = drop_p\n",
        "        self.output_classes = output_classes\n",
        "        self.transform=transform\n",
        "        self.dataset=dataset\n",
        "        self.network_type= network_type\n",
        "\n",
        "        # Input to a hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "        \n",
        "        # Add a variable number of more hidden layers\n",
        "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "        \n",
        "        if ( network_type = 'densenet121'):\n",
        "          assert input_size != 1024 ,  'input features of pre-trained networks must bet 1024'\n",
        "          self.output = models.densenet121(pretrained=True)\n",
        "        else:\n",
        "          self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "        \n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "######################    \n",
        "# LOADING DATA #\n",
        "######################\n",
        "def LoadData(datasetName,batch_size=20,valid_size = 0.2,network_type='normal'):\n",
        "      \n",
        "    # number of subprocesses to use for data loading\n",
        "    num_workers = 0\n",
        "\n",
        "    # convert data to torch.FloatTensor\n",
        "    #transform = transforms.ToTensor()\n",
        "\n",
        "    # convert data to a normalized torch.FloatTensor\n",
        "    transform = None\n",
        "    train_data= None\n",
        "    test_data=None\n",
        "    classes=None\n",
        "    normalize = None\n",
        "\n",
        "    if network_type != 'normal' :\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    elif network_type == 'normal' and datasetName == 'MINST' :\n",
        "      normalize = transforms.Normalize((0.5, ), (0.5, )\n",
        "    elif network_type == 'normal' and datasetName == 'CIFAR' :\n",
        "      normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    if(datasetName == 'MINST'):\n",
        "       # convert data to a normalized torch.FloatTensor\n",
        "      transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          #transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),          \n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.MNIST(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "      test_data = datasets.MNIST(root='data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "      # specify the image classes\n",
        "      classes = ['1', '2', '3', '4', '5',\n",
        "           '6', '7', '8', '9', '10']\n",
        "    elif(datasetName == 'CIFAR'):\n",
        "      \n",
        "      # convert data to a normalized torch.FloatTensor\n",
        "      transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "      test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "      # specify the image classes\n",
        "      classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "        sampler=valid_sampler, num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "        num_workers=num_workers)\n",
        "    return train_loader , valid_loader ,test_loader ,classes , transform\n",
        "\n",
        "\n",
        "#############################\n",
        "# VALIDATE MODEL\n",
        "############################\n",
        "def validation(model, validationloader, criterion):\n",
        "    accuracy = 0\n",
        "    validation_loss = 0\n",
        "    validate_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : ','Starting Validation....')\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if model.train_on_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "      for images, labels in validationloader:\n",
        "\n",
        "          if model.train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "          \n",
        "          images = images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "          output = model.forward(images)\n",
        "          validation_loss += criterion(output, labels).item()\n",
        "\n",
        "          ## Calculating the accuracy \n",
        "          # Model's output is log-softmax, take exponential to get the probabilities\n",
        "          ps = torch.exp(output)\n",
        "          # Class with highest probability is our predicted class, compare with true label\n",
        "          #equality = (labels.data == ps.max(1)[1])\n",
        "          # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "          #accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "\n",
        "\n",
        "        \n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))      \n",
        "        \n",
        "      validation_loss = validation_loss/len(validationloader)\n",
        "      accuracy = 100. * accuracy/len(validationloader)\n",
        "    \n",
        "    print('Finished Validation In ',datetime.timedelta(seconds = time.time() - validate_start_time) )\n",
        "    return validation_loss, accuracy\n",
        "\n",
        "#############################\n",
        "# TEST MODEL\n",
        "############################\n",
        "def test(model,test_loader,criterion,checkpoint,outputfilepath,batch_size,override_checkpoint):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(model.output_size))\n",
        "    class_total = list(0. for i in range(model.output_size))\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Testing....')\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      model.eval()\n",
        "      # iterate over test data\n",
        "      for data, target in test_loader:\n",
        "          # move tensors to GPU if CUDA is available\n",
        "          if model.train_on_gpu:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          \n",
        "          # Flatten images into a 784 long vector\n",
        "          data.resize_(data.size()[0], model.input_size)\n",
        "\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the batch loss\n",
        "          loss = criterion(output, target)\n",
        "          # update test loss \n",
        "          test_loss += loss.item()*data.size(0)\n",
        "          # convert output probabilities to predicted class\n",
        "          _, pred = torch.max(output, 1)    \n",
        "          # compare predictions to true label\n",
        "          correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "          correct = np.squeeze(correct_tensor.numpy()) if not model.train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "          # calculate test accuracy for each object class\n",
        "          for i in range(batch_size):\n",
        "              label = target.data[i]\n",
        "              class_correct[label] += correct[i].item()\n",
        "              class_total[label] += 1\n",
        "\n",
        "      # average test loss\n",
        "      print('Finished Testing during ',datetime.timedelta(seconds=time.time() - test_start_time))\n",
        "      test_loss = test_loss/len(test_loader.dataset)\n",
        "      print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "      \n",
        "      test_checkpoint = dict()\n",
        "\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update({'TestLoss': test_loss})\n",
        "      else :\n",
        "        test_checkpoint.update({'TestLoss': test_loss})\n",
        "\n",
        "      for i in range(model.output_size):\n",
        "          if class_total[i] > 0:\n",
        "            current_key  = 'Test Accuracy of '+model.output_classes[i]\n",
        "            current_val = '{:.3f}% ({}/{})'.format(100 *( class_correct[i] / class_total[i]),\n",
        "                  np.sum(class_correct[i]), np.sum(class_total[i]))\n",
        "            ele={current_key:current_val}\n",
        "            if(override_checkpoint):\n",
        "              checkpoint.update(ele)\n",
        "              print(current_key,current_val)\n",
        "            else:\n",
        "              test_checkpoint.update(ele)\n",
        "\n",
        "\n",
        "          \"\"\"else:\n",
        "              print('Test Accuracy of %5s: N/A (no training examples)' % (model.output_classes[i]))\n",
        "          \"\"\"\n",
        "\n",
        "      current_key = 'Test Accuracy (Overall): '#.format(100. * np.sum(class_correct) / np.sum(class_total))\n",
        "      current_val = '{:.3f}% ({}/{})'.format(100 * (np.sum(class_correct) / np.sum(class_total)),\n",
        "          np.sum(class_correct), np.sum(class_total) )\n",
        "      ele={current_key:current_val}\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update(ele)      \n",
        "        print(current_key,current_val)\n",
        "      else:\n",
        "        test_checkpoint.update(ele)\n",
        "        test_checkpoint.update({'Detailed ':checkpoint})\n",
        "\n",
        "    \n",
        "    #model.train()\n",
        "    if(override_checkpoint):\n",
        "      print('Overriding Checkpoint')\n",
        "      torch.save(checkpoint,outputfilepath)\n",
        "\n",
        "    return checkpoint , test_checkpoint\n",
        "\n",
        "#############################\n",
        "# TRAIN MODEL\n",
        "############################\n",
        "def train(model, trainloader, validationloader, criterion, optimizer, uploadToGDrive,checkpointPath,PreviousCheckPointId,\n",
        "          PreviousValidationLoss,start_time,exp_id,epochs=5, print_every=40):\n",
        "    # monitor training loss    \n",
        "    steps = 0    \n",
        "    #start_time = time.time()\n",
        "    #dateTimeObj = datetime.now()\n",
        "    #start_time_timestamp = './results/'+dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    #start_time_timestamp = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    train_losses, valid_losses = [], []\n",
        "    checkpoint = dict()\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training using Model Parameters \\n '+str(model)+' \\n from PreviousModel '+ PreviousCheckPointId + ' with validationLoss '+ str(PreviousValidationLoss) )\n",
        "    valid_loss_min = PreviousValidationLoss #np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "    for e in range(epochs):        \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training of Epoch'+str(e)  )\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            \n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if model.train_on_gpu:\n",
        "              images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Flatten images into a 784 long vector\n",
        "            images.resize_(images.size()[0], model.input_size)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        train_losses.append(train_loss)\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # Model in inference mode, dropout is off\n",
        "        model.eval()\n",
        "        print(' Finished Training of Epoch '+str(e),' In ',datetime.timedelta(seconds = time.time() - epoch_start_time) )\n",
        "\n",
        "        # Turn off gradients for validation, will speed up inference\n",
        "        with torch.no_grad():\n",
        "            valid_loss, accuracy = validation(model, validationloader, criterion)\n",
        "        \n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print('{} : Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy : {:.6f}'.format(\n",
        "            dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"),\n",
        "            e+1, \n",
        "            train_loss,\n",
        "            valid_loss,\n",
        "            accuracy\n",
        "            ))\n",
        "        \n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f} ,)  Accuracy: {:.6f}  TimeElapsed: {:.6f}.  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss,\n",
        "            accuracy,\n",
        "            (time.time() - start_time)))\n",
        "            \n",
        "            \n",
        "\n",
        "            checkpoint = {'InputSize': model.input_size,\n",
        "                  'OutputSize': model.output_size,\n",
        "                  'HiddenLayers': [each.out_features for each in model.hidden_layers],\n",
        "                  'LearningRate':model.learning_rate,\n",
        "                  'DropRatio':model.drop_ratio,\n",
        "                  'TrainingLoss' :train_loss,\n",
        "                  'ValidationLoss':valid_loss,\n",
        "                  'ValidationAccuracy':accuracy,\n",
        "                  'ElapsedTime': datetime.timedelta(seconds = time.time() - start_time),\n",
        "                  'Dataset':model.dataset,\n",
        "                  'LastEpoch': e,\n",
        "                  'PreviousCheckPoint': PreviousCheckPointId,\n",
        "                  'GPUState': model.train_on_gpu,                  \n",
        "                  'OutputFolder' : exp_id,\n",
        "                  'CheckPointTimestamp': dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"), #time.time(),\n",
        "                  'OutputFilePrefix' : 'checkpoint_',\n",
        "                  'OutputClasses': model.output_classes,\n",
        "                  'Transforms': model.transform,\n",
        "                  'TrainingLosses' :train_losses,\n",
        "                  'ValidationLosses':valid_losses,\n",
        "                  'StateDictionay': model.state_dict()}\n",
        "            \n",
        "            #print(checkpoint)\n",
        "            save_model(checkpoint,uploadToGDrive,checkpointPath)\n",
        "            valid_loss_min = valid_loss\n",
        "        \n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return train_losses , valid_losses , checkpoint\n",
        "\n",
        "\n",
        "#############################\n",
        "# SAVE MODEL TO GOOGLE DRIVE\n",
        "############################\n",
        "def save_model(checkpoint,uploadToGDrive,checkpointPath):\n",
        "  \n",
        "  file_path = ''\n",
        "  if(uploadToGDrive):\n",
        "    drive.mount('/content/gdrive')\n",
        "    file_path = checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)      \n",
        "    #path = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])+'.pt'\n",
        "    print('Saving Model to ',file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "  else:\n",
        "    file_path=checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "    \n",
        "    print('Saving Model to ',file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "\n",
        "\n",
        "###########################\n",
        "# SKIP CERTAIN ENTERIES FROM ITERTOOLS.PRODUCT\n",
        "#####################33\n",
        "def resume(iterable, sentinel):\n",
        "    yield from dropwhile(lambda x: x != sentinel, iterable)\n",
        "\n",
        "#############################\n",
        "# LOAD LAST EXPERIMENT PARAMS\n",
        "##################################\n",
        "def load_last_exp_param(logPath,checkpointPath):\n",
        "\n",
        "  #with open(checkpointPath+'/3-3-2020.txt') as f:\n",
        "  #    total = f.read()\n",
        "  #    print( total.count('Starting New') )\n",
        "      \n",
        "  f = open(logPath, 'r')\n",
        "  content = f.read()\n",
        "\n",
        "  hidden_layers = re.findall('hidden_layers: \\{(.+?)\\}', content)\n",
        "  hidden_layers = int(hidden_layers[len(hidden_layers)-1])\n",
        "\n",
        "  hidden_layer_width = re.findall('hidden_layer_width: \\{(.+?)\\}', content)\n",
        "  hidden_layer_width = hidden_layer_width[len(hidden_layer_width)-1]\n",
        "\n",
        "  Learning_rate = re.findall('Learning_Rate: \\{(.+?)\\}', content)\n",
        "  Learning_rate = float(Learning_rate[len(Learning_rate)-1])\n",
        "\n",
        "  drop_ratio = re.findall('drop_ratio: \\{(.+?)\\}', content)\n",
        "  drop_ratio = float(drop_ratio[len(drop_ratio)-1])\n",
        "  prev_valid_loss = np.Inf\n",
        "  return_checkpoint = None\n",
        "  return_model = None\n",
        "\n",
        "  #for filename in os.listdir(checkpointPath):\n",
        "  for root, dirs, files in os.walk(checkpointPath):\n",
        "    for filename in files:\n",
        "      model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "      if( float(checkpoint['ValidationLoss']) < prev_valid_loss ):\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "      \n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      hidden_layer_width_local = hidden_layer_width.strip()\n",
        "      hidden_layer_width_local = hidden_layer_width_local[1:len(hidden_layer_width_local)-1]\n",
        "      hidden_layer_width_local = list(map(int, hidden_layer_width_local.split(',')))\n",
        "\n",
        "      if(hidden_layer_width_local == checkpoint['HiddenLayers'] and float(checkpoint['ValidationLoss']) <= prev_valid_loss ):\n",
        "        print(' Found Matching Checkpoint with Last Experiement Parame ValidationLoss:{} hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} '.format(checkpoint['ValidationLoss'],hidden_layers,hidden_layer_width,Learning_rate,drop_ratio))\n",
        "        return_checkpoint = checkpoint\n",
        "        return_model = model\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "        #return checkpoint,model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "  if( not bool(return_checkpoint)): \n",
        "    print(' Not Found Matching Checkpoint with Last Experiement Parameters hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} '.format(hidden_layers,hidden_layer_width,Learning_rate,drop_ratio))    \n",
        "\n",
        "  return return_checkpoint,return_model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "\n",
        "################################\n",
        "# TUNE NETWORK LAYERS AND NO. OF NODES\n",
        "##################################\n",
        "def tune_train_network(dataset,epochs,resumeExp=False,resume_logPath='',resume_checkpointPath='',network_type='normal'):\n",
        "\n",
        "  start_time = time.time()\n",
        "  dateTimeObj = dt.now()\n",
        "  exp_id = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "  #dataset = 'MINST'\n",
        "  checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  \n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size)\n",
        "\n",
        "  nodes_per_layer =  [2048, 1024, 512, 256, 128, 64, 32]\n",
        "  full_learning_rates= [0.1, 0.01, 0.0001, 0.001]\n",
        "  full_drop_ratios= [0.8, 0.5, 0.3, 0.1]\n",
        "  learning_rates = None\n",
        "  drop_ratios = None\n",
        "  PreviousValidationLoss= np.Inf\n",
        "  max_hidden_layers = 3\n",
        "  iter_hidden_layer_nodes = None\n",
        "  iter_hidden_layer_nodes_list = None\n",
        "  model = None\n",
        "  resume_checkpoint = None\n",
        "\n",
        "  if(resumeExp):\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Resuming Experiment With New ID ',exp_id )\n",
        "    resume_checkpoint,model,resum_hidden_layers,resume_hidden_layer_width,resume_Learning_rate,resume_drop_ratio,resume_prev_valid_loss =  load_last_exp_param(resume_logPath,resume_checkpointPath)\n",
        "    max_hidden_layers = resum_hidden_layers  \n",
        "\n",
        "    learning_rates = full_learning_rates[full_learning_rates.index(resume_Learning_rate):]\n",
        "    drop_ratios = full_drop_ratios[full_drop_ratios.index(resume_drop_ratio):]\n",
        "    PreviousValidationLoss = resume_prev_valid_loss\n",
        "    checkpointPath = resume_checkpointPath\n",
        "\n",
        "      #iter_hidden_layer_nodes = resume(itertools.product(nodes_per_layer, repeat=hidden_layers+1), resume_checkpoint['HiddenLayers'])      \n",
        "    #if(bool(resume_checkpoint)):\n",
        "    #  model , resume_checkpoint , filepath = load_checkpoint(checkpointPath,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')])\n",
        "      \n",
        "\n",
        "  global_counter = 0\n",
        "  #VARY HIDDEN LAEYERS\n",
        "  for hidden_layers in range(max_hidden_layers,0,-1):\n",
        "    #VARY HIDDEN LAYER NODES/WIDTH\n",
        "    if(resumeExp ):\n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat= hidden_layers)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      resume_hidden_layer_width_list = tuple(list(map(int, resume_hidden_layer_width.split(','))))\n",
        "      iter_hidden_layer_nodes_list = iter_hidden_layer_nodes_list[iter_hidden_layer_nodes_list.index(resume_hidden_layer_width_list):]\n",
        "      \n",
        "      print('Resume ValidationLoss ',PreviousValidationLoss,'Resume Learning Rate Array ',learning_rates ,' Resume Drop Ratio List ',drop_ratios , ' Resume Hidden Nodes List with Size :',len(iter_hidden_layer_nodes_list) , ' Starting From :',iter_hidden_layer_nodes_list[0] )\n",
        "    else:      \n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat=hidden_layers+1)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "    for hidden_layer in iter_hidden_layer_nodes_list:\n",
        "      #VARY LEARNING RATE\n",
        "      if( not resumeExp):\n",
        "        learning_rates = full_learning_rates\n",
        "      for lr in learning_rates :\n",
        "        #VARY DROP RATIO\n",
        "        if( not resumeExp):\n",
        "          drop_ratios = full_drop_ratios\n",
        "        for drop_ratio in drop_ratios :\n",
        "          \n",
        "          print('Global Counter: '+str(global_counter)+' Network Type: '+network_type+' EXP-ID : '+exp_id+'  DurationSinceStart :',datetime.timedelta(seconds = time.time() - start_time),' Starting New Experiment with %hidden_layers: {',hidden_layers,'}% %hidden_layer_width: {',hidden_layer ,'}% %Learning_Rate: {',lr, '}% %drop_ratio: {',drop_ratio,'}%')\n",
        "          #model = Network(784, 10, [first_layer, second_layer, third_layer ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "          if( not bool(resume_checkpoint) or not resumeExp ):\n",
        "            if(dataset == 'MINST'):\n",
        "              model = Network(784, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "            else:\n",
        "              model = Network((32*32*3), 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "\n",
        "\n",
        "          criterion = nn.NLLLoss()\n",
        "          optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "          #Upload checkpoint to local drive\n",
        "          #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "          #Upload to gDrive\n",
        "          train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',PreviousValidationLoss,start_time,exp_id,epochs=epochs)\n",
        "          if( bool(checkpointt) and checkpointt['ValidationLoss'] < PreviousValidationLoss ):\n",
        "            PreviousValidationLoss = checkpointt['ValidationLoss']\n",
        "          global_counter+= 1\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        resumeExp=False\n",
        "\n",
        "\n",
        "#############################\n",
        "# PLOT TRAINING LOSS VS VALIDATION LOSS \n",
        "############################\n",
        "def plotLossTrend(train_losses,validation_losses,test_losses=[]):\n",
        "  #TRAINING LOSS DATA\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in train_losses:    \n",
        "    for key, value in item.items():\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Training Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #VALIDATON LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in validation_losses:   \n",
        "    #pdb.set_trace() \n",
        "    for key, value in item.items():      \n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Validation Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #TEST LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in test_losses:    \n",
        "    for key, value in item.items():\n",
        "      #pdb.set_trace()\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Test Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()\n",
        "  #for i, v in enumerate(values):\n",
        "  #    ax.text(i, v+25, \"%d\" %v, ha=\"center\")\n",
        "  #plt.ylim(-10, 595)\n",
        "\n",
        "#############################\n",
        "# LOAD CHECKPOINT\n",
        "############################\n",
        "def load_checkpoint(OutputFolder,OutputFilePrefix,max_epoch,exact_epoch,load_model=True):\n",
        "    \n",
        "    file_epoch = 0\n",
        "    if(max_epoch):\n",
        "      for filename in os.listdir(OutputFolder):\n",
        "        data = filename.split('_')\n",
        "        tmp=int(data[1][:-3])\n",
        "        #print('data[1]',data[1],' tmp[:-3] ',tmp[:-3])\n",
        "        if( file_epoch <= tmp ):\n",
        "          file_epoch = tmp\n",
        "    else:\n",
        "      file_epoch=exact_epoch\n",
        "\n",
        "    filepath = OutputFolder+'/'+OutputFilePrefix + str(file_epoch)+'.pt'\n",
        "    print('Loading Checkpoint from '+filepath+'...')\n",
        "    indent=1\n",
        "    checkpoint = torch.load(filepath)\n",
        "    for key, value in checkpoint.items():\n",
        "        if(key == 'StateDictionay'):\n",
        "          continue\n",
        "        print('\\t' * indent + str(key),'\\t' * (indent+1) + str(value))\n",
        "        #print('\\t' * (indent+1) + str(value))\n",
        "    \n",
        "    testmodel = None\n",
        "\n",
        "    if(load_model):\n",
        "      testmodel = Network(checkpoint['InputSize'],\n",
        "                              checkpoint['OutputSize'],\n",
        "                              checkpoint['HiddenLayers'],\n",
        "                              checkpoint['OutputClasses'],\n",
        "                              checkpoint['Transforms'],\n",
        "                              checkpoint['Dataset'],\n",
        "                              checkpoint['DropRatio'],\n",
        "                              checkpoint['LearningRate'],\n",
        "                              train_on_gpu=checkpoint['GPUState']\n",
        "                      )\n",
        "      testmodel.load_state_dict(checkpoint['StateDictionay'])\n",
        "      \n",
        "    return testmodel , checkpoint , filepath\n",
        "\n",
        "\n",
        "def test_all_epochs(OutputFolder,test_loader,criterion,batch_size,OutputFilePrefix='checkpoint_'):\n",
        "\n",
        "    #OutputFolder = './results/'\n",
        "    file_epoch = 0\n",
        "    result_list=[]\n",
        "    print('Starting Test All Saved Epochs ....')\n",
        "    trainig_loss=[]\n",
        "    validation_loss=[]\n",
        "    test_loss=[]\n",
        "    ele = dict()\n",
        "\n",
        "    \"\"\"\n",
        "        for folder in os.listdir(OutputFolder):\n",
        "          for filename in os.listdir(OutputFolder+folder):\n",
        "            data = filename.split('_')\n",
        "            #pdb.set_trace()\n",
        "            print(data)\n",
        "            file_epoch=int(data[1][:-3])\n",
        "\n",
        "            model , checkpoint , filepath = load_checkpoint(OutputFolder+folder,OutputFilePrefix,False,file_epoch)\n",
        "    \"\"\"\n",
        "    #for filename in os.listdir(checkpointPath):\n",
        "    for root, dirs, files in os.walk(OutputFolder):\n",
        "      for filename in files:\n",
        "        model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "\n",
        "        checkpoint , checkpoint_test = test(model,test_loader,criterion,checkpoint,filepath,batch_size,False)\n",
        "\n",
        "        print('Test Accuracy ',checkpoint_test['Test Accuracy (Overall): '])\n",
        "        result_list.append(checkpoint_test)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['TrainingLoss'])\n",
        "        key = checkpoint['Dataset'] + '_' + checkpoint['OutputFolder'] + '_' + str(checkpoint['LastEpoch'])\n",
        "        ele = {key: checkpoint['TrainingLoss']}\n",
        "        trainig_loss.append(ele)\n",
        "        \n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['ValidationLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint['ValidationLoss']}\n",
        "        validation_loss.append(ele)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint_test['TestLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint_test['TestLoss']}\n",
        "        test_loss.append(ele)\n",
        "    \n",
        "    #print('Result List',result_list)\n",
        "    plotLossTrend (trainig_loss,validation_loss,test_loss)\n",
        "    return result_list\n",
        "      \n",
        "\n",
        "#############################\n",
        "# VISUALIZE ALL IMAGES IN BATCH\n",
        "############################\n",
        "def visualize_images_in_batch(loader,batch_size) :\n",
        "    # obtain one batch of training images\n",
        "\t\tdataiter = iter(loader)\n",
        "\t\timages, labels = dataiter.next()\n",
        "\t\timages = images.numpy()\n",
        "\n",
        "\t\t# plot the images in the batch, along with the corresponding labels\n",
        "\t\tfig = plt.figure(figsize=(25, 4))\n",
        "\t\tfor idx in np.arange(batch_size):\n",
        "\t\t\tax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\t\t\tax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "\t\t\t# print out the correct label for each image\n",
        "\t\t\t# .item() gets the value contained in a Tensor\n",
        "\t\t\tax.set_title(str(labels[idx].item()))\n",
        "\t\treturn images\n",
        "\n",
        "\n",
        "#############################\n",
        "# VISUALIZE PIXEL OF AN IMAGE RETURN FROM PREV FUNCTION\n",
        "############################\n",
        "def visualize_image_pixels_value(image):\n",
        "\n",
        "  img = np.squeeze(image)\n",
        "  fig = plt.figure(figsize = (12,12)) \n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.imshow(img, cmap='gray')\n",
        "  width, height = img.shape\n",
        "  thresh = img.max()/2.5\n",
        "  for x in range(width):\n",
        "      for y in range(height):\n",
        "          val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "          ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center',\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_rgb_image_pixels_value(image):\n",
        "  rgb_img = np.squeeze(image)\n",
        "  channels = ['red channel', 'green channel', 'blue channel']\n",
        "\n",
        "  fig = plt.figure(figsize = (36, 36)) \n",
        "  for idx in np.arange(rgb_img.shape[0]):\n",
        "      ax = fig.add_subplot(1, 3, idx + 1)\n",
        "      img = rgb_img[idx]\n",
        "      ax.imshow(img, cmap='gray')\n",
        "      ax.set_title(channels[idx])\n",
        "      width, height = img.shape\n",
        "      thresh = img.max()/2.5\n",
        "      for x in range(width):\n",
        "          for y in range(height):\n",
        "              val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "              ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center', size=8,\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_test_results(model,test_loader,batch_size,RGB=False):\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  # move model inputs to cuda, if GPU available\n",
        "  if model.train_on_gpu:\n",
        "      images = images.cuda()\n",
        "\n",
        "  # Flatten images into a 784 long vector\n",
        "  images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "  # get sample outputs\n",
        "  output = model(images)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, preds_tensor = torch.max(output, 1)\n",
        "  preds = np.squeeze(preds_tensor.numpy()) if not model.train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "  # plot the images in the batch, along with predicted and true labels\n",
        "  fig = plt.figure(figsize=(25, 4))\n",
        "  for idx in np.arange(batch_size):\n",
        "      ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "      image_cpu = images.cpu()[idx]\n",
        "      image_cpu = image_cpu / 2 + 0.5  # unnormalize\n",
        "      if(RGB):\n",
        "        plt.imshow(np.transpose(image_cpu, (1, 2, 0)))  # convert from Tensor image\n",
        "      else:\n",
        "        ax.imshow(np.squeeze(image_cpu), cmap='gray')\n",
        "\n",
        "      ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                  color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voDNqAjioodn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# TEST TUNE NETWORK ####################################\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\"\"\"\n",
        "def test_tune_train_network(dataset):    \n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "  #tune_train_network(resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "  #tune_train_network('MINST',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt','/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #sys.path.append('./mydeeplearning/')\n",
        "  \n",
        "  #MINST\n",
        "  #cmd_str = \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/') 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt'\" \n",
        "  func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/all/')\\\"\" \n",
        "  log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume.txt'\"\n",
        "\n",
        "\n",
        "  #CIFAR\n",
        "  #func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\\\"\" \n",
        "  #log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt'\"\n",
        "\n",
        "  print(func_str)\n",
        "  print(log_path)\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network('CIFAR',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/06_Mar_2020_07_57_24/')\" 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt'\n",
        "  !python -c $func_str 2>&1 | tee -a $log_path\n",
        "\n",
        "\n",
        "  #!python -c $cmd_str\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\" -u /mydeeplearning/ez_mlp.py 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt' #2>&1\n",
        "  \n",
        "  \n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs','/content/gdrive/My Drive/Colab Notebooks/models/MINST/04_Mar_2020_20_13_34')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXvRQ0zv5hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### TEST ALL EPOCHS ##############################\n",
        "def test_test_all_epochs():\n",
        "    \n",
        "\n",
        "  #model , checkpointtt , filepathh = load_checkpoint(checkpointttt['OutputFolder'],checkpointttt['OutputFilePrefix'],False,10)\n",
        "\n",
        "  #result_list = test_all_epochs(test_loader,criterion,batch_size)\n",
        "\n",
        "  dataset = 'MINST'\n",
        "\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  dataset = 'MINST'\n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size)\n",
        "\n",
        "\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  gfolder = '/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/'\n",
        "\n",
        "  localfolder = './results/'+dataset+'/'\n",
        "\n",
        "  result_list = test_all_epochs(gfolder,test_loader,nn.NLLLoss(),batch_size,OutputFilePrefix='checkpoint_')\n",
        "\n",
        "  #checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks/models/minst/29_Feb_2020_20_06_50_74.pt')\n",
        "\n",
        "  #print(checkpoint)\n",
        "  #load_checkpoint('/gdrive/My Drive/Colab Notebooks/models/'+dataset,'29_Feb_2020_20_06_50',False,74)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu907NvWVlBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## TEST VISUALIZATION###################\n",
        "def test_visualize_images_in_batch():\n",
        "  images = visualize_images_in_batch(train_loader,batch_size)\n",
        "  visualize_image_pixels_value(images[2])\n",
        "\n",
        "  model , checkpointtt , filepathh = load_checkpoint(checkpointt['OutputFolder'],checkpointt['OutputFilePrefix'],True,0)\n",
        "\n",
        "  checkpointttt = test(model,test_loader,criterion,checkpointtt,filepathh,batch_size,True)\n",
        "\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  #visualize_rgb_image_pixels_value(images[2])\n",
        "  #visualize_test_results(model,test_loader,batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbNJdUIMSTBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository setup\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
        "from os.path import join  \n",
        "#import helper.py\n",
        "import imp \n",
        "\n",
        "\n",
        "################################\n",
        "# INIT GITHUB\n",
        "#######################################\n",
        "def init_github():\n",
        "  \"\"\"\n",
        "  !git init\n",
        "  !git config  global user.email ice.man011@gmail.com\n",
        "  !git config  global user.name iceman011\n",
        "  !git add -A\n",
        "  !git commit -m first commit\n",
        "  !git remote add origin https://<username>:<password>github@github.com/iceman011/mydeeplearning.git\n",
        "  !git push -u origin master\n",
        "  \"\"\"\n",
        "  #!git clone -l -s git://github.com/iceman011/mydeeplearning.git #mydeeplearning-repo\n",
        "  #!git clone -l -s https://github.com/iceman011@github.com/mydeeplearning.git\n",
        "\n",
        "  #%cd mydeeplearning-repo\n",
        "  #!ls\n",
        "\n",
        "  # path to your project on Google Drive\n",
        "  #MY_GOOGLE_DRIVE_PATH = 'My Drive/MyDrive/Udacity/deep-learning-v2-pytorch' \n",
        "  # replace with your Github username \n",
        "  GIT_USERNAME = \"iceman011\" \n",
        "  # definitely replace with your\n",
        "  GIT_TOKEN = \"1aeb0c6f424e3c604988a3a636f74c6e5180cd89\"  \n",
        "  # Replace with your github repository in this case we want \n",
        "  # to clone deep-learning-v2-pytorch repository\n",
        "  GIT_REPOSITORY = \"mydeeplearning\" \n",
        "\n",
        "  # REMOVE IT BEFORE INIT\n",
        "  if( os.path.isdir('./'+GIT_REPOSITORY) ):\n",
        "    shutil.rmtree('./'+GIT_REPOSITORY)\n",
        "\n",
        "  #PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "  # It's good to print out the value if you are not sure \n",
        "  #print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "  # In case we haven't created the folder already; we will create a folder in the project path \n",
        "  #!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "  #GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "\n",
        "  GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "  print(\"GIT_PATH: \", GIT_PATH)\n",
        "\n",
        "  !git clone \"{GIT_PATH}\" # clone the github repository\n",
        "\n",
        "############################3\n",
        "# CONVERT FROM COLAB INTO PYTHON\n",
        "#############################3\n",
        "def from_colab_to_python():\n",
        "\n",
        "  !pip install ipython\n",
        "  !pip install nbconvert\n",
        "  !ipython nbconvert --to python ./mydeeplearning/ez_mlp.ipynb\n",
        "\n",
        "  #helper = imp.new_module('ez-mlp')\n",
        "  #exec(open('./'+GIT_REPOSITORY+\"/ez-mlp.py\").read(), helper.__dict__)\n",
        "\n",
        "\n",
        "######################################\n",
        "# PUSH CHANGES TO GITHUB\n",
        "##################################\n",
        "def push_github():\n",
        "  !git add -u\n",
        "  !git commit -m \"new commit\"\n",
        "  !git push mydeeplearning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7of0_y_zsQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "######################    \n",
        "# MAIN #\n",
        "######################\n",
        "def main():\n",
        "\n",
        "  # Create the network, define the criterion and optimizer\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  dataset = 'MINST'\n",
        "\n",
        "  init_github()\n",
        "  from_colab_to_python()\n",
        "  test_tune_train_network(dataset)\n",
        "\n",
        "  #CONTINUE FROM PREVIOUS MODEL\n",
        "  #model , checkpoint , filepath = load_checkpoint('/content/drive/My Drive/Colab Notebooks/models/MINST/01_Mar_2020_18_42_35','checkpoint_',True,0)\n",
        "  #PreviousCheckPointId = checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])\n",
        "  #PreviousValidationLoss= checkpoint['ValidationLoss'] #np.Inf\n",
        "\n",
        "  #model = Network(784, 10, [512, 256, 128 , 64 ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "\n",
        "  #criterion = nn.NLLLoss()\n",
        "  #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  #checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "\n",
        "\n",
        "  #Upload checkpoint to local drive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "  #Upload to gDrive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',np.Inf,epochs=10)\n",
        "\n",
        "  #plotLossTrend (train_lossess , valid_lossess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mp5Be63OjI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26bfe6a8-311c-44f5-f35d-ee84a4e82e67"
      },
      "source": [
        "#if __name__ == '__main__':\n",
        "main()\n",
        "\n",
        "#tune_train_network('MINST',10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/all/')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "GIT_PATH:  https://1aeb0c6f424e3c604988a3a636f74c6e5180cd89@github.com/iceman011/mydeeplearning.git\n",
            "Cloning into 'mydeeplearning'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 165 (delta 76), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (165/165), 759.61 KiB | 2.51 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython) (45.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython) (4.4.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython) (0.1.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython) (1.12.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython) (0.6.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (5.6.1)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (5.0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.4.4)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.3.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.1.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert) (2.11.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert) (4.6.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert) (1.12.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
            "[TerminalIPythonApp] WARNING | Subcommand `ipython nbconvert` is deprecated and will be removed in future versions.\n",
            "[TerminalIPythonApp] WARNING | You likely want to use `jupyter nbconvert` in the future\n",
            "[NbConvertApp] Converting notebook ./mydeeplearning/ez_mlp.ipynb to python\n",
            "[NbConvertApp] Writing 43900 bytes to ./mydeeplearning/ez_mlp.py\n",
            "Mounted at /content/gdrive\n",
            "\"from mydeeplearning.ez_mlp import*; tune_train_network('MINST',10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/all/')\"\n",
            "'/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume.txt'\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
            "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n",
            "9920512it [00:03, 2505256.55it/s]                             \n",
            "32768it [00:00, 111956.62it/s]           \n",
            "1654784it [00:00, 2393440.79it/s]                           \n",
            "8192it [00:00, 44601.03it/s]            Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "14_Mar_2020_07_42_02 : Resuming Experiment With New ID  14_Mar_2020_07_41_56\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_14_16_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t20.977743203709522\n",
            "\tValidationLoss \t\t2.313524465560913\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t41.749404191970825\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_14_16\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[20.977743203709522]\n",
            "\tValidationLosses \t\t[2.313524465560913]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_14_46_1.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11590311275422573\n",
            "\tValidationLoss \t\t2.3115229586760204\n",
            "\tValidationAccuracy \t\ttensor(10.1750)\n",
            "\tElapsedTime \t\t71.41641139984131\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t1\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_14_46\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[20.977743203709522, 0.11590311275422573]\n",
            "\tValidationLosses \t\t[2.313524465560913, 2.3115229586760204]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_15_15_2.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.1158978755325079\n",
            "\tValidationLoss \t\t2.3063833125432334\n",
            "\tValidationAccuracy \t\ttensor(10.4083)\n",
            "\tElapsedTime \t\t101.25275611877441\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t2\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_15_15\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[20.977743203709522, 0.11590311275422573, 0.1158978755325079]\n",
            "\tValidationLosses \t\t[2.313524465560913, 2.3115229586760204, 2.3063833125432334]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_25_05_2.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.3\n",
            "\tTrainingLoss \t\t0.11583339028060437\n",
            "\tValidationLoss \t\t2.306087704102198\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t691.1725616455078\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t2\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_25_05\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[281.5077789706091, 0.11579228625694911, 0.11583339028060437]\n",
            "\tValidationLosses \t\t[2.3158168717225394, 2.315196233590444, 2.306087704102198]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_33_53_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.01\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11912868322432041\n",
            "\tValidationLoss \t\t2.301278197367986\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t1218.9278683662415\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_33_53\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11912868322432041]\n",
            "\tValidationLosses \t\t[2.301278197367986]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_34_51_2.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.01\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11515133934219678\n",
            "\tValidationLoss \t\t2.300851330757141\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t1276.9346287250519\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t2\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_34_51\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11912868322432041, 0.11514817476272583, 0.11515133934219678]\n",
            "\tValidationLosses \t\t[2.301278197367986, 2.3015719799200696, 2.300851330757141]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_53_31_1.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.1150956140657266\n",
            "\tValidationLoss \t\t2.300832857290904\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t2396.4296135902405\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t1\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_53_31\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11536191315452257, 0.1150956140657266]\n",
            "\tValidationLosses \t\t[2.3014506928126015, 2.300832857290904]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_20_59_43_4.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11508187551796437\n",
            "\tValidationLoss \t\t2.3008144863446556\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t2768.5276823043823\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t4\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_20_59_43\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.1151658234645923, 0.11508367783327897, 0.11508282598853112, 0.11508216535051664, 0.11508187551796437]\n",
            "\tValidationLosses \t\t[2.301037000815074, 2.301236010789871, 2.3010030853748322, 2.300855881770452, 2.3008144863446556]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_21_05_27_6.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.3\n",
            "\tTrainingLoss \t\t0.11507549087703228\n",
            "\tValidationLoss \t\t2.300806434949239\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t3112.7151877880096\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t6\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_21_05_27\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11511238146324952, 0.11508220896124839, 0.11508225373923779, 0.11508028693993887, 0.11507841654618581, 0.11507487197220326, 0.11507549087703228]\n",
            "\tValidationLosses \t\t[2.3013989722728727, 2.300894759496053, 2.3010033023357392, 2.30092342098554, 2.300829785267512, 2.3008612902959187, 2.300806434949239]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/04_Mar_2020_20_13_34/checkpoint_04_Mar_2020_21_06_24_8.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.3\n",
            "\tTrainingLoss \t\t0.11507420448462169\n",
            "\tValidationLoss \t\t2.3007216640313466\n",
            "\tValidationAccuracy \t\ttensor(11.4000)\n",
            "\tElapsedTime \t\t3169.854605436325\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t8\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t04_Mar_2020_20_13_34\n",
            "\tCheckPointTimestamp \t\t04_Mar_2020_21_06_24\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11511238146324952, 0.11508220896124839, 0.11508225373923779, 0.11508028693993887, 0.11507841654618581, 0.11507487197220326, 0.11507549087703228, 0.11507090218365193, 0.11507420448462169]\n",
            "\tValidationLosses \t\t[2.3013989722728727, 2.300894759496053, 2.3010033023357392, 2.30092342098554, 2.300829785267512, 2.3008612902959187, 2.300806434949239, 2.301010252634684, 2.3007216640313466]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/05_Mar_2020_17_58_54/checkpoint_05_Mar_2020_17_59_46_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t13.874534028187394\n",
            "\tValidationLoss \t\t2.3078386274973552\n",
            "\tValidationAccuracy \t\ttensor(10.0250)\n",
            "\tElapsedTime \t\t51.979976415634155\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t05_Mar_2020_17_58_54\n",
            "\tCheckPointTimestamp \t\t05_Mar_2020_17_59_46\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[13.874534028187394]\n",
            "\tValidationLosses \t\t[2.3078386274973552]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_54_39/checkpoint_06_Mar_2020_07_55_13_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t19.242717800756296\n",
            "\tValidationLoss \t\t2.3120368576049803\n",
            "\tValidationAccuracy \t\ttensor(10.3833)\n",
            "\tElapsedTime \t\t33.455825328826904\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_54_39\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_07_55_13\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[19.242717800756296]\n",
            "\tValidationLosses \t\t[2.3120368576049803]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_54_39/checkpoint_06_Mar_2020_07_56_44_3.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.1159083252598842\n",
            "\tValidationLoss \t\t2.309082218805949\n",
            "\tValidationAccuracy \t\ttensor(9.8166)\n",
            "\tElapsedTime \t\t124.79307007789612\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t3\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_54_39\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_07_56_44\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[19.242717800756296, 0.11581980217496554, 0.11588552070657412, 0.1159083252598842]\n",
            "\tValidationLosses \t\t[2.3120368576049803, 2.3197654072443643, 2.3206359187761945, 2.309082218805949]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_06_Mar_2020_07_57_57_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.1\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t25.76125770568351\n",
            "\tValidationLoss \t\t2.306779959599177\n",
            "\tValidationAccuracy \t\ttensor(9.9417)\n",
            "\tElapsedTime \t\t33.66304087638855\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_57_24\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_07_57_57\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[25.76125770568351]\n",
            "\tValidationLosses \t\t[2.306779959599177]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_06_Mar_2020_08_00_00_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.01\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11934958743552367\n",
            "\tValidationLoss \t\t2.302425160010656\n",
            "\tValidationAccuracy \t\ttensor(10.3750)\n",
            "\tElapsedTime \t\t156.75316905975342\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_57_24\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_08_00_00\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11934958743552367]\n",
            "\tValidationLosses \t\t[2.302425160010656]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_06_Mar_2020_08_02_04_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.1153421110411485\n",
            "\tValidationLoss \t\t2.3012513740857443\n",
            "\tValidationAccuracy \t\ttensor(11.3917)\n",
            "\tElapsedTime \t\t280.2341685295105\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_57_24\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_08_02_04\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.1153421110411485]\n",
            "\tValidationLosses \t\t[2.3012513740857443]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_06_Mar_2020_08_03_05_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.3\n",
            "\tTrainingLoss \t\t0.11511734959979852\n",
            "\tValidationLoss \t\t2.301214106082916\n",
            "\tValidationAccuracy \t\ttensor(11.3917)\n",
            "\tElapsedTime \t\t341.5318179130554\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_57_24\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_08_03_05\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11511734959979852]\n",
            "\tValidationLosses \t\t[2.301214106082916]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_06_Mar_2020_08_03_36_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.1\n",
            "\tTrainingLoss \t\t0.11510088475545248\n",
            "\tValidationLoss \t\t2.301190451780955\n",
            "\tValidationAccuracy \t\ttensor(11.3917)\n",
            "\tElapsedTime \t\t372.1698338985443\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_57_24\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_08_03_36\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11510088475545248]\n",
            "\tValidationLosses \t\t[2.301190451780955]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_06_Mar_2020_08_05_38_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 1024]\n",
            "\tLearningRate \t\t0.001\n",
            "\tDropRatio \t\t0.1\n",
            "\tTrainingLoss \t\t0.11509947038690249\n",
            "\tValidationLoss \t\t2.3011662662029266\n",
            "\tValidationAccuracy \t\ttensor(11.3917)\n",
            "\tElapsedTime \t\t494.0928854942322\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t06_Mar_2020_07_57_24\n",
            "\tCheckPointTimestamp \t\t06_Mar_2020_08_05_38\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11509947038690249]\n",
            "\tValidationLosses \t\t[2.3011662662029266]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_07_Mar_2020_09_41_02_1.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 512]\n",
            "\tLearningRate \t\t0.01\n",
            "\tDropRatio \t\t0.1\n",
            "\tTrainingLoss \t\t0.11514954202373823\n",
            "\tValidationLoss \t\t2.3011316565672555\n",
            "\tValidationAccuracy \t\ttensor(11.2333)\n",
            "\tElapsedTime \t\t57.15238881111145\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t1\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t07_Mar_2020_09_40_05\n",
            "\tCheckPointTimestamp \t\t07_Mar_2020_09_41_02\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.1260872092942397, 0.11514954202373823]\n",
            "\tValidationLosses \t\t[2.30335329413414, 2.3011316565672555]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/checkpoint_07_Mar_2020_09_54_44_2.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 512]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11507290008167426\n",
            "\tValidationLoss \t\t2.301096880833308\n",
            "\tValidationAccuracy \t\ttensor(11.2333)\n",
            "\tElapsedTime \t\t879.6431925296783\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t2\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t07_Mar_2020_09_40_05\n",
            "\tCheckPointTimestamp \t\t07_Mar_2020_09_54_44\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.115144493902723, 0.11507457349697749, 0.11507290008167426]\n",
            "\tValidationLosses \t\t[2.301135915915171, 2.3013081534703574, 2.301096880833308]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/07_Mar_2020_09_58_57/checkpoint_07_Mar_2020_10_04_17_1.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 512]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11508879185716311\n",
            "\tValidationLoss \t\t2.3010568388303123\n",
            "\tValidationAccuracy \t\ttensor(11.5333)\n",
            "\tElapsedTime \t\t320.57190561294556\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t1\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t07_Mar_2020_09_58_57\n",
            "\tCheckPointTimestamp \t\t07_Mar_2020_10_04_17\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11538717292745908, 0.11508879185716311]\n",
            "\tValidationLosses \t\t[2.301528647343318, 2.3010568388303123]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/07_Mar_2020_09_58_57/checkpoint_07_Mar_2020_10_07_49_9.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 512]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11507154820362726\n",
            "\tValidationLoss \t\t2.3010536726315816\n",
            "\tValidationAccuracy \t\ttensor(11.5333)\n",
            "\tElapsedTime \t\t532.6445009708405\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t9\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t07_Mar_2020_09_58_57\n",
            "\tCheckPointTimestamp \t\t07_Mar_2020_10_07_49\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11538717292745908, 0.11508879185716311, 0.11509019969403744, 0.11509061993161837, 0.11509118069211642, 0.11509961046775181, 0.11507301930089792, 0.11507826224962871, 0.11508845796684423, 0.11507154820362726]\n",
            "\tValidationLosses \t\t[2.301528647343318, 2.3010568388303123, 2.301455497741699, 2.3011225871245067, 2.3013400304317475, 2.3013796178499857, 2.3011358761787415, 2.30127081712087, 2.301152486403783, 2.3010536726315816]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/07_Mar_2020_09_58_57/checkpoint_07_Mar_2020_10_11_50_8.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 512]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11506870467960835\n",
            "\tValidationLoss \t\t2.30102055589358\n",
            "\tValidationAccuracy \t\ttensor(11.5333)\n",
            "\tElapsedTime \t\t773.3333265781403\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t8\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t07_Mar_2020_09_58_57\n",
            "\tCheckPointTimestamp \t\t07_Mar_2020_10_11_50\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11514368245999018, 0.11508080213765304, 0.11507484171787898, 0.11507781574626763, 0.11507756092151006, 0.11507819091777007, 0.1150748208463192, 0.11507202955087026, 0.11506870467960835]\n",
            "\tValidationLosses \t\t[2.30144775390625, 2.3013516223430632, 2.301413631439209, 2.3011373960971833, 2.3012535536289214, 2.3012785947322847, 2.3013112103939055, 2.301148924827576, 2.30102055589358]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/07_Mar_2020_09_58_57/checkpoint_07_Mar_2020_10_14_05_3.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 512]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.3\n",
            "\tTrainingLoss \t\t0.11507319716612498\n",
            "\tValidationLoss \t\t2.3009991105397543\n",
            "\tValidationAccuracy \t\ttensor(11.5333)\n",
            "\tElapsedTime \t\t907.9653749465942\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t3\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t07_Mar_2020_09_58_57\n",
            "\tCheckPointTimestamp \t\t07_Mar_2020_10_14_05\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11512529721856117, 0.11507846306761106, 0.11507514945665995, 0.11507319716612498]\n",
            "\tValidationLosses \t\t[2.3017901225884754, 2.3014088888963062, 2.301205315589905, 2.3009991105397543]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_17_53_18/checkpoint_08_Mar_2020_18_38_20_4.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11507978646953901\n",
            "\tValidationLoss \t\t2.3009497197469075\n",
            "\tValidationAccuracy \t\ttensor(11.4250)\n",
            "\tElapsedTime \t\t0:45:01.736092\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t4\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_17_53_18\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_18_38_20\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11540549431741237, 0.11510577751696109, 0.11509671521683534, 0.11508872573077679, 0.11507978646953901]\n",
            "\tValidationLosses \t\t[2.3018471074104307, 2.301228897571564, 2.301109787225723, 2.301007336775462, 2.3009497197469075]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_17_53_18/checkpoint_08_Mar_2020_18_39_02_5.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11507149637242159\n",
            "\tValidationLoss \t\t2.3009447610378264\n",
            "\tValidationAccuracy \t\ttensor(11.4250)\n",
            "\tElapsedTime \t\t0:45:43.140323\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t5\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_17_53_18\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_18_39_02\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11540549431741237, 0.11510577751696109, 0.11509671521683534, 0.11508872573077679, 0.11507978646953901, 0.11507149637242159]\n",
            "\tValidationLosses \t\t[2.3018471074104307, 2.301228897571564, 2.301109787225723, 2.301007336775462, 2.3009497197469075, 2.3009447610378264]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_17_53_18/checkpoint_08_Mar_2020_18_46_35_6.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11510007476806641\n",
            "\tValidationLoss \t\t2.300929999748866\n",
            "\tValidationAccuracy \t\ttensor(11.4250)\n",
            "\tElapsedTime \t\t0:53:16.225477\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t6\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_17_53_18\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_18_46_35\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11534030141929785, 0.11520791622499625, 0.11512308167914549, 0.11512999221682549, 0.11509456015129885, 0.11509338976442814, 0.11510007476806641]\n",
            "\tValidationLosses \t\t[2.303494987487793, 2.3017048915227254, 2.3009752074877423, 2.3010841902097066, 2.301273048321406, 2.3009478052457175, 2.300929999748866]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_31_58_0.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11509772455692291\n",
            "\tValidationLoss \t\t2.3006331849098207\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:00:51.700213\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t0\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_31_58\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11509772455692291]\n",
            "\tValidationLosses \t\t[2.3006331849098207]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_32_45_2.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11509295508265495\n",
            "\tValidationLoss \t\t2.3004842682679496\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:01:38.190367\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t2\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_32_45\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11509772455692291, 0.11509680426120758, 0.11509295508265495]\n",
            "\tValidationLosses \t\t[2.3006331849098207, 2.3006836251417795, 2.3004842682679496]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_33_54_5.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11508566900591055\n",
            "\tValidationLoss \t\t2.300455356836319\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:02:47.546700\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t5\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_33_54\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11509772455692291, 0.11509680426120758, 0.11509295508265495, 0.11509381775557995, 0.11508415538569292, 0.11508566900591055]\n",
            "\tValidationLosses \t\t[2.3006331849098207, 2.3006836251417795, 2.3004842682679496, 2.300719017982483, 2.3004879438877106, 2.300455356836319]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_37_47_5.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11507850277423859\n",
            "\tValidationLoss \t\t2.3004255441824597\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:06:40.288010\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t5\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_37_47\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11508050655325254, 0.11507943020761013, 0.11508011728525162, 0.11507630161444346, 0.1150815849105517, 0.11507850277423859]\n",
            "\tValidationLosses \t\t[2.3005482856432597, 2.3005296965440114, 2.3005069371064506, 2.300602573553721, 2.300498387813568, 2.3004255441824597]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_38_11_6.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.0001\n",
            "\tDropRatio \t\t0.5\n",
            "\tTrainingLoss \t\t0.11506878552337488\n",
            "\tValidationLoss \t\t2.300424952507019\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:07:03.889380\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t6\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_38_11\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11508050655325254, 0.11507943020761013, 0.11508011728525162, 0.11507630161444346, 0.1150815849105517, 0.11507850277423859, 0.11506878552337488]\n",
            "\tValidationLosses \t\t[2.3005482856432597, 2.3005296965440114, 2.3005069371064506, 2.300602573553721, 2.300498387813568, 2.3004255441824597, 2.300424952507019]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_40_08_1.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11508705192804336\n",
            "\tValidationLoss \t\t2.300394515991211\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:09:00.712669\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t1\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_40_08\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11536910290022691, 0.11508705192804336]\n",
            "\tValidationLosses \t\t[2.3005114909013114, 2.300394515991211]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_41_18_4.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.1150842467546463\n",
            "\tValidationLoss \t\t2.300310746828715\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:10:11.018901\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t4\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_41_18\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11536910290022691, 0.11508705192804336, 0.11509626411398252, 0.11509078412254652, 0.1150842467546463]\n",
            "\tValidationLosses \t\t[2.3005114909013114, 2.300394515991211, 2.300619271993637, 2.3005396370093028, 2.300310746828715]\n",
            "CUDA is available!  Using GPU ...\n",
            "Loading Checkpoint from /content/gdrive/My Drive/Colab Notebooks/models/MINST/all/06_Mar_2020_07_57_24/08_Mar_2020_20_31_07/checkpoint_08_Mar_2020_20_42_52_8.pt...\n",
            "\tInputSize \t\t784\n",
            "\tOutputSize \t\t10\n",
            "\tHiddenLayers \t\t[1024, 1024, 1024, 32]\n",
            "\tLearningRate \t\t0.001\n",
            "\tDropRatio \t\t0.8\n",
            "\tTrainingLoss \t\t0.11508324266970157\n",
            "\tValidationLoss \t\t2.3002626490592957\n",
            "\tValidationAccuracy \t\ttensor(11.6417)\n",
            "\tElapsedTime \t\t0:11:45.423925\n",
            "\tDataset \t\tMINST\n",
            "\tLastEpoch \t\t8\n",
            "\tPreviousCheckPoint \t\t\n",
            "\tGPUState \t\tTrue\n",
            "\tOutputFolder \t\t08_Mar_2020_20_31_07\n",
            "\tCheckPointTimestamp \t\t08_Mar_2020_20_42_52\n",
            "\tOutputFilePrefix \t\tcheckpoint_\n",
            "\tOutputClasses \t\t['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
            "\tTransforms \t\tCompose(\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.5,), std=(0.5,))\n",
            ")\n",
            "\tTrainingLosses \t\t[0.11536910290022691, 0.11508705192804336, 0.11509626411398252, 0.11509078412254652, 0.1150842467546463, 0.11508417503535748, 0.11508327387770018, 0.11508219531178475, 0.11508324266970157]\n",
            "\tValidationLosses \t\t[2.3005114909013114, 2.300394515991211, 2.300619271993637, 2.3005396370093028, 2.300310746828715, 2.300451920032501, 2.3004555130004882, 2.30043932000796, 2.3002626490592957]\n",
            "CUDA is available!  Using GPU ...\n",
            " Not Found Matching Checkpoint with Last Experiement Parameters hidden_layers:4  hidden_layer_width: (1024, 1024, 512, 32)  Learning_rate:0.001 drop_ratio:0.5 \n",
            "Resume ValidationLoss  2.3002626490592957 Resume Learning Rate Array  [0.001]  Resume Drop Ratio List  [0.5, 0.3, 0.1]  Resume Hidden Nodes List with Size : 1989  Starting From : (1024, 1024, 512, 32)\n",
            "Global Counter: 0 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:00:47.391816  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 512, 32) }% %Learning_Rate: { 0.001 }% %drop_ratio: { 0.5 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_07_42_43 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): Linear(in_features=512, out_features=32, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_07_42_43 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.312701\n",
            "14_Mar_2020_07_43_19 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.172823\n",
            "14_Mar_2020_07_43_25 : Epoch: 1 \tTraining Loss: 0.115165 \tValidation Loss: 2.301788 \tAccuracy : 11.216660\n",
            "14_Mar_2020_07_43_25 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.018355\n",
            "14_Mar_2020_07_44_00 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.237939\n",
            "14_Mar_2020_07_44_06 : Epoch: 2 \tTraining Loss: 0.115076 \tValidation Loss: 2.301587 \tAccuracy : 11.216665\n",
            "14_Mar_2020_07_44_06 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:34.897466\n",
            "14_Mar_2020_07_44_41 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.258781\n",
            "14_Mar_2020_07_44_47 : Epoch: 3 \tTraining Loss: 0.115067 \tValidation Loss: 2.301583 \tAccuracy : 11.216648\n",
            "14_Mar_2020_07_44_47 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.300994\n",
            "14_Mar_2020_07_45_23 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.265587\n",
            "14_Mar_2020_07_45_29 : Epoch: 4 \tTraining Loss: 0.115064 \tValidation Loss: 2.301642 \tAccuracy : 11.216656\n",
            "14_Mar_2020_07_45_29 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:35.454117\n",
            "14_Mar_2020_07_46_04 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.298636\n",
            "14_Mar_2020_07_46_11 : Epoch: 5 \tTraining Loss: 0.115067 \tValidation Loss: 2.301669 \tAccuracy : 11.216650\n",
            "14_Mar_2020_07_46_11 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:35.286312\n",
            "14_Mar_2020_07_46_46 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.349923\n",
            "14_Mar_2020_07_46_52 : Epoch: 6 \tTraining Loss: 0.115065 \tValidation Loss: 2.301784 \tAccuracy : 11.216652\n",
            "14_Mar_2020_07_46_52 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.383532\n",
            "14_Mar_2020_07_47_28 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.330102\n",
            "14_Mar_2020_07_47_34 : Epoch: 7 \tTraining Loss: 0.115066 \tValidation Loss: 2.301865 \tAccuracy : 11.216657\n",
            "14_Mar_2020_07_47_34 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.206072\n",
            "14_Mar_2020_07_48_09 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.251736\n",
            "14_Mar_2020_07_48_15 : Epoch: 8 \tTraining Loss: 0.115067 \tValidation Loss: 2.301521 \tAccuracy : 11.216660\n",
            "14_Mar_2020_07_48_15 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.260826\n",
            "14_Mar_2020_07_48_51 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.277885\n",
            "14_Mar_2020_07_48_57 : Epoch: 9 \tTraining Loss: 0.115066 \tValidation Loss: 2.301470 \tAccuracy : 11.216663\n",
            "14_Mar_2020_07_48_57 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.059324\n",
            "14_Mar_2020_07_49_32 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.237851\n",
            "14_Mar_2020_07_49_38 : Epoch: 10 \tTraining Loss: 0.115068 \tValidation Loss: 2.301525 \tAccuracy : 11.216657\n",
            "Global Counter: 1 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:07:42.293484  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 512, 32) }% %Learning_Rate: { 0.001 }% %drop_ratio: { 0.3 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_07_49_38 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): Linear(in_features=512, out_features=32, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_07_49_38 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.151095\n",
            "14_Mar_2020_07_50_13 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.223358\n",
            "14_Mar_2020_07_50_20 : Epoch: 1 \tTraining Loss: 0.115126 \tValidation Loss: 2.301522 \tAccuracy : 11.216657\n",
            "14_Mar_2020_07_50_20 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.108227\n",
            "14_Mar_2020_07_50_55 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.281642\n",
            "14_Mar_2020_07_51_01 : Epoch: 2 \tTraining Loss: 0.115073 \tValidation Loss: 2.301737 \tAccuracy : 11.216661\n",
            "14_Mar_2020_07_51_01 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.173183\n",
            "14_Mar_2020_07_51_36 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.234821\n",
            "14_Mar_2020_07_51_42 : Epoch: 3 \tTraining Loss: 0.115072 \tValidation Loss: 2.301453 \tAccuracy : 11.216657\n",
            "14_Mar_2020_07_51_42 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.174436\n",
            "14_Mar_2020_07_52_18 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.222276\n",
            "14_Mar_2020_07_52_24 : Epoch: 4 \tTraining Loss: 0.115069 \tValidation Loss: 2.301625 \tAccuracy : 11.216656\n",
            "14_Mar_2020_07_52_24 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:34.926404\n",
            "14_Mar_2020_07_52_59 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.111338\n",
            "14_Mar_2020_07_53_05 : Epoch: 5 \tTraining Loss: 0.115068 \tValidation Loss: 2.301686 \tAccuracy : 11.216648\n",
            "14_Mar_2020_07_53_05 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:34.834416\n",
            "14_Mar_2020_07_53_40 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.168422\n",
            "14_Mar_2020_07_53_46 : Epoch: 6 \tTraining Loss: 0.115065 \tValidation Loss: 2.301666 \tAccuracy : 11.216665\n",
            "14_Mar_2020_07_53_46 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.083877\n",
            "14_Mar_2020_07_54_21 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.294938\n",
            "14_Mar_2020_07_54_27 : Epoch: 7 \tTraining Loss: 0.115067 \tValidation Loss: 2.301680 \tAccuracy : 11.216655\n",
            "14_Mar_2020_07_54_27 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:34.853960\n",
            "14_Mar_2020_07_55_02 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.197494\n",
            "14_Mar_2020_07_55_08 : Epoch: 8 \tTraining Loss: 0.115067 \tValidation Loss: 2.301453 \tAccuracy : 11.216655\n",
            "14_Mar_2020_07_55_08 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:34.779845\n",
            "14_Mar_2020_07_55_43 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.174352\n",
            "14_Mar_2020_07_55_49 : Epoch: 9 \tTraining Loss: 0.115065 \tValidation Loss: 2.301458 \tAccuracy : 11.216663\n",
            "14_Mar_2020_07_55_49 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.025356\n",
            "14_Mar_2020_07_56_24 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.202441\n",
            "14_Mar_2020_07_56_30 : Epoch: 10 \tTraining Loss: 0.115064 \tValidation Loss: 2.301823 \tAccuracy : 11.216661\n",
            "Global Counter: 2 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:14:34.537881  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 512, 32) }% %Learning_Rate: { 0.001 }% %drop_ratio: { 0.1 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_07_56_31 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): Linear(in_features=512, out_features=32, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=32, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_07_56_31 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.014615\n",
            "14_Mar_2020_07_57_06 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.168102\n",
            "14_Mar_2020_07_57_12 : Epoch: 1 \tTraining Loss: 0.115090 \tValidation Loss: 2.301764 \tAccuracy : 11.216652\n",
            "14_Mar_2020_07_57_12 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:34.909056\n",
            "14_Mar_2020_07_57_47 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.172955\n",
            "14_Mar_2020_07_57_53 : Epoch: 2 \tTraining Loss: 0.115068 \tValidation Loss: 2.301773 \tAccuracy : 11.216659\n",
            "14_Mar_2020_07_57_53 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.001637\n",
            "14_Mar_2020_07_58_28 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.159642\n",
            "14_Mar_2020_07_58_34 : Epoch: 3 \tTraining Loss: 0.115066 \tValidation Loss: 2.301539 \tAccuracy : 11.216657\n",
            "14_Mar_2020_07_58_34 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:34.823609\n",
            "14_Mar_2020_07_59_09 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.201402\n",
            "14_Mar_2020_07_59_15 : Epoch: 4 \tTraining Loss: 0.115066 \tValidation Loss: 2.301595 \tAccuracy : 11.216653\n",
            "14_Mar_2020_07_59_15 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:34.959523\n",
            "14_Mar_2020_07_59_50 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.155566\n",
            "14_Mar_2020_07_59_56 : Epoch: 5 \tTraining Loss: 0.115068 \tValidation Loss: 2.301491 \tAccuracy : 11.216659\n",
            "14_Mar_2020_07_59_56 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:34.787001\n",
            "14_Mar_2020_08_00_31 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.134157\n",
            "14_Mar_2020_08_00_37 : Epoch: 6 \tTraining Loss: 0.115067 \tValidation Loss: 2.301413 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_00_37 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.021103\n",
            "14_Mar_2020_08_01_12 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.136676\n",
            "14_Mar_2020_08_01_18 : Epoch: 7 \tTraining Loss: 0.115065 \tValidation Loss: 2.301450 \tAccuracy : 11.216659\n",
            "14_Mar_2020_08_01_18 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:34.713061\n",
            "14_Mar_2020_08_01_53 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.198504\n",
            "14_Mar_2020_08_01_59 : Epoch: 8 \tTraining Loss: 0.115066 \tValidation Loss: 2.301582 \tAccuracy : 11.216653\n",
            "14_Mar_2020_08_01_59 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:34.763103\n",
            "14_Mar_2020_08_02_34 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.151018\n",
            "14_Mar_2020_08_02_40 : Epoch: 9 \tTraining Loss: 0.115066 \tValidation Loss: 2.301659 \tAccuracy : 11.216656\n",
            "14_Mar_2020_08_02_40 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:34.726061\n",
            "14_Mar_2020_08_03_15 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.119947\n",
            "14_Mar_2020_08_03_21 : Epoch: 10 \tTraining Loss: 0.115065 \tValidation Loss: 2.301468 \tAccuracy : 11.216657\n",
            "Global Counter: 3 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:21:24.878051  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.1 }% %drop_ratio: { 0.8 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_03_21 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.8, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_03_21 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.504453\n",
            "14_Mar_2020_08_03_56 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.179580\n",
            "14_Mar_2020_08_04_03 : Epoch: 1 \tTraining Loss: 13.123900 \tValidation Loss: 2.316656 \tAccuracy : 9.549986\n",
            "14_Mar_2020_08_04_03 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.681901\n",
            "14_Mar_2020_08_04_38 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.324413\n",
            "14_Mar_2020_08_04_45 : Epoch: 2 \tTraining Loss: 0.115845 \tValidation Loss: 2.308337 \tAccuracy : 11.216653\n",
            "14_Mar_2020_08_04_45 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.719080\n",
            "14_Mar_2020_08_05_20 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.252531\n",
            "14_Mar_2020_08_05_27 : Epoch: 3 \tTraining Loss: 0.115872 \tValidation Loss: 2.319415 \tAccuracy : 10.516654\n",
            "14_Mar_2020_08_05_27 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.648838\n",
            "14_Mar_2020_08_06_02 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.220691\n",
            "14_Mar_2020_08_06_08 : Epoch: 4 \tTraining Loss: 0.115872 \tValidation Loss: 2.321288 \tAccuracy : 9.633315\n",
            "14_Mar_2020_08_06_08 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:35.685077\n",
            "14_Mar_2020_08_06_44 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.243752\n",
            "14_Mar_2020_08_06_50 : Epoch: 5 \tTraining Loss: 0.115906 \tValidation Loss: 2.322531 \tAccuracy : 9.549982\n",
            "14_Mar_2020_08_06_50 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:35.633651\n",
            "14_Mar_2020_08_07_26 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.182331\n",
            "14_Mar_2020_08_07_32 : Epoch: 6 \tTraining Loss: 0.115862 \tValidation Loss: 2.324874 \tAccuracy : 9.441648\n",
            "14_Mar_2020_08_07_32 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.980824\n",
            "14_Mar_2020_08_08_08 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.310164\n",
            "14_Mar_2020_08_08_14 : Epoch: 7 \tTraining Loss: 0.115895 \tValidation Loss: 2.310029 \tAccuracy : 9.549980\n",
            "14_Mar_2020_08_08_14 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.879716\n",
            "14_Mar_2020_08_08_50 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.279798\n",
            "14_Mar_2020_08_08_57 : Epoch: 8 \tTraining Loss: 0.115879 \tValidation Loss: 2.316795 \tAccuracy : 10.258316\n",
            "14_Mar_2020_08_08_57 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.656040\n",
            "14_Mar_2020_08_09_32 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.227500\n",
            "14_Mar_2020_08_09_38 : Epoch: 9 \tTraining Loss: 0.115850 \tValidation Loss: 2.304094 \tAccuracy : 10.258323\n",
            "14_Mar_2020_08_09_38 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.698538\n",
            "14_Mar_2020_08_10_14 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.286995\n",
            "14_Mar_2020_08_10_20 : Epoch: 10 \tTraining Loss: 0.115877 \tValidation Loss: 2.330911 \tAccuracy : 9.441648\n",
            "Global Counter: 4 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:28:24.499448  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.1 }% %drop_ratio: { 0.5 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_10_20 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_10_20 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.524730\n",
            "14_Mar_2020_08_10_56 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.218085\n",
            "14_Mar_2020_08_11_02 : Epoch: 1 \tTraining Loss: 53.907966 \tValidation Loss: 2.321167 \tAccuracy : 9.441648\n",
            "14_Mar_2020_08_11_02 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.858265\n",
            "14_Mar_2020_08_11_38 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.262976\n",
            "14_Mar_2020_08_11_44 : Epoch: 2 \tTraining Loss: 0.115871 \tValidation Loss: 2.312264 \tAccuracy : 9.933316\n",
            "14_Mar_2020_08_11_44 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.911932\n",
            "14_Mar_2020_08_12_20 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.243975\n",
            "14_Mar_2020_08_12_26 : Epoch: 3 \tTraining Loss: 0.115806 \tValidation Loss: 2.306721 \tAccuracy : 9.933319\n",
            "14_Mar_2020_08_12_26 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.935948\n",
            "14_Mar_2020_08_13_02 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.294194\n",
            "14_Mar_2020_08_13_09 : Epoch: 4 \tTraining Loss: 0.115825 \tValidation Loss: 2.309955 \tAccuracy : 9.549983\n",
            "14_Mar_2020_08_13_09 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:35.963454\n",
            "14_Mar_2020_08_13_45 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.309613\n",
            "14_Mar_2020_08_13_51 : Epoch: 5 \tTraining Loss: 0.115862 \tValidation Loss: 2.313886 \tAccuracy : 10.516652\n",
            "14_Mar_2020_08_13_51 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:35.853198\n",
            "14_Mar_2020_08_14_27 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.271496\n",
            "14_Mar_2020_08_14_33 : Epoch: 6 \tTraining Loss: 0.115876 \tValidation Loss: 2.307612 \tAccuracy : 9.933325\n",
            "14_Mar_2020_08_14_33 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.114526\n",
            "14_Mar_2020_08_15_09 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.256131\n",
            "14_Mar_2020_08_15_15 : Epoch: 7 \tTraining Loss: 0.115867 \tValidation Loss: 2.314762 \tAccuracy : 9.933322\n",
            "14_Mar_2020_08_15_15 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.880965\n",
            "14_Mar_2020_08_15_51 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.216569\n",
            "14_Mar_2020_08_15_58 : Epoch: 8 \tTraining Loss: 0.115872 \tValidation Loss: 2.315071 \tAccuracy : 9.774990\n",
            "14_Mar_2020_08_15_58 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:36.096900\n",
            "14_Mar_2020_08_16_34 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.305738\n",
            "14_Mar_2020_08_16_40 : Epoch: 9 \tTraining Loss: 0.115836 \tValidation Loss: 2.318288 \tAccuracy : 9.633314\n",
            "14_Mar_2020_08_16_40 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:36.051100\n",
            "14_Mar_2020_08_17_16 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.275495\n",
            "14_Mar_2020_08_17_22 : Epoch: 10 \tTraining Loss: 0.115857 \tValidation Loss: 2.318703 \tAccuracy : 9.633318\n",
            "Global Counter: 5 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:35:26.370797  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.1 }% %drop_ratio: { 0.3 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_17_22 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_17_22 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:36.057881\n",
            "14_Mar_2020_08_17_58 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.358476\n",
            "14_Mar_2020_08_18_05 : Epoch: 1 \tTraining Loss: 193.566913 \tValidation Loss: 2.310430 \tAccuracy : 11.216660\n",
            "14_Mar_2020_08_18_05 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:36.445643\n",
            "14_Mar_2020_08_18_41 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.405195\n",
            "14_Mar_2020_08_18_48 : Epoch: 2 \tTraining Loss: 0.115838 \tValidation Loss: 2.316879 \tAccuracy : 10.266645\n",
            "14_Mar_2020_08_18_48 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:36.123183\n",
            "14_Mar_2020_08_19_24 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.367042\n",
            "14_Mar_2020_08_19_30 : Epoch: 3 \tTraining Loss: 0.115886 \tValidation Loss: 2.318472 \tAccuracy : 9.549980\n",
            "14_Mar_2020_08_19_30 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:36.072026\n",
            "14_Mar_2020_08_20_06 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.358503\n",
            "14_Mar_2020_08_20_13 : Epoch: 4 \tTraining Loss: 0.115907 \tValidation Loss: 2.319080 \tAccuracy : 9.408320\n",
            "14_Mar_2020_08_20_13 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:36.255597\n",
            "14_Mar_2020_08_20_49 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.394779\n",
            "14_Mar_2020_08_20_55 : Epoch: 5 \tTraining Loss: 0.115890 \tValidation Loss: 2.322187 \tAccuracy : 9.441644\n",
            "14_Mar_2020_08_20_55 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:36.229413\n",
            "14_Mar_2020_08_21_31 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.393288\n",
            "14_Mar_2020_08_21_38 : Epoch: 6 \tTraining Loss: 0.115900 \tValidation Loss: 2.312835 \tAccuracy : 10.516660\n",
            "14_Mar_2020_08_21_38 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.134099\n",
            "14_Mar_2020_08_22_14 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.345901\n",
            "14_Mar_2020_08_22_20 : Epoch: 7 \tTraining Loss: 0.115894 \tValidation Loss: 2.311535 \tAccuracy : 10.516649\n",
            "14_Mar_2020_08_22_20 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:36.007103\n",
            "14_Mar_2020_08_22_56 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.273527\n",
            "14_Mar_2020_08_23_03 : Epoch: 8 \tTraining Loss: 0.115871 \tValidation Loss: 2.314050 \tAccuracy : 9.774981\n",
            "14_Mar_2020_08_23_03 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.999193\n",
            "14_Mar_2020_08_23_39 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.284242\n",
            "14_Mar_2020_08_23_45 : Epoch: 9 \tTraining Loss: 0.115887 \tValidation Loss: 2.312617 \tAccuracy : 10.516650\n",
            "14_Mar_2020_08_23_45 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.929193\n",
            "14_Mar_2020_08_24_21 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.320029\n",
            "14_Mar_2020_08_24_27 : Epoch: 10 \tTraining Loss: 0.115874 \tValidation Loss: 2.316385 \tAccuracy : 11.216663\n",
            "Global Counter: 6 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:42:31.149583  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.1 }% %drop_ratio: { 0.1 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_24_27 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_24_27 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:36.039252\n",
            "14_Mar_2020_08_25_03 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.317440\n",
            "14_Mar_2020_08_25_09 : Epoch: 1 \tTraining Loss: 158.796449 \tValidation Loss: 2.303844 \tAccuracy : 10.266652\n",
            "14_Mar_2020_08_25_09 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:36.142372\n",
            "14_Mar_2020_08_25_46 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.334508\n",
            "14_Mar_2020_08_25_52 : Epoch: 2 \tTraining Loss: 0.115949 \tValidation Loss: 2.313015 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_25_52 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.948181\n",
            "14_Mar_2020_08_26_28 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.321432\n",
            "14_Mar_2020_08_26_34 : Epoch: 3 \tTraining Loss: 0.115845 \tValidation Loss: 2.314137 \tAccuracy : 10.266650\n",
            "14_Mar_2020_08_26_34 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.998045\n",
            "14_Mar_2020_08_27_10 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.298900\n",
            "14_Mar_2020_08_27_17 : Epoch: 4 \tTraining Loss: 0.115949 \tValidation Loss: 2.317074 \tAccuracy : 9.633322\n",
            "14_Mar_2020_08_27_17 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:36.019371\n",
            "14_Mar_2020_08_27_53 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.290417\n",
            "14_Mar_2020_08_27_59 : Epoch: 5 \tTraining Loss: 0.115853 \tValidation Loss: 2.331817 \tAccuracy : 9.933319\n",
            "14_Mar_2020_08_27_59 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:35.946403\n",
            "14_Mar_2020_08_28_35 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.268088\n",
            "14_Mar_2020_08_28_41 : Epoch: 6 \tTraining Loss: 0.115831 \tValidation Loss: 2.311510 \tAccuracy : 10.516649\n",
            "14_Mar_2020_08_28_41 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.037828\n",
            "14_Mar_2020_08_29_17 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.298599\n",
            "14_Mar_2020_08_29_23 : Epoch: 7 \tTraining Loss: 0.115800 \tValidation Loss: 2.313012 \tAccuracy : 9.633314\n",
            "14_Mar_2020_08_29_23 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.928990\n",
            "14_Mar_2020_08_29_59 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.306185\n",
            "14_Mar_2020_08_30_06 : Epoch: 8 \tTraining Loss: 0.115844 \tValidation Loss: 2.331201 \tAccuracy : 9.933316\n",
            "14_Mar_2020_08_30_06 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:36.196445\n",
            "14_Mar_2020_08_30_42 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.304187\n",
            "14_Mar_2020_08_30_48 : Epoch: 9 \tTraining Loss: 0.115873 \tValidation Loss: 2.327184 \tAccuracy : 10.266649\n",
            "14_Mar_2020_08_30_48 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:36.055662\n",
            "14_Mar_2020_08_31_24 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.282840\n",
            "14_Mar_2020_08_31_30 : Epoch: 10 \tTraining Loss: 0.115842 \tValidation Loss: 2.311085 \tAccuracy : 9.633316\n",
            "Global Counter: 7 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:49:34.510958  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.01 }% %drop_ratio: { 0.8 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_31_30 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.8, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_31_30 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.954827\n",
            "14_Mar_2020_08_32_06 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.239062\n",
            "14_Mar_2020_08_32_13 : Epoch: 1 \tTraining Loss: 0.119187 \tValidation Loss: 2.303846 \tAccuracy : 9.549985\n",
            "14_Mar_2020_08_32_13 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.910399\n",
            "14_Mar_2020_08_32_49 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.281893\n",
            "14_Mar_2020_08_32_55 : Epoch: 2 \tTraining Loss: 0.115144 \tValidation Loss: 2.302089 \tAccuracy : 11.216665\n",
            "14_Mar_2020_08_32_55 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:36.028346\n",
            "14_Mar_2020_08_33_31 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.242734\n",
            "14_Mar_2020_08_33_37 : Epoch: 3 \tTraining Loss: 0.115150 \tValidation Loss: 2.302240 \tAccuracy : 11.216659\n",
            "14_Mar_2020_08_33_37 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.975635\n",
            "14_Mar_2020_08_34_13 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.272967\n",
            "14_Mar_2020_08_34_19 : Epoch: 4 \tTraining Loss: 0.115145 \tValidation Loss: 2.301851 \tAccuracy : 11.216650\n",
            "14_Mar_2020_08_34_19 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:36.005132\n",
            "14_Mar_2020_08_34_55 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.258479\n",
            "14_Mar_2020_08_35_02 : Epoch: 5 \tTraining Loss: 0.115133 \tValidation Loss: 2.304199 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_35_02 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:36.143351\n",
            "14_Mar_2020_08_35_38 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.338797\n",
            "14_Mar_2020_08_35_44 : Epoch: 6 \tTraining Loss: 0.115140 \tValidation Loss: 2.302284 \tAccuracy : 11.216653\n",
            "14_Mar_2020_08_35_44 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.124211\n",
            "14_Mar_2020_08_36_20 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.321024\n",
            "14_Mar_2020_08_36_27 : Epoch: 7 \tTraining Loss: 0.115132 \tValidation Loss: 2.303550 \tAccuracy : 11.216650\n",
            "14_Mar_2020_08_36_27 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.855824\n",
            "14_Mar_2020_08_37_02 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.301099\n",
            "14_Mar_2020_08_37_09 : Epoch: 8 \tTraining Loss: 0.115139 \tValidation Loss: 2.303281 \tAccuracy : 11.216655\n",
            "14_Mar_2020_08_37_09 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.918955\n",
            "14_Mar_2020_08_37_45 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.332702\n",
            "14_Mar_2020_08_37_51 : Epoch: 9 \tTraining Loss: 0.115141 \tValidation Loss: 2.302574 \tAccuracy : 10.258326\n",
            "14_Mar_2020_08_37_51 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.910917\n",
            "14_Mar_2020_08_38_27 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.256628\n",
            "14_Mar_2020_08_38_33 : Epoch: 10 \tTraining Loss: 0.115151 \tValidation Loss: 2.303545 \tAccuracy : 10.258317\n",
            "Global Counter: 8 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 0:56:37.208200  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.01 }% %drop_ratio: { 0.5 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_38_33 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_38_33 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.963336\n",
            "14_Mar_2020_08_39_09 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.332606\n",
            "14_Mar_2020_08_39_15 : Epoch: 1 \tTraining Loss: 0.118530 \tValidation Loss: 2.303399 \tAccuracy : 9.549983\n",
            "14_Mar_2020_08_39_15 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:36.131584\n",
            "14_Mar_2020_08_39_52 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.282733\n",
            "14_Mar_2020_08_39_58 : Epoch: 2 \tTraining Loss: 0.115134 \tValidation Loss: 2.302600 \tAccuracy : 11.216660\n",
            "14_Mar_2020_08_39_58 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:36.002324\n",
            "14_Mar_2020_08_40_34 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.278274\n",
            "14_Mar_2020_08_40_40 : Epoch: 3 \tTraining Loss: 0.115140 \tValidation Loss: 2.302932 \tAccuracy : 11.216655\n",
            "14_Mar_2020_08_40_40 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:36.193236\n",
            "14_Mar_2020_08_41_16 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.328634\n",
            "14_Mar_2020_08_41_23 : Epoch: 4 \tTraining Loss: 0.115135 \tValidation Loss: 2.302300 \tAccuracy : 11.216650\n",
            "14_Mar_2020_08_41_23 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:36.131491\n",
            "14_Mar_2020_08_41_59 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.263798\n",
            "14_Mar_2020_08_42_05 : Epoch: 5 \tTraining Loss: 0.115135 \tValidation Loss: 2.303014 \tAccuracy : 11.216659\n",
            "14_Mar_2020_08_42_05 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:36.071918\n",
            "14_Mar_2020_08_42_41 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.328468\n",
            "14_Mar_2020_08_42_47 : Epoch: 6 \tTraining Loss: 0.115151 \tValidation Loss: 2.302048 \tAccuracy : 11.216655\n",
            "14_Mar_2020_08_42_47 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.849914\n",
            "14_Mar_2020_08_43_23 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.277326\n",
            "14_Mar_2020_08_43_30 : Epoch: 7 \tTraining Loss: 0.115138 \tValidation Loss: 2.303689 \tAccuracy : 10.516651\n",
            "14_Mar_2020_08_43_30 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:36.015296\n",
            "14_Mar_2020_08_44_06 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.301335\n",
            "14_Mar_2020_08_44_12 : Epoch: 8 \tTraining Loss: 0.115145 \tValidation Loss: 2.302908 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_44_12 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:36.172301\n",
            "14_Mar_2020_08_44_48 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.351676\n",
            "14_Mar_2020_08_44_54 : Epoch: 9 \tTraining Loss: 0.115151 \tValidation Loss: 2.302052 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_44_54 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:36.207550\n",
            "14_Mar_2020_08_45_31 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.359465\n",
            "14_Mar_2020_08_45_37 : Epoch: 10 \tTraining Loss: 0.115147 \tValidation Loss: 2.303396 \tAccuracy : 11.216659\n",
            "Global Counter: 9 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 1:03:41.077586  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.01 }% %drop_ratio: { 0.3 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_45_37 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_45_37 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:36.296283\n",
            "14_Mar_2020_08_46_13 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.265222\n",
            "14_Mar_2020_08_46_20 : Epoch: 1 \tTraining Loss: 0.120974 \tValidation Loss: 2.304155 \tAccuracy : 11.216652\n",
            "14_Mar_2020_08_46_20 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:36.225352\n",
            "14_Mar_2020_08_46_56 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.324296\n",
            "14_Mar_2020_08_47_02 : Epoch: 2 \tTraining Loss: 0.115142 \tValidation Loss: 2.303445 \tAccuracy : 11.216653\n",
            "14_Mar_2020_08_47_02 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:36.400956\n",
            "14_Mar_2020_08_47_39 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.339960\n",
            "14_Mar_2020_08_47_45 : Epoch: 3 \tTraining Loss: 0.115148 \tValidation Loss: 2.302731 \tAccuracy : 11.216660\n",
            "14_Mar_2020_08_47_45 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:36.217856\n",
            "14_Mar_2020_08_48_21 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.356195\n",
            "14_Mar_2020_08_48_27 : Epoch: 4 \tTraining Loss: 0.115143 \tValidation Loss: 2.302123 \tAccuracy : 11.216652\n",
            "14_Mar_2020_08_48_27 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:36.169268\n",
            "14_Mar_2020_08_49_04 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.390839\n",
            "14_Mar_2020_08_49_10 : Epoch: 5 \tTraining Loss: 0.115135 \tValidation Loss: 2.303815 \tAccuracy : 10.258319\n",
            "14_Mar_2020_08_49_10 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:36.206477\n",
            "14_Mar_2020_08_49_46 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.318305\n",
            "14_Mar_2020_08_49_53 : Epoch: 6 \tTraining Loss: 0.115138 \tValidation Loss: 2.303119 \tAccuracy : 11.216655\n",
            "14_Mar_2020_08_49_53 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.106707\n",
            "14_Mar_2020_08_50_29 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.345775\n",
            "14_Mar_2020_08_50_35 : Epoch: 7 \tTraining Loss: 0.115135 \tValidation Loss: 2.304899 \tAccuracy : 9.549980\n",
            "14_Mar_2020_08_50_35 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:36.191422\n",
            "14_Mar_2020_08_51_11 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.347382\n",
            "14_Mar_2020_08_51_18 : Epoch: 8 \tTraining Loss: 0.115147 \tValidation Loss: 2.305211 \tAccuracy : 9.549982\n",
            "14_Mar_2020_08_51_18 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:36.155833\n",
            "14_Mar_2020_08_51_54 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.303853\n",
            "14_Mar_2020_08_52_00 : Epoch: 9 \tTraining Loss: 0.115148 \tValidation Loss: 2.302601 \tAccuracy : 10.258314\n",
            "14_Mar_2020_08_52_00 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:36.080631\n",
            "14_Mar_2020_08_52_36 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.333220\n",
            "14_Mar_2020_08_52_42 : Epoch: 10 \tTraining Loss: 0.115140 \tValidation Loss: 2.302294 \tAccuracy : 11.216659\n",
            "Global Counter: 10 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 1:10:46.479028  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.01 }% %drop_ratio: { 0.1 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_52_42 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_52_42 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:36.013538\n",
            "14_Mar_2020_08_53_18 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.323519\n",
            "14_Mar_2020_08_53_25 : Epoch: 1 \tTraining Loss: 0.123543 \tValidation Loss: 2.302428 \tAccuracy : 11.216655\n",
            "14_Mar_2020_08_53_25 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.734757\n",
            "14_Mar_2020_08_54_01 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.317473\n",
            "14_Mar_2020_08_54_07 : Epoch: 2 \tTraining Loss: 0.115151 \tValidation Loss: 2.302935 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_54_07 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:36.006551\n",
            "14_Mar_2020_08_54_43 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.281370\n",
            "14_Mar_2020_08_54_49 : Epoch: 3 \tTraining Loss: 0.115140 \tValidation Loss: 2.304060 \tAccuracy : 10.516655\n",
            "14_Mar_2020_08_54_49 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:36.018149\n",
            "14_Mar_2020_08_55_25 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.290490\n",
            "14_Mar_2020_08_55_31 : Epoch: 4 \tTraining Loss: 0.115141 \tValidation Loss: 2.302901 \tAccuracy : 11.216657\n",
            "14_Mar_2020_08_55_31 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:35.913386\n",
            "14_Mar_2020_08_56_07 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.291949\n",
            "14_Mar_2020_08_56_14 : Epoch: 5 \tTraining Loss: 0.115133 \tValidation Loss: 2.303058 \tAccuracy : 11.216656\n",
            "14_Mar_2020_08_56_14 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:36.127580\n",
            "14_Mar_2020_08_56_50 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.359639\n",
            "14_Mar_2020_08_56_56 : Epoch: 6 \tTraining Loss: 0.115143 \tValidation Loss: 2.302704 \tAccuracy : 10.258317\n",
            "14_Mar_2020_08_56_56 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.330337\n",
            "14_Mar_2020_08_57_32 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.343188\n",
            "14_Mar_2020_08_57_39 : Epoch: 7 \tTraining Loss: 0.115140 \tValidation Loss: 2.303925 \tAccuracy : 9.774981\n",
            "14_Mar_2020_08_57_39 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.882162\n",
            "14_Mar_2020_08_58_15 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.264608\n",
            "14_Mar_2020_08_58_21 : Epoch: 8 \tTraining Loss: 0.115146 \tValidation Loss: 2.302861 \tAccuracy : 11.216650\n",
            "14_Mar_2020_08_58_21 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.863767\n",
            "14_Mar_2020_08_58_57 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.396096\n",
            "14_Mar_2020_08_59_03 : Epoch: 9 \tTraining Loss: 0.115151 \tValidation Loss: 2.301884 \tAccuracy : 11.216659\n",
            "14_Mar_2020_08_59_03 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:36.365539\n",
            "14_Mar_2020_08_59_40 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.244959\n",
            "14_Mar_2020_08_59_46 : Epoch: 10 \tTraining Loss: 0.115142 \tValidation Loss: 2.302696 \tAccuracy : 10.516652\n",
            "Global Counter: 11 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 1:17:49.874156  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.0001 }% %drop_ratio: { 0.8 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_08_59_46 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.8, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_08_59_46 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.857183\n",
            "14_Mar_2020_09_00_22 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.265394\n",
            "14_Mar_2020_09_00_28 : Epoch: 1 \tTraining Loss: 0.115354 \tValidation Loss: 2.301342 \tAccuracy : 11.216650\n",
            "14_Mar_2020_09_00_28 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:36.026959\n",
            "14_Mar_2020_09_01_04 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.270210\n",
            "14_Mar_2020_09_01_10 : Epoch: 2 \tTraining Loss: 0.115088 \tValidation Loss: 2.301632 \tAccuracy : 11.216653\n",
            "14_Mar_2020_09_01_10 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.836412\n",
            "14_Mar_2020_09_01_46 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.233001\n",
            "14_Mar_2020_09_01_52 : Epoch: 3 \tTraining Loss: 0.115099 \tValidation Loss: 2.301479 \tAccuracy : 11.216660\n",
            "14_Mar_2020_09_01_52 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.848441\n",
            "14_Mar_2020_09_02_28 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.293918\n",
            "14_Mar_2020_09_02_34 : Epoch: 4 \tTraining Loss: 0.115093 \tValidation Loss: 2.301545 \tAccuracy : 11.216655\n",
            "14_Mar_2020_09_02_34 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:35.948531\n",
            "14_Mar_2020_09_03_10 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.267786\n",
            "14_Mar_2020_09_03_17 : Epoch: 5 \tTraining Loss: 0.115107 \tValidation Loss: 2.301578 \tAccuracy : 11.216656\n",
            "14_Mar_2020_09_03_17 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:35.768002\n",
            "14_Mar_2020_09_03_52 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.279992\n",
            "14_Mar_2020_09_03_59 : Epoch: 6 \tTraining Loss: 0.115096 \tValidation Loss: 2.301965 \tAccuracy : 11.216652\n",
            "14_Mar_2020_09_03_59 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.845356\n",
            "14_Mar_2020_09_04_35 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.288157\n",
            "14_Mar_2020_09_04_41 : Epoch: 7 \tTraining Loss: 0.115094 \tValidation Loss: 2.301498 \tAccuracy : 11.216661\n",
            "14_Mar_2020_09_04_41 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:35.893373\n",
            "14_Mar_2020_09_05_17 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.248739\n",
            "14_Mar_2020_09_05_23 : Epoch: 8 \tTraining Loss: 0.115082 \tValidation Loss: 2.301972 \tAccuracy : 11.216653\n",
            "14_Mar_2020_09_05_23 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.866852\n",
            "14_Mar_2020_09_05_59 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.248398\n",
            "14_Mar_2020_09_06_05 : Epoch: 9 \tTraining Loss: 0.115104 \tValidation Loss: 2.301653 \tAccuracy : 11.216656\n",
            "14_Mar_2020_09_06_05 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.918402\n",
            "14_Mar_2020_09_06_41 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.352609\n",
            "14_Mar_2020_09_06_47 : Epoch: 10 \tTraining Loss: 0.115101 \tValidation Loss: 2.301697 \tAccuracy : 11.216659\n",
            "Global Counter: 12 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 1:24:51.457616  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.0001 }% %drop_ratio: { 0.5 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_09_06_47 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_09_06_47 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.913967\n",
            "14_Mar_2020_09_07_23 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.296340\n",
            "14_Mar_2020_09_07_30 : Epoch: 1 \tTraining Loss: 0.115134 \tValidation Loss: 2.301502 \tAccuracy : 11.216650\n",
            "14_Mar_2020_09_07_30 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.770739\n",
            "14_Mar_2020_09_08_05 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.282624\n",
            "14_Mar_2020_09_08_12 : Epoch: 2 \tTraining Loss: 0.115079 \tValidation Loss: 2.301552 \tAccuracy : 11.216656\n",
            "14_Mar_2020_09_08_12 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.954622\n",
            "14_Mar_2020_09_08_48 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.299465\n",
            "14_Mar_2020_09_08_54 : Epoch: 3 \tTraining Loss: 0.115088 \tValidation Loss: 2.302221 \tAccuracy : 11.216652\n",
            "14_Mar_2020_09_08_54 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.832416\n",
            "14_Mar_2020_09_09_30 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.286303\n",
            "14_Mar_2020_09_09_36 : Epoch: 4 \tTraining Loss: 0.115085 \tValidation Loss: 2.301372 \tAccuracy : 11.216656\n",
            "14_Mar_2020_09_09_36 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:35.832760\n",
            "14_Mar_2020_09_10_12 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.284574\n",
            "14_Mar_2020_09_10_18 : Epoch: 5 \tTraining Loss: 0.115081 \tValidation Loss: 2.301708 \tAccuracy : 11.216660\n",
            "14_Mar_2020_09_10_18 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:35.950615\n",
            "14_Mar_2020_09_10_54 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.339240\n",
            "14_Mar_2020_09_11_00 : Epoch: 6 \tTraining Loss: 0.115084 \tValidation Loss: 2.302009 \tAccuracy : 11.216655\n",
            "14_Mar_2020_09_11_00 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:35.913040\n",
            "14_Mar_2020_09_11_36 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.276510\n",
            "14_Mar_2020_09_11_43 : Epoch: 7 \tTraining Loss: 0.115084 \tValidation Loss: 2.301469 \tAccuracy : 11.216659\n",
            "14_Mar_2020_09_11_43 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:36.062539\n",
            "14_Mar_2020_09_12_19 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.304477\n",
            "14_Mar_2020_09_12_25 : Epoch: 8 \tTraining Loss: 0.115076 \tValidation Loss: 2.301601 \tAccuracy : 11.216661\n",
            "14_Mar_2020_09_12_25 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:35.971299\n",
            "14_Mar_2020_09_13_01 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.281492\n",
            "14_Mar_2020_09_13_07 : Epoch: 9 \tTraining Loss: 0.115076 \tValidation Loss: 2.301986 \tAccuracy : 11.216656\n",
            "14_Mar_2020_09_13_07 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:35.988291\n",
            "14_Mar_2020_09_13_43 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.302503\n",
            "14_Mar_2020_09_13_50 : Epoch: 10 \tTraining Loss: 0.115077 \tValidation Loss: 2.301674 \tAccuracy : 11.216653\n",
            "Global Counter: 13 EXP-ID : 14_Mar_2020_07_41_56  DurationSinceStart : 1:31:53.626934  Starting New Experiment with %hidden_layers: { 4 }% %hidden_layer_width: { (1024, 1024, 256, 2048) }% %Learning_Rate: { 0.0001 }% %drop_ratio: { 0.3 }%\n",
            "CUDA is available!  Using GPU ...\n",
            "14_Mar_2020_09_13_50 : Starting Training using Model Parameters \n",
            " Network(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "    (3): Linear(in_features=256, out_features=2048, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3002626490592957\n",
            "14_Mar_2020_09_13_50 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:35.941763\n",
            "14_Mar_2020_09_14_26 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.239272\n",
            "14_Mar_2020_09_14_32 : Epoch: 1 \tTraining Loss: 0.115104 \tValidation Loss: 2.301607 \tAccuracy : 11.216655\n",
            "14_Mar_2020_09_14_32 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:35.971892\n",
            "14_Mar_2020_09_15_08 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.276917\n",
            "14_Mar_2020_09_15_14 : Epoch: 2 \tTraining Loss: 0.115086 \tValidation Loss: 2.301667 \tAccuracy : 11.216655\n",
            "14_Mar_2020_09_15_14 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:35.988192\n",
            "14_Mar_2020_09_15_50 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.312727\n",
            "14_Mar_2020_09_15_56 : Epoch: 3 \tTraining Loss: 0.115082 \tValidation Loss: 2.301656 \tAccuracy : 11.216666\n",
            "14_Mar_2020_09_15_56 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:35.854287\n",
            "14_Mar_2020_09_16_32 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.325908\n",
            "14_Mar_2020_09_16_39 : Epoch: 4 \tTraining Loss: 0.115080 \tValidation Loss: 2.301772 \tAccuracy : 11.216657\n",
            "14_Mar_2020_09_16_39 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:36.138510\n",
            "14_Mar_2020_09_17_15 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.407384\n",
            "14_Mar_2020_09_17_21 : Epoch: 5 \tTraining Loss: 0.115083 \tValidation Loss: 2.301885 \tAccuracy : 11.216652\n",
            "14_Mar_2020_09_17_21 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:36.247148\n",
            "14_Mar_2020_09_17_57 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.401021\n",
            "14_Mar_2020_09_18_04 : Epoch: 6 \tTraining Loss: 0.115080 \tValidation Loss: 2.301515 \tAccuracy : 11.216657\n",
            "14_Mar_2020_09_18_04 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:36.296065\n",
            "14_Mar_2020_09_18_40 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.291547\n",
            "14_Mar_2020_09_18_46 : Epoch: 7 \tTraining Loss: 0.115074 \tValidation Loss: 2.301551 \tAccuracy : 11.216656\n",
            "14_Mar_2020_09_18_46 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:36.059669\n",
            "14_Mar_2020_09_19_22 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.376624\n",
            "14_Mar_2020_09_19_29 : Epoch: 8 \tTraining Loss: 0.115075 \tValidation Loss: 2.301525 \tAccuracy : 11.216653\n",
            "14_Mar_2020_09_19_29 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:36.180880\n",
            "14_Mar_2020_09_20_05 :  Starting Validation....\n",
            "Finished Validation In  0:00:06.327076\n",
            "14_Mar_2020_09_20_11 : Epoch: 9 \tTraining Loss: 0.115071 \tValidation Loss: 2.301864 \tAccuracy : 11.216660\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}