{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZRW6Z2nFFfZ6n4TAdvtXT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceman011/mydeeplearning/blob/master/ez_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXy2soVzPQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "import datetime\n",
        "import os\n",
        "import pdb\n",
        "import sys\n",
        "import torchvision.models as pretrained_models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "#%config InlineBackend.figure_format = 'retina'\n",
        "  \n",
        "from google.colab import drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "from torch import optim\n",
        "import itertools\n",
        "#!/usr/bin/env python3\n",
        "import mmap\n",
        "import re\n",
        "from itertools import dropwhile, product\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers,output_classes,transform,dataset, drop_p=0.5,lr =0.001, train_on_gpu=False):\n",
        "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
        "        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Using CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Using GPU ...')\n",
        "            \n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.drop_ratio = drop_p\n",
        "        self.output_classes = output_classes\n",
        "        self.transform=transform\n",
        "        self.dataset=dataset\n",
        "\n",
        "        # Input to a hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "        \n",
        "        # Add a variable number of more hidden layers\n",
        "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "        \n",
        "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "        \n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "class PreTrainenModels(nn.Module):\n",
        "      \n",
        "      def __init__(self, input_size, output_size, hidden_layers,output_classes,transform,dataset, drop_p=0.5,lr =0.001, train_on_gpu=False,network_type ='normal'):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if ( network_type == 'resnet18'):\n",
        "          #assert input_size == 1024 ,  'input features of pre-trained networks must bet 1024'\n",
        "          model = pretrained_models.resnet18(pretrained=True)\n",
        "          input_size = model.fc.in_features\n",
        "          self.hidden_layers = None\n",
        "        \n",
        "        print(model)\n",
        "        # here we get all the modules(layers) before the fc layer at the end\n",
        "        # note that currently at pytorch 1.0 the named_children() is not supported\n",
        "        # and using that instead of children() will fail with an error\n",
        "        self.features = nn.ModuleList(model.children())[:-1]\n",
        "\n",
        "        # Now we have our layers up to the fc layer, but we are not finished yet \n",
        "        # we need to feed these to nn.Sequential() as well, this is needed because,\n",
        "        # nn.ModuleList doesnt implement forward() \n",
        "        # so you cant do sth like self.features(images). Therefore we use \n",
        "        # nn.Sequential and since sequential doesnt accept lists, we \n",
        "        # unpack all the items and send them like this\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        # now lets add our new layers \n",
        "        in_features = model.fc.in_features\n",
        "\n",
        "        # from now, you can add any kind of layers in any quantity!  \n",
        "        # Here I'm creating two new layers \n",
        "        #self.fc0 = nn.Linear(in_features, 256)\n",
        "        #self.fc0_bn = nn.BatchNorm1d(256, eps = 1e-2)\n",
        "        #self.fc1 = nn.Linear(256, output_size)\n",
        "        #self.fc1_bn = nn.BatchNorm1d(output_size, eps = 1e-2)\n",
        "        \n",
        "        # initialize all fc layers to xavier\n",
        "        #for m in self.modules():\n",
        "        #    if isinstance(m, nn.Linear):\n",
        "        #        torch.nn.init.xavier_normal_(m.weight, gain = 1)\n",
        "        \n",
        "        self.classifier = MyPreTrainedClassifier(hidden_layers,output_size)\n",
        "\n",
        "        self.classifier = nn.Sequential(OrderedDict([\n",
        "                                  ('fc1', nn.Linear(in_features, 500)),\n",
        "                                  ('relu', nn.ReLU()),\n",
        "                                  ('fc2', nn.Linear(500, output_size)),\n",
        "                                  ('output', nn.LogSoftmax(dim=1))\n",
        "                                  ]))\n",
        "            \n",
        "        #model.classifier = classifier\n",
        "\n",
        "        # Freeze parameters so we don't backprop through them\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        if train_on_gpu:\n",
        "          # check if CUDA is available\n",
        "          self.train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "        if not self.train_on_gpu:\n",
        "            print('CUDA is not available.  Using CPU ...')\n",
        "        else:\n",
        "            print('CUDA is available!  Using GPU ...')\n",
        "            \n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = lr\n",
        "        self.drop_ratio = drop_p\n",
        "        self.output_classes = output_classes\n",
        "        self.transform=transform\n",
        "        self.dataset=dataset\n",
        "        self.network_type= network_type\n",
        "\n",
        "\n",
        "      def get_dense_net(self):        \n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = None\n",
        "\n",
        "        if ( self.network_type == 'densenet121'):\n",
        "          assert self.input_size == 1024 ,  'input features of pre-trained networks must bet 1024'\n",
        "          model = pretrained_models.densenet121(pretrained=True)\n",
        "\n",
        "        # Freeze parameters so we don't backprop through them\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        model.classifier = nn.Sequential(nn.Linear(self.input_size, 256),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(self.drop_ratio),\n",
        "                                        nn.Linear(256, self.output_size),\n",
        "                                        nn.LogSoftmax(dim=1))\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "\n",
        "        # Only train the classifier parameters, feature parameters are frozen\n",
        "        optimizer = optim.Adam(model.classifier.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        model.to(device)\n",
        "        \n",
        "        return model , criterion , optimizer\n",
        "\n",
        "\n",
        "      def forward(self, x):\n",
        "       # now in forward pass, you have the full control, \n",
        "       # we can use the feature part from our pretrained model  like this\n",
        "        x = self.features(x)\n",
        "        \n",
        "        # since we are using fc layers from now on, we need to flatten the output.\n",
        "        # we used the avgpooling but we still need to flatten from the shape (batch, 1,1, features)\n",
        "        # to (batch, features) so we reshape like this. input_imgs.size(0) gives the batchsize, and \n",
        "        # we use -1 for inferring the rest\n",
        "        #output = output.view(input_imgs.size(0), -1)\n",
        "       # and also our new layers. \n",
        "        #output = self.fc0_bn(F.relu(self.fc1(output)))\n",
        "        #output = self.fc1_bn(F.relu(self.fc1(output)))\n",
        "\n",
        "        x = F.relu(self.classifier.fc1(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = F.relu(self.classifier.fc2(x))\n",
        "        #x = self.dropout(x)\n",
        "        x = self.classifier.output(x)\n",
        "                \n",
        "        return F.log_softmax(x, dim=1)\n",
        "        #return output\n",
        "\n",
        "\n",
        "class MyPreTrainedClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, classifier_sizes, n_classes):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(*[classifier_blocks(in_f, out_f) \n",
        "                       for in_f, out_f in zip(classifier_sizes, dec_sizes[1:])])\n",
        "        self.output = nn.Linear(classifier_sizes[-1], n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "      for each in self.hidden_layers:\n",
        "        x = F.relu(each(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "        x = F.relu(self.classifier(x))\n",
        "        return )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################    \n",
        "# LOADING DATA #\n",
        "######################\n",
        "def LoadData(datasetName,batch_size=20,valid_size = 0.2,network_type='normal'):\n",
        "      \n",
        "    # number of subprocesses to use for data loading\n",
        "    num_workers = 0\n",
        "\n",
        "    # convert data to torch.FloatTensor\n",
        "    #transform = transforms.ToTensor()\n",
        "\n",
        "    # convert data to a normalized torch.FloatTensor\n",
        "    transform = None\n",
        "    train_data= None\n",
        "    test_data=None\n",
        "    classes=None\n",
        "    normalize = None\n",
        "\n",
        "    if network_type != 'normal' :\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    elif network_type == 'normal' and datasetName == 'MINST' :\n",
        "      normalize = transforms.Normalize((0.5, ), (0.5, ))\n",
        "    elif network_type == 'normal' and datasetName == 'CIFAR' :\n",
        "      normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    if(datasetName == 'MINST'):\n",
        "       # convert data to a normalized torch.FloatTensor\n",
        "      transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          #transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),          \n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.MNIST(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "      test_data = datasets.MNIST(root='data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "      # specify the image classes\n",
        "      classes = ['1', '2', '3', '4', '5',\n",
        "           '6', '7', '8', '9', '10']\n",
        "    elif(datasetName == 'CIFAR'):\n",
        "      \n",
        "      # convert data to a normalized torch.FloatTensor\n",
        "      transform = transforms.Compose([\n",
        "          transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          normalize\n",
        "          ])\n",
        "      \n",
        "      # choose the training and test datasets\n",
        "      train_data = datasets.CIFAR10(root='data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "      test_data = datasets.CIFAR10(root='data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "      # specify the image classes\n",
        "      classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # prepare data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "        sampler=valid_sampler, num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "        num_workers=num_workers)\n",
        "    return train_loader , valid_loader ,test_loader ,classes , transform\n",
        "\n",
        "\n",
        "#############################\n",
        "# VALIDATE MODEL\n",
        "############################\n",
        "def validation(model, validationloader, criterion):\n",
        "    accuracy = 0\n",
        "    validation_loss = 0\n",
        "    validate_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : ','Starting Validation....')\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if model.train_on_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "      for images, labels in validationloader:\n",
        "\n",
        "          if model.train_on_gpu:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "          \n",
        "          images = images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "          output = model.forward(images)\n",
        "          validation_loss += criterion(output, labels).item()\n",
        "\n",
        "          ## Calculating the accuracy \n",
        "          # Model's output is log-softmax, take exponential to get the probabilities\n",
        "          ps = torch.exp(output)\n",
        "          # Class with highest probability is our predicted class, compare with true label\n",
        "          #equality = (labels.data == ps.max(1)[1])\n",
        "          # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "          #accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "\n",
        "\n",
        "        \n",
        "          top_p, top_class = ps.topk(1, dim=1)\n",
        "          equals = top_class == labels.view(*top_class.shape)\n",
        "          accuracy += torch.mean(equals.type(torch.FloatTensor))      \n",
        "        \n",
        "      validation_loss = validation_loss/len(validationloader)\n",
        "      accuracy = 100. * accuracy/len(validationloader)\n",
        "    \n",
        "    print('Finished Validation In ',datetime.timedelta(seconds = time.time() - validate_start_time) )\n",
        "    return validation_loss, accuracy\n",
        "\n",
        "#############################\n",
        "# TEST MODEL\n",
        "############################\n",
        "def test(model,test_loader,criterion,checkpoint,outputfilepath,batch_size,override_checkpoint):\n",
        "    # track test loss\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(model.output_size))\n",
        "    class_total = list(0. for i in range(model.output_size))\n",
        "    test_start_time = time.time()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Testing....')\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    with torch.no_grad():    \n",
        "      model.eval()\n",
        "      # iterate over test data\n",
        "      for data, target in test_loader:\n",
        "          # move tensors to GPU if CUDA is available\n",
        "          if model.train_on_gpu:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "          \n",
        "          # Flatten images into a 784 long vector\n",
        "          data.resize_(data.size()[0], model.input_size)\n",
        "\n",
        "          # forward pass: compute predicted outputs by passing inputs to the model\n",
        "          output = model(data)\n",
        "          # calculate the batch loss\n",
        "          loss = criterion(output, target)\n",
        "          # update test loss \n",
        "          test_loss += loss.item()*data.size(0)\n",
        "          # convert output probabilities to predicted class\n",
        "          _, pred = torch.max(output, 1)    \n",
        "          # compare predictions to true label\n",
        "          correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "          correct = np.squeeze(correct_tensor.numpy()) if not model.train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "          # calculate test accuracy for each object class\n",
        "          for i in range(batch_size):\n",
        "              label = target.data[i]\n",
        "              class_correct[label] += correct[i].item()\n",
        "              class_total[label] += 1\n",
        "\n",
        "      # average test loss\n",
        "      print('Finished Testing during ',datetime.timedelta(seconds=time.time() - test_start_time))\n",
        "      test_loss = test_loss/len(test_loader.dataset)\n",
        "      print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "      \n",
        "      test_checkpoint = dict()\n",
        "\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update({'TestLoss': test_loss})\n",
        "      else :\n",
        "        test_checkpoint.update({'TestLoss': test_loss})\n",
        "\n",
        "      for i in range(model.output_size):\n",
        "          if class_total[i] > 0:\n",
        "            current_key  = 'Test Accuracy of '+model.output_classes[i]\n",
        "            current_val = '{:.3f}% ({}/{})'.format(100 *( class_correct[i] / class_total[i]),\n",
        "                  np.sum(class_correct[i]), np.sum(class_total[i]))\n",
        "            ele={current_key:current_val}\n",
        "            if(override_checkpoint):\n",
        "              checkpoint.update(ele)\n",
        "              print(current_key,current_val)\n",
        "            else:\n",
        "              test_checkpoint.update(ele)\n",
        "\n",
        "\n",
        "          \"\"\"else:\n",
        "              print('Test Accuracy of %5s: N/A (no training examples)' % (model.output_classes[i]))\n",
        "          \"\"\"\n",
        "\n",
        "      current_key = 'Test Accuracy (Overall): '#.format(100. * np.sum(class_correct) / np.sum(class_total))\n",
        "      current_val = '{:.3f}% ({}/{})'.format(100 * (np.sum(class_correct) / np.sum(class_total)),\n",
        "          np.sum(class_correct), np.sum(class_total) )\n",
        "      ele={current_key:current_val}\n",
        "      if(override_checkpoint):\n",
        "        checkpoint.update(ele)      \n",
        "        print(current_key,current_val)\n",
        "      else:\n",
        "        test_checkpoint.update(ele)\n",
        "        test_checkpoint.update({'Detailed ':checkpoint})\n",
        "\n",
        "    \n",
        "    #model.train()\n",
        "    if(override_checkpoint):\n",
        "      print('Overriding Checkpoint')\n",
        "      torch.save(checkpoint,outputfilepath)\n",
        "\n",
        "    return checkpoint , test_checkpoint\n",
        "\n",
        "#############################\n",
        "# TRAIN MODEL\n",
        "############################\n",
        "def train(model, trainloader, validationloader, criterion, optimizer, uploadToGDrive,checkpointPath,PreviousCheckPointId,\n",
        "          PreviousValidationLoss,start_time,exp_id,epochs=5, print_every=40):\n",
        "    # monitor training loss    \n",
        "    steps = 0    \n",
        "    #start_time = time.time()\n",
        "    #dateTimeObj = datetime.now()\n",
        "    #start_time_timestamp = './results/'+dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    #start_time_timestamp = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "    train_losses, valid_losses = [], []\n",
        "    checkpoint = dict()\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if model.train_on_gpu:\n",
        "      model.cuda()\n",
        "\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training using Model Parameters \\n '+str(model)+' \\n from PreviousModel '+ PreviousCheckPointId + ' with validationLoss '+ str(PreviousValidationLoss) )\n",
        "    valid_loss_min = PreviousValidationLoss #np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "    for e in range(epochs):        \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Starting Training of Epoch'+str(e)  )\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            \n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if model.train_on_gpu:\n",
        "              images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Flatten images into a 784 long vector\n",
        "            images.resize_(images.size()[0], model.input_size)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        train_losses.append(train_loss)\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        # Model in inference mode, dropout is off\n",
        "        model.eval()\n",
        "        print(' Finished Training of Epoch '+str(e),' In ',datetime.timedelta(seconds = time.time() - epoch_start_time) )\n",
        "\n",
        "        # Turn off gradients for validation, will speed up inference\n",
        "        with torch.no_grad():\n",
        "            valid_loss, accuracy = validation(model, validationloader, criterion)\n",
        "        \n",
        "        valid_losses.append(valid_loss)\n",
        "\n",
        "        print('{} : Epoch: {} \\tTraining Loss: {:.9f} \\tValidation Loss: {:.9f} \\tAccuracy : {:.6f}'.format(\n",
        "            dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"),\n",
        "            e+1, \n",
        "            train_loss,\n",
        "            valid_loss,\n",
        "            accuracy\n",
        "            ))\n",
        "        \n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.9f} --> {:.9f} ,)  Accuracy: {:.6f}  TimeElapsed: {:.6f}.  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss,\n",
        "            accuracy,\n",
        "            (datetime.timedelta(seconds = time.time() - start_time))))\n",
        "            \n",
        "            \n",
        "\n",
        "            checkpoint = {'InputSize': model.input_size,\n",
        "                  'OutputSize': model.output_size,\n",
        "                  'HiddenLayers': [each.out_features for each in model.hidden_layers] if model.hidden_layers is not None else model.classifier ,\n",
        "                  'LearningRate':model.learning_rate,\n",
        "                  'DropRatio':model.drop_ratio,\n",
        "                  'TrainingLoss' :train_loss,\n",
        "                  'ValidationLoss':valid_loss,\n",
        "                  'ValidationAccuracy':accuracy,\n",
        "                  'ElapsedTime': datetime.timedelta(seconds = time.time() - start_time),\n",
        "                  'Dataset':model.dataset,\n",
        "                  'LastEpoch': e,\n",
        "                  'PreviousCheckPoint': PreviousCheckPointId,\n",
        "                  'GPUState': model.train_on_gpu,                  \n",
        "                  'OutputFolder' : exp_id,\n",
        "                  'CheckPointTimestamp': dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\"), #time.time(),\n",
        "                  'OutputFilePrefix' : 'checkpoint_',\n",
        "                  'OutputClasses': model.output_classes,\n",
        "                  'Transforms': model.transform,\n",
        "                  'TrainingLosses' :train_losses,\n",
        "                  'ValidationLosses':valid_losses,\n",
        "                  'StateDictionay': model.state_dict()}\n",
        "            \n",
        "            #print(checkpoint)\n",
        "            save_model(checkpoint,uploadToGDrive,checkpointPath)\n",
        "            valid_loss_min = valid_loss\n",
        "        \n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return train_losses , valid_losses , checkpoint\n",
        "\n",
        "\n",
        "#############################\n",
        "# SAVE MODEL TO GOOGLE DRIVE\n",
        "############################\n",
        "def save_model(checkpoint,uploadToGDrive,checkpointPath):\n",
        "  \n",
        "  file_path = ''\n",
        "  if(uploadToGDrive):\n",
        "    drive.mount('/content/gdrive')\n",
        "    file_path = checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)      \n",
        "    #path = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])+'.pt'\n",
        "    print('Saving Model to ',file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path +'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "  else:\n",
        "    file_path=checkpointPath+checkpoint['OutputFolder']\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "    \n",
        "    print('Saving Model to ',file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "    torch.save(checkpoint, file_path+'/'+checkpoint['OutputFilePrefix']+str(checkpoint['CheckPointTimestamp'])+'_'+str(checkpoint['LastEpoch'])+'.pt')\n",
        "\n",
        "\n",
        "###########################\n",
        "# SKIP CERTAIN ENTERIES FROM ITERTOOLS.PRODUCT\n",
        "#####################33\n",
        "def resume(iterable, sentinel):\n",
        "    yield from dropwhile(lambda x: x != sentinel, iterable)\n",
        "\n",
        "#############################\n",
        "# LOAD LAST EXPERIMENT PARAMS\n",
        "##################################\n",
        "def load_last_exp_param(logPath,checkpointPath):\n",
        "\n",
        "  #with open(checkpointPath+'/3-3-2020.txt') as f:\n",
        "  #    total = f.read()\n",
        "  #    print( total.count('Starting New') )\n",
        "      \n",
        "  f = open(logPath, 'r')\n",
        "  content = f.read()\n",
        "\n",
        "  hidden_layers = re.findall('hidden_layers: \\{(.+?)\\}', content)\n",
        "  hidden_layers = int(hidden_layers[len(hidden_layers)-1])\n",
        "\n",
        "  hidden_layer_width = re.findall('hidden_layer_width: \\{(.+?)\\}', content)\n",
        "  hidden_layer_width = hidden_layer_width[len(hidden_layer_width)-1]\n",
        "\n",
        "  Learning_rate = re.findall('Learning_Rate: \\{(.+?)\\}', content)\n",
        "  Learning_rate = float(Learning_rate[len(Learning_rate)-1])\n",
        "\n",
        "  drop_ratio = re.findall('drop_ratio: \\{(.+?)\\}', content)\n",
        "  drop_ratio = float(drop_ratio[len(drop_ratio)-1])\n",
        "  prev_valid_loss = np.Inf\n",
        "  return_checkpoint = None\n",
        "  return_model = None\n",
        "\n",
        "  #for filename in os.listdir(checkpointPath):\n",
        "  for root, dirs, files in os.walk(checkpointPath):\n",
        "    for filename in files:\n",
        "      model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "      if( float(checkpoint['ValidationLoss']) < prev_valid_loss ):\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "      \n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      hidden_layer_width_local = hidden_layer_width.strip()\n",
        "      hidden_layer_width_local = hidden_layer_width_local[1:len(hidden_layer_width_local)-1]\n",
        "      hidden_layer_width_local = list(map(int, hidden_layer_width_local.split(',')))\n",
        "\n",
        "      if(hidden_layer_width_local == checkpoint['HiddenLayers'] and float(checkpoint['ValidationLoss']) <= prev_valid_loss ):\n",
        "        print(' Found Matching Checkpoint with Last Experiement Parame ValidationLoss:{} hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} '.format(checkpoint['ValidationLoss'],hidden_layers,hidden_layer_width,Learning_rate,drop_ratio))\n",
        "        return_checkpoint = checkpoint\n",
        "        return_model = model\n",
        "        prev_valid_loss = checkpoint['ValidationLoss']\n",
        "        #return checkpoint,model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "  if( not bool(return_checkpoint)): \n",
        "    print(' Not Found Matching Checkpoint with Last Experiement Parameters hidden_layers:{}  hidden_layer_width:{} Learning_rate:{} drop_ratio:{} '.format(hidden_layers,hidden_layer_width,Learning_rate,drop_ratio))    \n",
        "\n",
        "  return return_checkpoint,return_model,hidden_layers,hidden_layer_width,Learning_rate,drop_ratio,prev_valid_loss\n",
        "\n",
        "\n",
        "################################\n",
        "# TUNE NETWORK LAYERS AND NO. OF NODES\n",
        "##################################\n",
        "def tune_train_network(dataset,epochs,resumeExp=False,resume_logPath='',resume_checkpointPath='',network_type='normal'):\n",
        "\n",
        "  start_time = time.time()\n",
        "  dateTimeObj = dt.now()\n",
        "  exp_id = dateTimeObj.strftime(\"%d_%b_%Y_%H_%M_%S\")\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "  #dataset = 'MINST'\n",
        "  checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  \n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size,network_type)\n",
        "\n",
        "  nodes_per_layer =  [2048, 1024, 512, 256, 128, 64, 32]\n",
        "  full_learning_rates= [0.1, 0.01, 0.0001, 0.001]\n",
        "  full_drop_ratios= [0.8, 0.5, 0.3, 0.1]\n",
        "  learning_rates = None\n",
        "  drop_ratios = None\n",
        "  PreviousValidationLoss= np.Inf\n",
        "  max_hidden_layers = 3\n",
        "  iter_hidden_layer_nodes = None\n",
        "  iter_hidden_layer_nodes_list = None\n",
        "  model = None\n",
        "  resume_checkpoint = None\n",
        "\n",
        "  if(resumeExp):\n",
        "    print(dt.now().strftime(\"%d_%b_%Y_%H_%M_%S\")+' : Resuming Experiment With New ID ',exp_id )\n",
        "    resume_checkpoint,model,resum_hidden_layers,resume_hidden_layer_width,resume_Learning_rate,resume_drop_ratio,resume_prev_valid_loss =  load_last_exp_param(resume_logPath,resume_checkpointPath)\n",
        "    max_hidden_layers = resum_hidden_layers  \n",
        "\n",
        "    learning_rates = full_learning_rates[full_learning_rates.index(resume_Learning_rate):]\n",
        "    drop_ratios = full_drop_ratios[full_drop_ratios.index(resume_drop_ratio):]\n",
        "    PreviousValidationLoss = resume_prev_valid_loss\n",
        "    checkpointPath = resume_checkpointPath\n",
        "\n",
        "      #iter_hidden_layer_nodes = resume(itertools.product(nodes_per_layer, repeat=hidden_layers+1), resume_checkpoint['HiddenLayers'])      \n",
        "    #if(bool(resume_checkpoint)):\n",
        "    #  model , resume_checkpoint , filepath = load_checkpoint(checkpointPath,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')])\n",
        "      \n",
        "\n",
        "  global_counter = 0\n",
        "  #VARY HIDDEN LAEYERS\n",
        "  for hidden_layers in range(max_hidden_layers,0,-1):\n",
        "    #VARY HIDDEN LAYER NODES/WIDTH\n",
        "    if(resumeExp):\n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat= hidden_layers)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      \n",
        "      #pdb.set_trace()\n",
        "      resume_hidden_layer_width = resume_hidden_layer_width[1:len(resume_hidden_layer_width)-1]\n",
        "      resume_hidden_layer_width_list = tuple(list(map(int, resume_hidden_layer_width.split(','))))\n",
        "      iter_hidden_layer_nodes_list = iter_hidden_layer_nodes_list[iter_hidden_layer_nodes_list.index(resume_hidden_layer_width_list):]\n",
        "      \n",
        "      print('Resume ValidationLoss ',PreviousValidationLoss,'Resume Learning Rate Array ',learning_rates ,' Resume Drop Ratio List ',drop_ratios , ' Resume Hidden Nodes List with Size :',len(iter_hidden_layer_nodes_list) , ' Starting From :',iter_hidden_layer_nodes_list[0] )\n",
        "    else:      \n",
        "      iter_hidden_layer_nodes = itertools.product(nodes_per_layer, repeat=hidden_layers+1)\n",
        "      iter_hidden_layer_nodes_list = [item for item in iter_hidden_layer_nodes]\n",
        "    for hidden_layer in iter_hidden_layer_nodes_list:\n",
        "      #VARY LEARNING RATE\n",
        "      if( not resumeExp):\n",
        "        learning_rates = full_learning_rates\n",
        "      for lr in learning_rates :\n",
        "        #VARY DROP RATIO\n",
        "        if( not resumeExp):\n",
        "          drop_ratios = full_drop_ratios\n",
        "        for drop_ratio in drop_ratios :\n",
        "          \n",
        "          print('Global Counter: '+str(global_counter)+' Network Type: '+network_type+' EXP-ID : '+exp_id+'  DurationSinceStart :',datetime.timedelta(seconds = time.time() - start_time),' Starting New Experiment with %hidden_layers: {',hidden_layers,'}% %hidden_layer_width: {',hidden_layer ,'}% %Learning_Rate: {',lr, '}% %drop_ratio: {',drop_ratio,'}%')\n",
        "          #model = Network(784, 10, [first_layer, second_layer, third_layer ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "\n",
        "          if( not bool(resume_checkpoint) or not resumeExp ):\n",
        "            #intput_size = (28*28) if network_type == 'normal' and dataset == 'MINST' else (32*32*3) if network_type == 'normal' and dataset == 'CIFAR' else 1024 # 1024 is defult for pretrained networks\n",
        "            input_size = 0\n",
        "            if network_type == 'normal' and dataset == 'MINST':\n",
        "              input_size = (28*28) \n",
        "            elif network_type == 'normal' and dataset == 'CIFAR' :\n",
        "              input_size = (32*32*3)  \n",
        "            elif network_type != 'normal' :\n",
        "              input_size = 1024 # 1024 is defult for pretrained networks\n",
        "\n",
        "            if network_type == 'normal':\n",
        "              model = Network(input_size, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True)\n",
        "            else:\n",
        "              model = PreTrainenModels(input_size, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "\n",
        "            criterion = nn.NLLLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            #if(dataset == 'MINST'):\n",
        "            #  model = Network(784, 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "            #else:\n",
        "            #  model = Network((32*32*3), 10, hidden_layer ,classes,transform,dataset,drop_p=drop_ratio,lr =lr,train_on_gpu=True,network_type=network_type)\n",
        "\n",
        "          #Upload checkpoint to local drive\n",
        "          #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "          #Upload to gDrive\n",
        "          train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',PreviousValidationLoss,start_time,exp_id,epochs=epochs)\n",
        "          if( bool(checkpointt) and checkpointt['ValidationLoss'] < PreviousValidationLoss ):\n",
        "            PreviousValidationLoss = checkpointt['ValidationLoss']\n",
        "          global_counter+= 1\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        resumeExp=False\n",
        "\n",
        "\n",
        "#############################\n",
        "# PLOT TRAINING LOSS VS VALIDATION LOSS \n",
        "############################\n",
        "def plotLossTrend(train_losses,validation_losses,test_losses=[]):\n",
        "  #TRAINING LOSS DATA\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in train_losses:    \n",
        "    for key, value in item.items():\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Training Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #VALIDATON LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in validation_losses:   \n",
        "    #pdb.set_trace() \n",
        "    for key, value in item.items():      \n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Validation Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  #plt.show()\n",
        "\n",
        "  #TEST LOSS\n",
        "  values = []\n",
        "  labels = []\n",
        "  for item in test_losses:    \n",
        "    for key, value in item.items():\n",
        "      #pdb.set_trace()\n",
        "      values.append(\"{0:.5f}\".format(value)) #= train_losses[:1] #[7, 57, 121, 192, 123, 240, 546]\n",
        "      labels.append(key) #= train_losses[:0] #['1950s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s']\n",
        "\n",
        "\n",
        "  #plt.figure(figsize = (12,12))   \n",
        "  plt.plot(labels,values,label='Test Loss')\n",
        "  for i,j in zip(labels,values):\n",
        "      plt.annotate(str(j),xy=(i,j))\n",
        "  \n",
        "  plt.xticks(rotation=90)\n",
        "  plt.show()\n",
        "  #for i, v in enumerate(values):\n",
        "  #    ax.text(i, v+25, \"%d\" %v, ha=\"center\")\n",
        "  #plt.ylim(-10, 595)\n",
        "\n",
        "#############################\n",
        "# LOAD CHECKPOINT\n",
        "############################\n",
        "def load_checkpoint(OutputFolder,OutputFilePrefix,max_epoch,exact_epoch,load_model=True):\n",
        "    \n",
        "    file_epoch = 0\n",
        "    if(max_epoch):\n",
        "      for filename in os.listdir(OutputFolder):\n",
        "        data = filename.split('_')\n",
        "        tmp=int(data[1][:-3])\n",
        "        #print('data[1]',data[1],' tmp[:-3] ',tmp[:-3])\n",
        "        if( file_epoch <= tmp ):\n",
        "          file_epoch = tmp\n",
        "    else:\n",
        "      file_epoch=exact_epoch\n",
        "\n",
        "    filepath = OutputFolder+'/'+OutputFilePrefix + str(file_epoch)+'.pt'\n",
        "    print('Loading Checkpoint from '+filepath+'...')\n",
        "    indent=1\n",
        "    \n",
        "    cpu_or_gpu = torch.cuda.is_available()\n",
        "    checkpoint = None\n",
        "\n",
        "    if not cpu_or_gpu :\n",
        "      checkpoint = torch.load(filepath,map_location='cpu')\n",
        "    else:\n",
        "      checkpoint = torch.load(filepath)\n",
        "\n",
        "    for key, value in checkpoint.items():\n",
        "        if(key == 'StateDictionay'):\n",
        "          continue\n",
        "        print('\\t' * indent + str(key),'\\t' * (indent+1) + str(value))\n",
        "        #print('\\t' * (indent+1) + str(value))\n",
        "    \n",
        "    testmodel = None\n",
        "\n",
        "    if(load_model):\n",
        "      testmodel = Network(checkpoint['InputSize'],\n",
        "                              checkpoint['OutputSize'],\n",
        "                              checkpoint['HiddenLayers'],\n",
        "                              checkpoint['OutputClasses'],\n",
        "                              checkpoint['Transforms'],\n",
        "                              checkpoint['Dataset'],\n",
        "                              checkpoint['DropRatio'],\n",
        "                              checkpoint['LearningRate'],\n",
        "                              train_on_gpu=checkpoint['GPUState']\n",
        "                      )\n",
        "      testmodel.load_state_dict(checkpoint['StateDictionay'])\n",
        "      \n",
        "    return testmodel , checkpoint , filepath\n",
        "\n",
        "\n",
        "def test_all_epochs(OutputFolder,test_loader,criterion,batch_size,OutputFilePrefix='checkpoint_'):\n",
        "\n",
        "    #OutputFolder = './results/'\n",
        "    file_epoch = 0\n",
        "    result_list=[]\n",
        "    print('Starting Test All Saved Epochs ....')\n",
        "    trainig_loss=[]\n",
        "    validation_loss=[]\n",
        "    test_loss=[]\n",
        "    ele = dict()\n",
        "\n",
        "    \"\"\"\n",
        "        for folder in os.listdir(OutputFolder):\n",
        "          for filename in os.listdir(OutputFolder+folder):\n",
        "            data = filename.split('_')\n",
        "            #pdb.set_trace()\n",
        "            print(data)\n",
        "            file_epoch=int(data[1][:-3])\n",
        "\n",
        "            model , checkpoint , filepath = load_checkpoint(OutputFolder+folder,OutputFilePrefix,False,file_epoch)\n",
        "    \"\"\"\n",
        "    #for filename in os.listdir(checkpointPath):\n",
        "    for root, dirs, files in os.walk(OutputFolder):\n",
        "      for filename in files:\n",
        "        model , checkpoint , filepath = load_checkpoint(root,filename[:filename.rfind('_')+1],False,filename[filename.rfind('_')+1:filename.rfind('.')],load_model=True)\n",
        "\n",
        "        checkpoint , checkpoint_test = test(model,test_loader,criterion,checkpoint,filepath,batch_size,False)\n",
        "\n",
        "        print('Test Accuracy ',checkpoint_test['Test Accuracy (Overall): '])\n",
        "        result_list.append(checkpoint_test)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['TrainingLoss'])\n",
        "        key = checkpoint['Dataset'] + '_' + checkpoint['OutputFolder'] + '_' + str(checkpoint['LastEpoch'])\n",
        "        ele = {key: checkpoint['TrainingLoss']}\n",
        "        trainig_loss.append(ele)\n",
        "        \n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint['ValidationLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint['ValidationLoss']}\n",
        "        validation_loss.append(ele)\n",
        "\n",
        "        #ele = [list() for f in range(1)] # We have Three Empty Rows\n",
        "        #ele[0].append(checkpoint['Dataset']+'_'+checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch']))\n",
        "        #ele[0].append(checkpoint_test['TestLoss'])\n",
        "        ele = {}\n",
        "        ele ={key: checkpoint_test['TestLoss']}\n",
        "        test_loss.append(ele)\n",
        "    \n",
        "    #print('Result List',result_list)\n",
        "    plotLossTrend (trainig_loss,validation_loss,test_loss)\n",
        "    return result_list\n",
        "      \n",
        "\n",
        "#############################\n",
        "# VISUALIZE ALL IMAGES IN BATCH\n",
        "############################\n",
        "def visualize_images_in_batch(loader,batch_size) :\n",
        "    # obtain one batch of training images\n",
        "\t\tdataiter = iter(loader)\n",
        "\t\timages, labels = dataiter.next()\n",
        "\t\timages = images.numpy()\n",
        "\n",
        "\t\t# plot the images in the batch, along with the corresponding labels\n",
        "\t\tfig = plt.figure(figsize=(25, 4))\n",
        "\t\tfor idx in np.arange(batch_size):\n",
        "\t\t\tax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\t\t\tax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "\t\t\t# print out the correct label for each image\n",
        "\t\t\t# .item() gets the value contained in a Tensor\n",
        "\t\t\tax.set_title(str(labels[idx].item()))\n",
        "\t\treturn images\n",
        "\n",
        "\n",
        "#############################\n",
        "# VISUALIZE PIXEL OF AN IMAGE RETURN FROM PREV FUNCTION\n",
        "############################\n",
        "def visualize_image_pixels_value(image):\n",
        "\n",
        "  img = np.squeeze(image)\n",
        "  fig = plt.figure(figsize = (12,12)) \n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.imshow(img, cmap='gray')\n",
        "  width, height = img.shape\n",
        "  thresh = img.max()/2.5\n",
        "  for x in range(width):\n",
        "      for y in range(height):\n",
        "          val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "          ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center',\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_rgb_image_pixels_value(image):\n",
        "  rgb_img = np.squeeze(image)\n",
        "  channels = ['red channel', 'green channel', 'blue channel']\n",
        "\n",
        "  fig = plt.figure(figsize = (36, 36)) \n",
        "  for idx in np.arange(rgb_img.shape[0]):\n",
        "      ax = fig.add_subplot(1, 3, idx + 1)\n",
        "      img = rgb_img[idx]\n",
        "      ax.imshow(img, cmap='gray')\n",
        "      ax.set_title(channels[idx])\n",
        "      width, height = img.shape\n",
        "      thresh = img.max()/2.5\n",
        "      for x in range(width):\n",
        "          for y in range(height):\n",
        "              val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "              ax.annotate(str(val), xy=(y,x),\n",
        "                      horizontalalignment='center',\n",
        "                      verticalalignment='center', size=8,\n",
        "                      color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "def visualize_test_results(model,test_loader,batch_size,RGB=False):\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  # move model inputs to cuda, if GPU available\n",
        "  if model.train_on_gpu:\n",
        "      images = images.cuda()\n",
        "\n",
        "  # Flatten images into a 784 long vector\n",
        "  images.resize_(images.size()[0], model.input_size)\n",
        "\n",
        "  # get sample outputs\n",
        "  output = model(images)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, preds_tensor = torch.max(output, 1)\n",
        "  preds = np.squeeze(preds_tensor.numpy()) if not model.train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "  # plot the images in the batch, along with predicted and true labels\n",
        "  fig = plt.figure(figsize=(25, 4))\n",
        "  for idx in np.arange(batch_size):\n",
        "      ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "      image_cpu = images.cpu()[idx]\n",
        "      image_cpu = image_cpu / 2 + 0.5  # unnormalize\n",
        "      if(RGB):\n",
        "        plt.imshow(np.transpose(image_cpu, (1, 2, 0)))  # convert from Tensor image\n",
        "      else:\n",
        "        ax.imshow(np.squeeze(image_cpu), cmap='gray')\n",
        "\n",
        "      ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                  color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voDNqAjioodn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# TEST TUNE NETWORK ####################################\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\"\"\"\n",
        "def test_tune_train_network(dataset):    \n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "  #tune_train_network(resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "  #tune_train_network('MINST',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/3-6-2020.txt','/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/')\n",
        "\n",
        "  #sys.path.append('./mydeeplearning/')\n",
        "  \n",
        "  #MINST\n",
        "  #cmd_str = \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/') 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt'\" \n",
        "  func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/all/')\\\"\" \n",
        "  log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume.txt'\"\n",
        "\n",
        "\n",
        "  #CIFAR\n",
        "  #func_str = \"\\\"from mydeeplearning.ez_mlp import*; tune_train_network(\\'\"+dataset+\"\\',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\\\"\" \n",
        "  #log_path = \"'/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/3-8-2020.txt'\"\n",
        "\n",
        "  print(func_str)\n",
        "  print(log_path)\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network('CIFAR',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/06_Mar_2020_07_57_24/')\" 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/3-8-2020.txt'\n",
        "  !python -c $func_str 2>&1 | tee -a $log_path\n",
        "\n",
        "\n",
        "  #!python -c $cmd_str\n",
        "\n",
        "  #!python -c \"from mydeeplearning.ez_mlp import*; tune_train_network(\"+dataset+\",10,resumeExp=True,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/logs/resume-3-6-2020.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/\"+dataset+\"/06_Mar_2020_07_57_24/')\" -u /mydeeplearning/ez_mlp.py 2>&1 | tee -a '/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs/resume-3-6-2020.txt' #2>&1\n",
        "  \n",
        "  \n",
        "\n",
        "  #load_last_exp_param('/content/gdrive/My Drive/Colab Notebooks/models/MINST/logs','/content/gdrive/My Drive/Colab Notebooks/models/MINST/04_Mar_2020_20_13_34')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXvRQ0zv5hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### TEST ALL EPOCHS ##############################\n",
        "def test_test_all_epochs():\n",
        "    \n",
        "\n",
        "  #model , checkpointtt , filepathh = load_checkpoint(checkpointttt['OutputFolder'],checkpointttt['OutputFilePrefix'],False,10)\n",
        "\n",
        "  #result_list = test_all_epochs(test_loader,criterion,batch_size)\n",
        "\n",
        "  dataset = 'MINST'\n",
        "\n",
        "\n",
        "  #How many samples loaded per batch\n",
        "  batch_size = 20\n",
        "  # percentage of training set to use as validation\n",
        "  valid_size = 0.2\n",
        "\n",
        "  #train_loader =None\n",
        "  #valid_loader =None\n",
        "  #Load Data\n",
        "  dataset = 'MINST'\n",
        "  train_loader , valid_loader ,test_loader , classes , transform = LoadData(dataset,batch_size,valid_size)\n",
        "\n",
        "\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  gfolder = '/content/gdrive/My Drive/Colab Notebooks/models/MINST/06_Mar_2020_07_57_24/'\n",
        "\n",
        "  localfolder = './results/'+dataset+'/'\n",
        "\n",
        "  result_list = test_all_epochs(gfolder,test_loader,nn.NLLLoss(),batch_size,OutputFilePrefix='checkpoint_')\n",
        "\n",
        "  #checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks/models/minst/29_Feb_2020_20_06_50_74.pt')\n",
        "\n",
        "  #print(checkpoint)\n",
        "  #load_checkpoint('/gdrive/My Drive/Colab Notebooks/models/'+dataset,'29_Feb_2020_20_06_50',False,74)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu907NvWVlBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## TEST VISUALIZATION###################\n",
        "def test_visualize_images_in_batch():\n",
        "  images = visualize_images_in_batch(train_loader,batch_size)\n",
        "  visualize_image_pixels_value(images[2])\n",
        "\n",
        "  model , checkpointtt , filepathh = load_checkpoint(checkpointt['OutputFolder'],checkpointt['OutputFilePrefix'],True,0)\n",
        "\n",
        "  checkpointttt = test(model,test_loader,criterion,checkpointtt,filepathh,batch_size,True)\n",
        "\n",
        "  # obtain one batch of test images\n",
        "  dataiter = iter(test_loader)\n",
        "  images, labels = dataiter.next()\n",
        "  images.numpy()\n",
        "\n",
        "  #visualize_rgb_image_pixels_value(images[2])\n",
        "  #visualize_test_results(model,test_loader,batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbNJdUIMSTBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository setup\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
        "from os.path import join  \n",
        "#import helper.py\n",
        "import imp \n",
        "\n",
        "\n",
        "################################\n",
        "# INIT GITHUB\n",
        "#######################################\n",
        "def init_github():\n",
        "  \"\"\"\n",
        "  !git init\n",
        "  !git config  global user.email ice.man011@gmail.com\n",
        "  !git config  global user.name iceman011\n",
        "  !git add -A\n",
        "  !git commit -m first commit\n",
        "  !git remote add origin https://<username>:<password>github@github.com/iceman011/mydeeplearning.git\n",
        "  !git push -u origin master\n",
        "  \"\"\"\n",
        "  #!git clone -l -s git://github.com/iceman011/mydeeplearning.git #mydeeplearning-repo\n",
        "  #!git clone -l -s https://github.com/iceman011@github.com/mydeeplearning.git\n",
        "\n",
        "  #%cd mydeeplearning-repo\n",
        "  #!ls\n",
        "\n",
        "  # path to your project on Google Drive\n",
        "  #MY_GOOGLE_DRIVE_PATH = 'My Drive/MyDrive/Udacity/deep-learning-v2-pytorch' \n",
        "  # replace with your Github username \n",
        "  GIT_USERNAME = \"iceman011\" \n",
        "  # definitely replace with your\n",
        "  GIT_TOKEN = \"1aeb0c6f424e3c604988a3a636f74c6e5180cd89\"  \n",
        "  # Replace with your github repository in this case we want \n",
        "  # to clone deep-learning-v2-pytorch repository\n",
        "  GIT_REPOSITORY = \"mydeeplearning\" \n",
        "\n",
        "  # REMOVE IT BEFORE INIT\n",
        "  if( os.path.isdir('./'+GIT_REPOSITORY) ):\n",
        "    shutil.rmtree('./'+GIT_REPOSITORY)\n",
        "\n",
        "  #PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "  # It's good to print out the value if you are not sure \n",
        "  #print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "  # In case we haven't created the folder already; we will create a folder in the project path \n",
        "  #!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "  #GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "\n",
        "  GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "  print(\"GIT_PATH: \", GIT_PATH)\n",
        "\n",
        "  !git clone \"{GIT_PATH}\" # clone the github repository\n",
        "\n",
        "############################3\n",
        "# CONVERT FROM COLAB INTO PYTHON\n",
        "#############################3\n",
        "def from_colab_to_python():\n",
        "\n",
        "  !pip install ipython\n",
        "  !pip install nbconvert\n",
        "  !ipython nbconvert --to python ./mydeeplearning/ez_mlp.ipynb\n",
        "\n",
        "  #helper = imp.new_module('ez-mlp')\n",
        "  #exec(open('./'+GIT_REPOSITORY+\"/ez-mlp.py\").read(), helper.__dict__)\n",
        "\n",
        "\n",
        "######################################\n",
        "# PUSH CHANGES TO GITHUB\n",
        "##################################\n",
        "def push_github():\n",
        "  !git add -u\n",
        "  !git commit -m \"new commit\"\n",
        "  !git push mydeeplearning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7of0_y_zsQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "######################    \n",
        "# MAIN #\n",
        "######################\n",
        "def main():\n",
        "\n",
        "  # Create the network, define the criterion and optimizer\n",
        "  drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "  dataset = 'MINST'\n",
        "\n",
        "  init_github()\n",
        "  from_colab_to_python()\n",
        "  test_tune_train_network(dataset)\n",
        "\n",
        "  #CONTINUE FROM PREVIOUS MODEL\n",
        "  #model , checkpoint , filepath = load_checkpoint('/content/drive/My Drive/Colab Notebooks/models/MINST/01_Mar_2020_18_42_35','checkpoint_',True,0)\n",
        "  #PreviousCheckPointId = checkpoint['OutputFolder']+'_'+str(checkpoint['LastEpoch'])\n",
        "  #PreviousValidationLoss= checkpoint['ValidationLoss'] #np.Inf\n",
        "\n",
        "  #model = Network(784, 10, [512, 256, 128 , 64 ],classes,transform,dataset, lr=0.001,train_on_gpu=True)\n",
        "\n",
        "  #criterion = nn.NLLLoss()\n",
        "  #optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  #checkpointPath = f'/content/gdrive/My Drive/Colab Notebooks/models/'+dataset+'/'\n",
        "  #checkpointPath = f'./results/'+dataset+'/'\n",
        "\n",
        "\n",
        "\n",
        "  #Upload checkpoint to local drive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, False,checkpointPath,PreviousCheckPointId,PreviousValidationLoss,epochs=100)\n",
        "\n",
        "  #Upload to gDrive\n",
        "  #train_lossess , valid_lossess , checkpointt = train(model, train_loader, valid_loader, criterion, optimizer, True,checkpointPath,'',np.Inf,epochs=10)\n",
        "\n",
        "  #plotLossTrend (train_lossess , valid_lossess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mp5Be63OjI7",
        "colab_type": "code",
        "outputId": "01d1a163-fa96-4815-d753-c013dc6ffeba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#if __name__ == '__main__':\n",
        "#main()\n",
        "\n",
        "tune_train_network('CIFAR',10,resumeExp=False,resume_logPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/test.txt',resume_checkpointPath='/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/test/',network_type='resnet18')\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Global Counter: 0 Network Type: resnet18 EXP-ID : 15_Mar_2020_16_13_20  DurationSinceStart : 0:00:01.616333  Starting New Experiment with %hidden_layers: { 3 }% %hidden_layer_width: { (2048, 2048, 2048, 2048) }% %Learning_Rate: { 0.1 }% %drop_ratio: { 0.8 }%\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "CUDA is available!  Using GPU ...\n",
            "15_Mar_2020_16_13_22 : Starting Training using Model Parameters \n",
            " PreTrainenModels(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (fc1): Linear(in_features=512, out_features=500, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            "    (output): LogSoftmax()\n",
            "  )\n",
            ") \n",
            " from PreviousModel  with validationLoss inf\n",
            "15_Mar_2020_16_13_22 : Starting Training of Epoch0\n",
            " Finished Training of Epoch 0  In  0:00:47.067093\n",
            "15_Mar_2020_16_14_09 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.227022\n",
            "15_Mar_2020_16_14_20 : Epoch: 1 \tTraining Loss: 0.150899 \tValidation Loss: 2.302585 \tAccuracy : 10.449986\n",
            "Validation loss decreased (inf --> 2.302585 ,)  Accuracy: 10.449986  TimeElapsed: 60.175545.  Saving model ...\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/gdrive\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_14_20_0.pt\n",
            "15_Mar_2020_16_15_34 : Starting Training of Epoch1\n",
            " Finished Training of Epoch 1  In  0:00:49.420816\n",
            "15_Mar_2020_16_16_23 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.654550\n",
            "15_Mar_2020_16_16_35 : Epoch: 2 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449987\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449987  TimeElapsed: 194.938659.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_16_35_1.pt\n",
            "15_Mar_2020_16_16_35 : Starting Training of Epoch2\n",
            " Finished Training of Epoch 2  In  0:00:49.060383\n",
            "15_Mar_2020_16_17_24 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.437427\n",
            "15_Mar_2020_16_17_35 : Epoch: 3 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449991\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449991  TimeElapsed: 255.565037.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_17_35_2.pt\n",
            "15_Mar_2020_16_17_36 : Starting Training of Epoch3\n",
            " Finished Training of Epoch 3  In  0:00:49.601352\n",
            "15_Mar_2020_16_18_25 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.561868\n",
            "15_Mar_2020_16_18_37 : Epoch: 4 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449987\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449987  TimeElapsed: 316.864551.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_18_37_3.pt\n",
            "15_Mar_2020_16_18_37 : Starting Training of Epoch4\n",
            " Finished Training of Epoch 4  In  0:00:48.733330\n",
            "15_Mar_2020_16_19_26 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.296381\n",
            "15_Mar_2020_16_19_37 : Epoch: 5 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449991\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449991  TimeElapsed: 376.994978.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_19_37_4.pt\n",
            "15_Mar_2020_16_19_37 : Starting Training of Epoch5\n",
            " Finished Training of Epoch 5  In  0:00:47.091031\n",
            "15_Mar_2020_16_20_24 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.327717\n",
            "15_Mar_2020_16_20_35 : Epoch: 6 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449986\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449986  TimeElapsed: 435.512164.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_20_35_5.pt\n",
            "15_Mar_2020_16_20_35 : Starting Training of Epoch6\n",
            " Finished Training of Epoch 6  In  0:00:47.557888\n",
            "15_Mar_2020_16_21_23 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.149189\n",
            "15_Mar_2020_16_21_34 : Epoch: 7 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449990\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449990  TimeElapsed: 494.356939.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_21_34_6.pt\n",
            "15_Mar_2020_16_21_34 : Starting Training of Epoch7\n",
            " Finished Training of Epoch 7  In  0:00:47.058898\n",
            "15_Mar_2020_16_22_21 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.180278\n",
            "15_Mar_2020_16_22_33 : Epoch: 8 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449992\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449992  TimeElapsed: 552.736397.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_22_33_7.pt\n",
            "15_Mar_2020_16_22_33 : Starting Training of Epoch8\n",
            " Finished Training of Epoch 8  In  0:00:47.310465\n",
            "15_Mar_2020_16_23_20 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.131895\n",
            "15_Mar_2020_16_23_31 : Epoch: 9 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449991\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449991  TimeElapsed: 611.305577.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_23_31_8.pt\n",
            "15_Mar_2020_16_23_31 : Starting Training of Epoch9\n",
            " Finished Training of Epoch 9  In  0:00:46.865180\n",
            "15_Mar_2020_16_24_18 :  Starting Validation....\n",
            "Finished Validation In  0:00:11.147792\n",
            "15_Mar_2020_16_24_29 : Epoch: 10 \tTraining Loss: 0.115129 \tValidation Loss: 2.302585 \tAccuracy : 10.449985\n",
            "Validation loss decreased (2.302585 --> 2.302585 ,)  Accuracy: 10.449985  TimeElapsed: 669.445474.  Saving model ...\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Saving Model to  /content/gdrive/My Drive/Colab Notebooks/models/CIFAR/15_Mar_2020_16_13_20/checkpoint_15_Mar_2020_16_24_29_9.pt\n",
            "Global Counter: 1 Network Type: resnet18 EXP-ID : 15_Mar_2020_16_13_20  DurationSinceStart : 0:11:09.547751  Starting New Experiment with %hidden_layers: { 3 }% %hidden_layer_width: { (2048, 2048, 2048, 2048) }% %Learning_Rate: { 0.1 }% %drop_ratio: { 0.5 }%\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "CUDA is available!  Using GPU ...\n",
            "15_Mar_2020_16_24_30 : Starting Training using Model Parameters \n",
            " PreTrainenModels(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (fc1): Linear(in_features=512, out_features=500, bias=True)\n",
            "    (relu): ReLU()\n",
            "    (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            "    (output): LogSoftmax()\n",
            "  )\n",
            ") \n",
            " from PreviousModel  with validationLoss 2.3025853633880615\n",
            "15_Mar_2020_16_24_30 : Starting Training of Epoch0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-393d558ce3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtune_train_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CIFAR'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresumeExp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresume_logPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/logs/test.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresume_checkpointPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/models/CIFAR/test/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'resnet18'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-b9925c67a2e5>\u001b[0m in \u001b[0;36mtune_train_network\u001b[0;34m(dataset, epochs, resumeExp, resume_logPath, resume_checkpointPath, network_type)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m           \u001b[0;31m#Upload to gDrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m           \u001b[0mtrain_lossess\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalid_lossess\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcheckpointt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpointPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPreviousValidationLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m           \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpointt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheckpointt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ValidationLoss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mPreviousValidationLoss\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mPreviousValidationLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpointt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ValidationLoss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-b9925c67a2e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, validationloader, criterion, optimizer, uploadToGDrive, checkpointPath, PreviousCheckPointId, PreviousValidationLoss, start_time, exp_id, epochs, print_every)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prep model for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}